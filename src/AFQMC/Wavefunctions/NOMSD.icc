//////////////////////////////////////////////////////////////////////
// This file is distributed under the University of Illinois/NCSA Open Source
// License.  See LICENSE file in top directory for details.
//
// Copyright (c) 2016 Jeongnim Kim and QMCPACK developers.
//
// File developed by:
// Miguel A. Morales, moralessilva2@llnl.gov 
//    Lawrence Livermore National Laboratory 
//
// File created by:
// Miguel A. Morales, moralessilva2@llnl.gov 
//    Lawrence Livermore National Laboratory 
////////////////////////////////////////////////////////////////////////////////

#include <vector>
#include <map>
#include <string>
#include <iostream>
#include <tuple>
#include<mutex>

#include "OhmmsApp/RandomNumberControl.h"
#include "Utilities/Timer.h"
#include "Utilities/FairDivide.h" 
#include "AFQMC/config.h"
#include "AFQMC/Numerics/csr_blas.hpp"
#include "AFQMC/Numerics/tensor_operations.hpp"
#include "AFQMC/Walkers/WalkerSet.hpp"

//#include "AFQMC/Wavefunctions/NOMSD.h"

namespace qmcplusplus
{

namespace afqmc
{
  /*
   * Calculates the local energy and overlaps of all the walkers in the set and 
   * returns them in the appropriate data structures
  */
  template<class devPsiT>  
  template<class WlkSet, class Mat, class TVec>
  void NOMSD<devPsiT>::Energy_shared(const WlkSet& wset, Mat&& E, TVec&& Ov)
  {
    size_t nt = wset.size()*(1+dm_size(false));
    if(shmbuff_for_E.num_elements() < nt)
      shmbuff_for_E.reextent(iextensions<1u>{nt}); 
    assert(E.dimensionality==2);
    assert(Ov.dimensionality==1);
    assert(E.size(0)==wset.size()); 
    assert(E.stride(0)==E.size(1)); 
    assert(Ov.size(0)==wset.size()); 
    assert(E.size(1)==3); 

    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr(make_device_ptr(E.origin()));
      auto dev_ptr_(make_device_ptr(Ov.origin()));
    }

    ComplexType zero(0.0);
    auto Gsize = dm_size(false);
    auto nwalk = wset.size();
    int nr=Gsize,nc=nwalk;
    if(transposed_G_for_E_) std::swap(nr,nc);
    CMatrix_ref G(make_device_ptr(shmbuff_for_E.origin()),{nr,nc});
    CVector_ref ov_(G.origin()+G.num_elements(),iextensions<1u>{nwalk});
    if(eloc2.size(0) != nwalk || eloc2.size(1) != 3) eloc2.reextent({nwalk,3}); 

    using std::fill_n;
    fill_n(Ov.origin(),nwalk,zero);
    fill_n(E.origin(),3*nwalk,zero);

    for(int nd=0, nref=0; nd<ci.size(); nd++, nref+=nspins) {
      DensityMatrix(wset,OrbMats[nref],OrbMats[nref+nspins-1],
                        G,ov_,true,true,transposed_G_for_E_);
      ma::axpy(ma::conj(ci[nd]),ov_,Ov);
      HamOp.energy(eloc2,G,nd,TG.TG_local().root());  
      //TG.TG_local().all_reduce_in_place_n(eloc2.origin(),3*nwalk,std::plus<>());
      for(int i=0; i<nwalk; ++i) 
        for(int k=0; k<3; ++k) 
          E[i][k] += ma::conj(ci[nd])*ov_[i]*eloc2[i][k];  
    }     

    if(TG.TG_local().size() > 1) 
      TG.TG_local().all_reduce_in_place_n(to_address(E.origin()),3*nwalk,std::plus<>());
    for(int i=0; i<nwalk; ++i) 
      for(int k=0; k<3; k++) 
        E[i][k] /= Ov[i]; 
    TG.local_barrier();
  }

  /*
   * Calculates the local energy and overlaps of all the walkers in the set and 
   * returns them in the appropriate data structures
   */
  template<class devPsiT>  
  template<class WlkSet, class Mat, class TVec>
  void NOMSD<devPsiT>::Energy_distributed_singleDet(const WlkSet& wset, Mat&& E, TVec&& Ov)
  {
    //1. Calculate G and overlaps
    //2. Loop over nodes in TG
    // 2.a isend G to next node. irecv next G from "previous" node 
    // 2.b add local contribution to current G
    // 2.c wait for comms to finish
    //3. all reduce resulting energies    

    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr(make_device_ptr(E.origin()));
      auto dev_ptr_(make_device_ptr(Ov.origin()));
    }
    assert(ci.size()==1);
    bool new_shm_space=false; 
    const int node_number = TG.getLocalGroupNumber();
    const int ngroups = TG.getNGroupsPerTG();
    const int Gsize = dm_size(false);
    const ComplexType zero(0.0);
    const int nwalk = wset.size();
    // allocte space in shared memory for:
    //  i.  2 copies of G (always compact),
    //  ii. ovlps for local walkers
    //  iii. energies[3] for all walkers on all nodes of TG (assume all nodes have same # of walkers)
    int nt = nwalk*(2*Gsize+1);
    // in case the number of walkers changes
    if(shmbuff_for_E.num_elements() < nt) {
      shmbuff_for_E.reextent(iextensions<1u>{nt});
      new_shm_space=true;
    }
    assert(E.dimensionality==2);
    assert(Ov.dimensionality==1);
    assert(E.size(0)==wset.size());
    assert(Ov.size(0)==wset.size());
    assert(E.size(1)==3);

    int nr=Gsize,nc=nwalk;
    if(transposed_G_for_E_) std::swap(nr,nc);
    CMatrix_ref Gwork(make_device_ptr(shmbuff_for_E.origin()),{nr,nc});
    CMatrix_ref Grecv(Gwork.origin()+Gwork.num_elements(),{nr,nc});
    CVector_ref overlaps(Grecv.origin()+Grecv.num_elements(),iextensions<1u>{nwalk});
    if(eloc2.size(0) != ngroups*nwalk || eloc2.size(1) != 3) 
        eloc2.reextent({ngroups*nwalk,3});
    int nak0,nak1;
    std::tie(nak0,nak1) = FairDivideBoundary(TG.getLocalTGRank(),Gsize*nwalk,TG.getNCoresPerTG());

    if(new_shm_space) {
      // use mpi3 when ready
      if(req_Grecv!=MPI_REQUEST_NULL)
          MPI_Request_free(&req_Grecv);
      if(req_Gsend!=MPI_REQUEST_NULL)
          MPI_Request_free(&req_Gsend);
      MPI_Send_init(to_address(Gwork.origin())+nak0,(nak1-nak0)*sizeof(ComplexType),MPI_CHAR,
                    TG.prev_core(),1234,&(TG.TG()),&req_Gsend);
      MPI_Recv_init(to_address(Grecv.origin())+nak0,(nak1-nak0)*sizeof(ComplexType),MPI_CHAR,
                    TG.next_core(),1234,&(TG.TG()),&req_Grecv);
    }
   
    fill_n(eloc2.origin(),3*ngroups*nwalk,ComplexType(0.0)); 
    TG.local_barrier();

    MPI_Status st;

    // calculate G for local walkers
    DensityMatrix(wset,OrbMats[0],OrbMats[nspins-1],Gwork,overlaps,true,true,transposed_G_for_E_);

    qmcplusplus::Timer Time;  
  
    for(int k=0; k<ngroups; k++) {

      // wait for G from node behind you, copy to Gwork  
      if(k>0) {
        MPI_Wait(&req_Grecv,&st);
        MPI_Wait(&req_Gsend,&st);     // need to wait for Gsend in order to overwrite Gwork  
        copy_n(Grecv.origin()+nak0,(nak1-nak0),Gwork.origin()+nak0);
        TG.local_barrier();
      }

      // post send/recv messages with nodes ahead and behind you
      if(k < ngroups-1) {
        MPI_Start(&req_Gsend); 
        MPI_Start(&req_Grecv); 
      }
      // calculate your contribution of the local enery to the set of walkers in Gwork
      int q = (k+node_number)%ngroups;
      HamOp.energy(eloc2.sliced(q*nwalk,(q+1)*nwalk),
                   Gwork,0,TG.TG_local().root() && k==0);
      TG.local_barrier();

    }
    if(TG.TG().size() > 1)
      TG.TG().all_reduce_in_place_n(to_address(eloc2.origin()),3*ngroups*nwalk,std::plus<>());
    TG.local_barrier();    
    copy_n(eloc2[node_number*nwalk].origin(),3*nwalk,E.origin());
    copy_n(overlaps.origin(),nwalk,Ov.origin());
    TG.local_barrier();    
  }

  /*
   * Calculates the local energy and overlaps of all the walkers in the set and 
   * returns them in the appropriate data structures
   * NOTE: This version assumes balanced partition of all the determinants in the list
   * Depending on the number of determinants and the number of nodes in TG, it may be better
   * to keep entire determinants on nodes and distribute just the excess ones.
   */
  template<class devPsiT>  
  template<class WlkSet, class Mat, class TVec>
  void NOMSD<devPsiT>::Energy_distributed_multiDet(const WlkSet& wset, Mat&& E, TVec&& Ov)
  {
    //1. Copies SM from wset to shm buffer. 
    //2. Loop over nodes in TG
    // 2.a isend SM to next node. irecv next SM from "previous" node 
    // 2.b Calculate G and add local contribution to energy
    // 2.c wait for comms to finish
    //3. all reduce resulting energies    

    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr(make_device_ptr(E.origin()));
      auto dev_ptr_(make_device_ptr(Ov.origin()));
    }
    assert(ci.size()>1);
    bool new_shm_space=false; 
    const int node_number = TG.getLocalGroupNumber();
    const int nnodes = TG.getNGroupsPerTG();
    const int Gsize = dm_size(false); // this will also be the SlaterMat size
    const ComplexType zero(0.0);
    const int nwalk = wset.size();
    double LogOverlapFactor(wset.getLogOverlapFactor());
    // allocte space in shared memory for:
    //  i.  2 copies of SM, 
    //  ii.  1 copy of G (always compact),
    //  iii. ovlps for local walkers
    int nt = nwalk*(3*Gsize+1);
    // in case the number of walkers changes
    if(shmbuff_for_E.num_elements() < nt) {
      shmbuff_for_E.reextent(iextensions<1u>{nt}); 
      new_shm_space=true;
    }
    assert(E.dimensionality==2);
    assert(Ov.dimensionality==1);
    assert(E.size(0)==wset.size());
    assert(Ov.size(0)==wset.size());
    assert(E.size(1)==3);

    int nr=Gsize,nc=nwalk;
    if(transposed_G_for_E_) std::swap(nr,nc);
    CMatrix_ref SMwork(make_device_ptr(shmbuff_for_E.origin()),{nwalk,Gsize});
    CMatrix_ref SMrecv(SMwork.origin()+SMwork.num_elements(),{nwalk,Gsize});
    CMatrix_ref Gwork(SMrecv.origin()+SMrecv.num_elements(),{nr,nc});
    CVector_ref overlaps(Gwork.origin()+Gwork.num_elements(),iextensions<1u>{nwalk});
    // used for temporary storage in ndet loop for local calculation
    if(eloc3.size(0) != nwalk || eloc3.size(1) != 3) 
        eloc3.reextent({nwalk,3});
    // used for temporary storage of numerator in energy expression for all walkers in TG
    if(eloc2.size(0) != nnodes*nwalk || eloc2.size(1) != 3) 
        eloc2.reextent({nnodes*nwalk,3});
    // matrix view to local segment of eloc2, for wasy access later
    // split SM evenly for communication
    int nak0,nak1;
    std::tie(nak0,nak1) = FairDivideBoundary(TG.getLocalTGRank(),Gsize*nwalk,TG.getNCoresPerTG());

    if(new_shm_space) {
      if(req_SMrecv!=MPI_REQUEST_NULL)
          MPI_Request_free(&req_SMrecv);
      if(req_SMsend!=MPI_REQUEST_NULL)
          MPI_Request_free(&req_SMsend);
      // use mpi3 when ready
      MPI_Send_init(to_address(SMwork.origin())+nak0,(nak1-nak0)*sizeof(ComplexType),MPI_CHAR,
                    TG.prev_core(),2345,&(TG.TG()),&req_SMsend);
      MPI_Recv_init(to_address(SMrecv.origin())+nak0,(nak1-nak0)*sizeof(ComplexType),MPI_CHAR,
                    TG.next_core(),2345,&(TG.TG()),&req_SMrecv);
    }
   
    fill_n(eloc2.origin(),3*nnodes*nwalk,ComplexType(0.0)); 
    fill_n(Ov.origin(),nwalk,ComplexType(0));
    if(TG.TG_local().root())
      fill_n(overlaps.origin(),nwalk,ComplexType(0));

    MPI_Status st;

    // copy SM from wset. Assumes that SM data is contiguous in memory (Alpha+Beta)
    for(int i=0; i<nwalk; i++) 
      if( i%TG.TG_local().size() == TG.TG_local().rank() )
        copy_n(wset[i].SlaterMatrix(Alpha).origin(),Gsize,SMwork[i].origin());
    TG.local_barrier();

    for(int k=0; k<nnodes; k++) {

      // wait for G from node behind you, copy to Gwork  
      if(k>0) {
        MPI_Wait(&req_SMrecv,&st);
        MPI_Wait(&req_SMsend,&st);     // need to wait for Gsend in order to overwrite Gwork  
        copy_n(SMrecv.origin()+nak0,(nak1-nak0),SMwork.origin()+nak0);
        TG.local_barrier();
      }

      // post send/recv messages with nodes ahead and behind you
      if(k < nnodes-1) {
        MPI_Start(&req_SMsend); 
        MPI_Start(&req_SMrecv); 
      }

      // calculate your contribution of the local enery to the set of walkers in Gwork
      int q = (k+node_number)%nnodes;
      for(int nd=0; nd<ci.size(); nd++) {
        MixedDensityMatrix_for_E_from_SM(SMwork,Gwork,overlaps,nd,LogOverlapFactor);
        if(k==0)  
          ma::axpy(ma::conj(ci[nd]),overlaps,Ov);
        HamOp.energy(eloc3,Gwork,nd,TG.TG_local().root()&&k==0);
        for(int i=0; i<nwalk; ++i) 
          for(int k=0; k<3; k++)
// TO GPU
            eloc2[q*nwalk+i][k] += ma::conj(ci[nd])*overlaps[i]*eloc3[i][k];
      }
      TG.local_barrier();

    }
    if(TG.TG().size() > 1)
      TG.TG().all_reduce_in_place_n(to_address(eloc2.origin()),3*nnodes*nwalk,std::plus<>());
    TG.local_barrier();    
    auto elocal = eloc2.sliced(node_number*nwalk,(node_number+1)*nwalk);
    for(int i=0; i<nwalk; i++) {
// TO GPU
      E[i][0] = elocal[i][0]/Ov[i];
      E[i][1] = elocal[i][1]/Ov[i];
      E[i][2] = elocal[i][2]/Ov[i];
    }
    TG.local_barrier();    
  }

  /* 
   * Computes the mixed density matrix of a single given determinant in the trial wave function.
   * Intended to be used in combination with the energy evaluation routine.
   * G and Ov are expected to be in shared memory.
   * Simple round-robin is used. 
   */
  template<class devPsiT>  
  template<class MatSM, class MatG, class TVec>
  void NOMSD<devPsiT>::MixedDensityMatrix_for_E_from_SM(const MatSM& SM, MatG&& G, TVec&& Ov, int nd, double LogOverlapFactor) 
  {
    using std::fill_n;
    using std::copy_n;
    auto Gsize = size_t(dm_size(false));
    const int nw = SM.size(0); 
    assert(G.stride(1)==1);
    assert(Ov.stride(0)==1);
    if(transposed_G_for_E_) 
      assert(G.size(0) == nw && G.size(1) == size_t(dm_size(false)));
    else
      assert(G.size(1) == nw && G.size(0) == size_t(dm_size(false)));
    assert(Ov.size() >= nw);  
    assert(SM.size(1) == Gsize);
    // to force synchronization before modifying structures in SHM
    TG.local_barrier();
    if(TG.TG_local().root()) {
      fill_n(Ov.origin(),nw,0);
      fill_n(G.origin(),G.num_elements(),ComplexType(0.0));
    }
    TG.local_barrier();
    if(localGbuff.size() < Gsize)
      localGbuff.reextent(iextensions<1u>{Gsize});
    if(walker_type != COLLINEAR) {

      auto Gdims = dm_dims(false,Alpha);
      CMatrix_ref G2D_(localGbuff.origin(),{Gdims.first,Gdims.second});
      CVector_ref G1D_(G2D_.origin(),iextensions<1u>{Gsize});
      // notice interchange of dimensions
      CTensor_cref A(SM.origin(),{nw,Gdims.second,Gdims.first});

      for(int iw=0; iw<nw; ++iw) {
        if(iw%TG.TG_local().size() != TG.TG_local().rank()) continue;
        Ov[iw] = SDetOp.MixedDensityMatrix(OrbMats[nd],A[iw],G2D_,LogOverlapFactor,true);
        if(walker_type==CLOSED) Ov[iw] *= ComplexType(Ov[iw]);
        if(transposed_G_for_E_) 
          G[iw] = G1D_; 
        else
          G(G.extension(0),iw) = G1D_; 
      }

    } else {

      // store overlaps locally to be able to split alpha/beta pairs
      if(ovlp2.size(0) < 2*nw) ovlp2.reextent(iextensions<1u>{2*nw});
      fill_n(ovlp2.origin(),2*nw,ComplexType(0.0));
      auto GAdims = dm_dims(false,Alpha);
      auto GBdims = dm_dims(false,Beta);
      CMatrix_ref GA2D_(localGbuff.origin(),{GAdims.first,GAdims.second});
      CMatrix_ref GB2D_(GA2D_.origin()+GA2D_.num_elements(),
                                   {GBdims.first,GBdims.second});
      CVector_ref GA1D_(GA2D_.origin(),iextensions<1u>{GA2D_.num_elements()});
      CVector_ref GB1D_(GB2D_.origin(),iextensions<1u>{GB2D_.num_elements()});

      for(int iw=0; iw<2*nw; ++iw) {
        if(iw%TG.TG_local().size() != TG.TG_local().rank()) continue;
        
        if(iw%2==0) {
          // notice interchange of dimensions
          CMatrix_cref M(SM[iw/2].origin(),{GAdims.second,GAdims.first});
          ovlp2[iw] = SDetOp.MixedDensityMatrix(OrbMats[2*nd],M,GA2D_,LogOverlapFactor,true);
          if(transposed_G_for_E_) 
            G[iw/2].sliced(0,GAdims.first*GAdims.second) = GA1D_;
          else
            G({0,GAdims.first*GAdims.second},iw/2) = GA1D_;
        } else {
          // notice interchange of dimensions
          CMatrix_cref M(SM[iw/2].origin()+GAdims.first*GAdims.second,
                                               {GBdims.second,GBdims.first});
          ovlp2[iw] = SDetOp.MixedDensityMatrix(OrbMats[2*nd+1],M,GB2D_,LogOverlapFactor,true);
          if(transposed_G_for_E_) 
            G[iw/2].sliced(GAdims.first*GAdims.second,G.size(1)) = GB1D_;
          else
            G({GAdims.first*GAdims.second,G.size(0)},iw/2) = GB1D_;
        }
      }
      // CHECK: I don't need all_reduce here, but the current version of mpi3 
      //        fails if I use reduce_in_place_n  
      if(TG.TG_local().size() > 1 )  
        TG.TG_local().all_reduce_in_place_n(to_address(ovlp2.origin()),2*nw,std::plus<>());  
      if(TG.TG_local().root())
        for(int iw=0; iw<nw; ++iw)
          Ov[iw] = ovlp2[2*iw]*ovlp2[2*iw+1];
    }
    TG.local_barrier();
  }

  /* 
   * Computes the density matrix for a given reference. 
   * G and Ov are expected to be in shared memory.
   * Simple round-robin is used. 
   */
  template<class devPsiT>  
  template<class WlkSet, class MatA, class MatB, class MatG, class TVec>
  void NOMSD<devPsiT>::DensityMatrix_shared(const WlkSet& wset, MatA&& RefA, MatB&& RefB,
                            MatG&& G, TVec&& Ov, bool herm, bool compact, bool transposed) 
  {
    assert(G.stride(1)==1);
    assert(Ov.stride(0)==1);
    if(transposed) 
      assert(G.size(0) == wset.size() && G.size(1) == size_t(dm_size(not compact)));
    else
      assert(G.size(1) == wset.size() && G.size(0) == size_t(dm_size(not compact)));
    const int nw = wset.size(); 
    assert(Ov.size() >= nw);  
    // to force synchronization before modifying structures in SHM
    TG.local_barrier();
    fill_n(Ov.origin(),Ov.num_elements(),0);
    fill_n(G.origin(),G.num_elements(),ComplexType(0.0));
    TG.local_barrier();
    double LogOverlapFactor(wset.getLogOverlapFactor());
    auto Gsize = size_t(dm_size(not compact));
    if(localGbuff.size() < Gsize)
      localGbuff.reextent(iextensions<1u>{Gsize});
      
    if(walker_type != COLLINEAR) {

      if(herm)
        assert( RefA.size(0) == dm_dims(false,Alpha).first &&
                RefA.size(1) == dm_dims(false,Alpha).second );
      else
        assert( RefA.size(1) == dm_dims(false,Alpha).first && 
                RefA.size(0) == dm_dims(false,Alpha).second );

      auto Gdims = dm_dims(not compact,Alpha);
      CMatrix_ref G2D_(localGbuff.origin(),{Gdims.first,Gdims.second});
      CVector_ref G1D_(G2D_.origin(),iextensions<1u>{G2D_.num_elements()});

      for(int iw=0; iw<nw; ++iw) {
        if(iw%TG.TG_local().size() != TG.TG_local().rank()) continue;
        Ov[iw] = SDetOp.MixedDensityMatrix(RefA,wset[iw].SlaterMatrix(Alpha),G2D_,
                                             LogOverlapFactor,compact,herm);
        if(walker_type==CLOSED) Ov[iw] *= ComplexType(Ov[iw]);
        if(transposed) 
          G[iw] = G1D_; 
        else
          G(G.extension(0),iw) = G1D_;
      }

    } else {

      if(herm)
        assert( RefA.size(0) == dm_dims(false,Alpha).first &&
                RefA.size(1) == dm_dims(false,Alpha).second );
      else
        assert( RefA.size(1) == dm_dims(false,Alpha).first &&
                RefA.size(0) == dm_dims(false,Alpha).second );
      if(herm)
        assert( RefB.size(0) == dm_dims(false,Beta).first &&
                RefB.size(1) == dm_dims(false,Beta).second );
      else
        assert( RefB.size(1) == dm_dims(false,Beta).first &&
                RefB.size(0) == dm_dims(false,Beta).second );

      if(ovlp2.size(0) < 2*nw) ovlp2.reextent(iextensions<1u>{2*nw});
      fill_n(ovlp2.origin(),2*nw,ComplexType(0.0));
      auto GAdims = dm_dims(not compact,Alpha);
      auto GBdims = dm_dims(not compact,Beta);
      CMatrix_ref GA2D_(localGbuff.origin(),{GAdims.first,GAdims.second});
      CMatrix_ref GB2D_(GA2D_.origin()+GA2D_.num_elements(),
                                   {GBdims.first,GBdims.second});
      CVector_ref GA1D_(GA2D_.origin(),iextensions<1u>{GA2D_.num_elements()});
      CVector_ref GB1D_(GB2D_.origin(),iextensions<1u>{GB2D_.num_elements()});

      for(int iw=0; iw<2*nw; ++iw) {
        if(iw%TG.TG_local().size() != TG.TG_local().rank()) continue;
        
        if(iw%2==0) {
          ovlp2[iw] = SDetOp.MixedDensityMatrix(RefA,wset[iw/2].SlaterMatrix(Alpha),
                               GA2D_,LogOverlapFactor,compact,herm);
          if(transposed) 
            G[iw/2].sliced(0,GAdims.first*GAdims.second) = GA1D_;
          else  
            G({0,GAdims.first*GAdims.second},iw/2) = GA1D_;
        } else {
          ovlp2[iw] = SDetOp.MixedDensityMatrix(RefB,wset[iw/2].SlaterMatrix(Beta),
                                 GB2D_,LogOverlapFactor,compact,herm);
          if(transposed) 
            G[iw/2].sliced(GAdims.first*GAdims.second,G.size(1)) = GB1D_;
          else  
            G({GAdims.first*GAdims.second,G.size(0)},iw/2) = GB1D_;
        }
      }
      if(TG.TG_local().size() > 1 )
        TG.TG_local().all_reduce_in_place_n(to_address(ovlp2.origin()),2*nw,std::plus<>());
      if(TG.TG_local().root())
        for(int iw=0; iw<nw; ++iw)
          Ov[iw] = ovlp2[2*iw]*ovlp2[2*iw+1];
    }
    TG.local_barrier();
  }

  template<class devPsiT>  
  template<class WlkSet, class MatA, class MatB, class MatG, class TVec>
  void NOMSD<devPsiT>::DensityMatrix_batched(const WlkSet& wset, MatA&& RefA, MatB&& RefB, 
                            MatG&& G, TVec&& Ov, bool herm, bool compact, bool transposed) 
  {
    if(TG.TG_local().size() > 1)
      APP_ABORT(" Error: Batched routine called with TG.TG_local().size() > 1 \n");
    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr_(make_device_ptr(Ov.origin()));
      auto dev_ptr(make_device_ptr(G.origin()));
    }

    assert(G.stride(1)==1);
    assert(Ov.stride(0)==1);
    if(transposed) 
      assert(G.size(0) == wset.size() && G.size(1) == size_t(dm_size(not compact)));
    else
      assert(G.size(1) == wset.size() && G.size(0) == size_t(dm_size(not compact)));
    const int nw = wset.size(); 
    int nbatch__ = std::min(nw,(nbatch<0?nw:nbatch));
    assert(Ov.size() >= nw);  
    // to force synchronization before modifying structures in SHM
    TG.local_barrier();
    fill_n(Ov.origin(),nw,0);
    fill_n(G.origin(),G.num_elements(),ComplexType(0.0));
    if(ovlp2.size(0) < 2*nbatch__) ovlp2.reextent(iextensions<1u>{2*nbatch__});
    fill_n(ovlp2.origin(),ovlp2.num_elements(),ComplexType(0.0));
    if(G.size(1) != G.stride(0))
      APP_ABORT(" Error: FIX FIX FIX need strided fill_n\n");
    double LogOverlapFactor(wset.getLogOverlapFactor());
    stdCVector hvec(iextensions<1u>{2*nbatch__});
    TG.local_barrier();
    auto GAdims = dm_dims(not compact,Alpha);
    auto Gsize = size_t(GAdims.first*GAdims.second);
    if(localGbuff.size() < nbatch__*Gsize+1)
      localGbuff.reextent(iextensions<1u>{nbatch__*Gsize+1});
    std::vector<CMatrix_ref> Ai;
    Ai.reserve(nbatch__);
    std::vector<decltype(std::addressof(RefA))> Oia;
    Oia.reserve(nw);
    std::vector<decltype(std::addressof(RefA))> Oib;
    Oib.reserve(nw);
      
    if(walker_type != COLLINEAR) {

      if(herm)
        assert( RefA.size(0) == dm_dims(false,Alpha).first && 
                RefA.size(1) == dm_dims(false,Alpha).second );
      else
        assert( RefA.size(1) == dm_dims(false,Alpha).first && 
                RefA.size(0) == dm_dims(false,Alpha).second );

      CTensor_ref G3D_(localGbuff.origin(),{nbatch__,GAdims.first,GAdims.second});
      CMatrix_ref G2D_(G3D_.origin(),iextensions<2u>{nbatch__,GAdims.first*GAdims.second});

      for(int iw=0; iw<nw; iw+=nbatch__) {
        int nb = std::min(nbatch__,nw-iw);
        Ai.clear();
        for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Alpha));
        Oia.clear();
        for(int ni=0; ni<nb; ni++) Oia.emplace_back(std::addressof(RefA));
        SDetOp.BatchedMixedDensityMatrix(Oia,Ai,
              G3D_.sliced(0,nb),LogOverlapFactor,ovlp2.sliced(0,nb),compact,herm);
        copy_n(ovlp2.origin(),nb,hvec.origin());
        for(int ib=0; ib<nb; ++ib) {
          ComplexType ov(hvec[ib]);
          if(walker_type==CLOSED) ov *= ov;
          if(transposed) {
            ma::copy(G2D_[ib],G[iw+ib]);
          } else {
            ma::copy(G2D_[ib],G(G.extension(0),iw+ib));
          }
          Ov[iw+ib] = ov; 
        }
      }

    } else {

      if(herm)
        assert( RefA.size(0) == dm_dims(false,Alpha).first && 
                RefA.size(1) == dm_dims(false,Alpha).second );
      else
        assert( RefA.size(1) == dm_dims(false,Alpha).first && 
                RefA.size(0) == dm_dims(false,Alpha).second );
      if(herm)
        assert( RefB.size(0) == dm_dims(false,Beta).first && 
                RefB.size(1) == dm_dims(false,Beta).second );
      else
        assert( RefB.size(1) == dm_dims(false,Beta).first && 
                RefB.size(0) == dm_dims(false,Beta).second );

      auto GBdims = dm_dims(not compact,Beta);
      CTensor_ref GA3D_(localGbuff.origin(),{nbatch__,GAdims.first,GAdims.second});
      CMatrix_ref GA2D_(GA3D_.origin(),iextensions<2u>{nbatch__,GAdims.first*GAdims.second});
      CTensor_ref GB3D_(localGbuff.origin(),{nbatch__,GBdims.first,GBdims.second});
      CMatrix_ref GB2D_(GB3D_.origin(),iextensions<2u>{nbatch__,GBdims.first*GBdims.second});

      for(int iw=0; iw<nw; iw+=nbatch__) {
        int nb = std::min(nbatch__,nw-iw);
        Ai.clear();
        for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Alpha));
        Oia.clear();
        for(int ni=0; ni<nb; ni++) Oia.emplace_back(std::addressof(RefA));
        SDetOp.BatchedMixedDensityMatrix(Oia,Ai,
                          GA3D_.sliced(0,nb),LogOverlapFactor,ovlp2.sliced(0,nb),compact,herm);
        for(int ib=0; ib<nb; ++ib) {
          if(transposed) {
            ma::copy(GA2D_[ib],G[iw+ib].sliced(0,GAdims.first*GAdims.second));
          } else {
            ma::copy(GA2D_[ib],G({0,GAdims.first*GAdims.second},iw+ib));
          }
          Ov[iw+ib] = ovlp2[ib];
        }

        Ai.clear();
        for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Beta));
        Oib.clear();
        for(int ni=0; ni<nb; ni++) Oib.emplace_back(std::addressof(RefB));
        SDetOp.BatchedMixedDensityMatrix(Oib,Ai,
                          GB3D_.sliced(0,nb),LogOverlapFactor,ovlp2.sliced(0,nb),compact,herm);
        for(int ib=0; ib<nb; ++ib) {
          if(transposed) {
            ma::copy(GB2D_[ib],G[iw+ib].sliced(GAdims.first*GAdims.second,G.size(1)));
          } else {
            ma::copy(GB2D_[ib],G({GAdims.first*GAdims.second,G.size(0)},iw+ib));
          }
          Ov[iw+ib] *= ovlp2[ib];
        }
      }
    }
    TG.local_barrier();
  }

  template<class devPsiT>  
  template<class MatA, class MatB, class MatG, class TVec>
  void NOMSD<devPsiT>::DensityMatrix_shared(std::vector<MatA>& Left, std::vector<MatB>& Right, 
                            std::vector<MatG>& G, TVec&& Ov, double LogOverlapFactor, 
                            bool herm, bool compact)
  {
    const int nw = Left.size(); 
    assert(Right.size() == nw);
    assert(G.size() == nw);
    assert(Ov.size() >= nw);  
    // to force synchronization before modifying structures in SHM
    TG.local_barrier();
    for(int iw=0; iw<nw; ++iw) {
      if(iw%TG.TG_local().size() != TG.TG_local().rank()) continue;
      Ov[iw] = SDetOp.MixedDensityMatrix( Left[iw], Right[iw], G[iw], 
                                           LogOverlapFactor,compact,herm);
    }
    TG.local_barrier();
  }

  template<class devPsiT>  
  template<class MatA, class MatB, class MatG, class TVec>
  void NOMSD<devPsiT>::DensityMatrix_batched(std::vector<MatA>& Left, std::vector<MatB>& Right, 
                            std::vector<MatG>& G, TVec&& Ov, double LogOverlapFactor, 
                            bool herm, bool compact)
  {
    if(TG.TG_local().size() > 1)
      APP_ABORT(" Error: Batched routine called with TG.TG_local().size() > 1 \n");
    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr_(make_device_ptr(Ov.origin()));
      auto dev_ptr(make_device_ptr(G[0].origin()));
    }
    const int nw = Left.size(); 
    assert(Right.size() == nw);
    assert(G.size() == nw);
    assert(Ov.size() >= nw);  
    int nbatch__ = std::min(nw,(nbatch<0?nw:nbatch));
    TG.local_barrier();
    std::vector<MatA> Li;
    std::vector<MatB> Ri;
    std::vector<MatG> Gi;
    Li.reserve(nbatch__);
    Ri.reserve(nbatch__);
    Gi.reserve(nbatch__);
    if(ovlp2.size(0) < nbatch__) ovlp2.reextent(iextensions<1u>{nbatch__});
    fill_n(ovlp2.origin(),ovlp2.num_elements(),ComplexType(0.0));
      
    for(int iw=0; iw<nw; iw+=nbatch__) {
      int nb = std::min(nbatch__,nw-iw);
      Li.clear();
      Ri.clear();
      Gi.clear();
      for(int ni=0; ni<nb; ni++) Li.emplace_back(Left[iw+ni]);
      for(int ni=0; ni<nb; ni++) Ri.emplace_back(Right[iw+ni]);
      for(int ni=0; ni<nb; ni++) Gi.emplace_back(G[iw+ni]);
      SDetOp.BatchedDensityMatrices(Li,Ri,Gi,
            LogOverlapFactor,ovlp2.sliced(0,nb),compact,herm);
      // in case Ov is in a different memory space
      ma::copy(ovlp2.sliced(0,nb),Ov.sliced(iw,iw+nb));
    }
    TG.local_barrier();
  }

  /*
   * This routine has (potentially) considerable overhead if either the number of determinants
   *   or the number of walkers changes.   
   * G is assumed to be in shared memory
   * 1. calculate G(iw,nd) in a local buffer  
   * 2. accumulate the numerator in G with mutex
   * 3. 
   * Ov is assumed to be local to the core
   */ 
  template<class devPsiT>  
  template<class WlkSet, class MatG, class TVec>
  void NOMSD<devPsiT>::MixedDensityMatrix_shared(const WlkSet& wset, MatG&& G, TVec&& Ov, bool compact, bool transpose)
  {
    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr_(make_device_ptr(Ov.origin()));
      auto dev_ptr(make_device_ptr(G.origin()));
    }

    assert(G.stride(1)==1);
    assert(Ov.stride(0)==1);
    if(transpose)
      assert(G.size(0) == wset.size() && G.size(1) == size_t(dm_size(not compact)));
    else
      assert(G.size(1) == wset.size() && G.size(0) == size_t(dm_size(not compact)));
    const int nw = wset.size(); 
    const int ndet = ci.size();
    double LogOverlapFactor(wset.getLogOverlapFactor());
    assert(Ov.size() >= nw);  
    fill_n(Ov.origin(),nw,0);
    if(TG.TG_local().root() ) {
      // need strided fill_n????
      if(G.size(1) != G.stride(0))
        APP_ABORT(" Error: FIX FIX FIX need strided fill_n\n"); 
      fill_n(G.origin(),G.num_elements(),ComplexType(0.0));
    }
    TG.local_barrier();
    auto Gsize = size_t(dm_size(not compact));
    if(localGbuff.size() < Gsize+1)
      localGbuff.reextent(iextensions<1u>{Gsize+1});
    auto wlk_dims = wset.walker_dims();
    if(walker_type != COLLINEAR) {

      auto Gdims = dm_dims(not compact,Alpha);
      CMatrix_ref G2D_(localGbuff.origin(),{Gdims.first,Gdims.second});
      CVector_ref G1D_(G2D_.origin(),iextensions<1u>{G2D_.num_elements()});

      const int ntasks_percore = (nw*ndet)/TG.getNCoresPerTG();
      const int ntasks_total_serial = ntasks_percore*TG.getNCoresPerTG();
      const int nextra = nw*ndet - ntasks_total_serial; 

      // each processor does ntasks_percore_serial overlaps serially
      const int tk0 = TG.getLocalTGRank()*ntasks_percore; 
      const int tkN = (TG.getLocalTGRank()+1)*ntasks_percore; 

      // task_w_d = = wlk_w*ndet + d 
      for(int tk=tk0; tk<tkN; ++tk) {
        int iw = tk/ndet;
        int nd = tk%ndet;
        ComplexType ov = SDetOp.MixedDensityMatrix(OrbMats[nd],wset[iw].SlaterMatrix(Alpha),
                                       G2D_,LogOverlapFactor,compact);
        if(walker_type==CLOSED) ov *= ov;
        ov *= ma::conj(ci[nd]);
        // if the overhead of the mutex is too much, invert the loop order (make iw the fast index)
        // and have a mutex for each walker index 
        if(transpose) {
          std::lock_guard<shared_mutex> guard(*mutex);
          ma::axpy(ov,G1D_,G[iw]);   
        } else {
          std::lock_guard<shared_mutex> guard(*mutex);
          ma::axpy(ov,G1D_,G(G.extension(0),iw));   
        }
        Ov[iw] += ov; 
      }

      // all remaining overlaps are performed in parallel with blocks of cores
      // partition processors in nextra groups
      if(nextra > 0) {
        // check if new communicator is necessary
        if( last_number_extra_tasks != nextra ) {
          last_number_extra_tasks = nextra;
          for(int n=0; n<nextra; n++) {
            int n0,n1;
            std::tie(n0,n1) = FairDivideBoundary(n,TG.getNCoresPerTG(),nextra);
            if(TG.getLocalTGRank()>=n0 && TG.getLocalTGRank()<n1) {
              last_task_index = n;
              break;
            }
          }   
          // first setup
          local_group_comm = std::move(shared_communicator(TG.TG_local().split(last_task_index,TG.TG_local().rank()))); 
          shmbuff_for_G = std::move(std::make_unique<mpi3CVector>(iextensions<1u>{dm_size(true)},
                                                shared_allocator<ComplexType>{local_group_comm}));
        } 
        if(last_task_index < 0 || last_task_index > nextra) 
          APP_ABORT("Error: Problems in NOMSD<devPsiT>::Overlap(WSet,Ov)");
        {
          boost::multi::array_ref<ComplexType,2> G2D_2(to_address(shmbuff_for_G->origin()),
                                                 {Gdims.first,Gdims.second});
          boost::multi::array_ref<ComplexType,1> G1D_2(to_address(shmbuff_for_G->origin()),iextensions<1u>{Gsize});
          int iw = (last_task_index+ntasks_total_serial)/ndet;
          int nd = (last_task_index+ntasks_total_serial)%ndet;
          ComplexType ov = SDetOp.MixedDensityMatrix(OrbMats[nd],wset[iw].SlaterMatrix(Alpha),
                                G2D_2,LogOverlapFactor,local_group_comm,compact);
          if(walker_type==CLOSED) ov *= ov;
          ov *= ma::conj(ci[nd]);

          if(local_group_comm.rank()==0) {
            if(transpose) {
              std::lock_guard<shared_mutex> guard(*mutex);
              ma::axpy(ov,G1D_2,G[iw]);
            } else {
              std::lock_guard<shared_mutex> guard(*mutex);
              ma::axpy(ov,G1D_2,G(G.extension(0),iw));
            }
            Ov[iw] += ov; 
          }
        }
      }

    } else {

      auto GAdims = dm_dims(not compact,Alpha);
      auto GBdims = dm_dims(not compact,Beta);
      CMatrix_ref GA2D_(localGbuff.origin(),{GAdims.first,GAdims.second});
      CMatrix_ref GB2D_(GA2D_.origin()+GA2D_.num_elements(),
                                   {GBdims.first,GBdims.second});
      CVector_ref GA1D_(GA2D_.origin(),iextensions<1u>{GA2D_.num_elements()});
      CVector_ref GB1D_(GB2D_.origin(),iextensions<1u>{GB2D_.num_elements()});

      const int ntasks_percore = (nw*ndet)/TG.getNCoresPerTG();
      const int ntasks_total_serial = ntasks_percore*TG.getNCoresPerTG();
      const int nextra = nw*ndet - ntasks_total_serial;

      // each processor does ntasks_percore_serial overlaps serially
      const int tk0 = TG.getLocalTGRank()*ntasks_percore;
      const int tkN = (TG.getLocalTGRank()+1)*ntasks_percore;


      // task_w_d = = wlk_w*ndet + d 
      for(int tk=tk0; tk<tkN; ++tk) {
        int iw = tk/ndet;
        int nd = tk%ndet;
        ComplexType ov = SDetOp.MixedDensityMatrix(OrbMats[2*nd],wset[iw].SlaterMatrix(Alpha),
                                        GA2D_,LogOverlapFactor,compact);
        ov *= SDetOp.MixedDensityMatrix(OrbMats[2*nd+1],wset[iw].SlaterMatrix(Beta),
                                         GB2D_,LogOverlapFactor,compact);
        ov *= ma::conj(ci[nd]);
        if(transpose) {
          std::lock_guard<shared_mutex> guard(*mutex);
          ma::axpy(ov,GA1D_,G[iw].sliced(0,GAdims.first*GAdims.second));
          ma::axpy(ov,GB1D_,G[iw].sliced(GAdims.first*GAdims.second,G.size(1)));
        } else {
          std::lock_guard<shared_mutex> guard(*mutex);
          ma::axpy(ov,GA1D_,G({0,GAdims.first*GAdims.second},iw));
          ma::axpy(ov,GB1D_,G({GAdims.first*GAdims.second,G.size(0)},iw));
        }
        Ov[iw] += ov;
      }

      // all remaining overlaps are performed in parallel with blocks of cores
      // partition processors in nextra groups
      if(nextra > 0) {
        // check if new communicator is necessary
        if( last_number_extra_tasks != nextra ) {
          last_number_extra_tasks = nextra;
          for(int n=0; n<nextra; n++) {
            int n0,n1;
            std::tie(n0,n1) = FairDivideBoundary(n,TG.getNCoresPerTG(),nextra);
            if(TG.getLocalTGRank()>=n0 && TG.getLocalTGRank()<n1) {
              last_task_index = n;
              break;
            }
          }
          // first setup
          //local_group_comm = std::move(std::make_unique<shared_communicator>(TG.TG_local().split(last_task_index))); 
          local_group_comm = std::move(shared_communicator(TG.TG_local().split(last_task_index,TG.TG_local().rank()))); 
          shmbuff_for_G = std::move(std::make_unique<mpi3CVector>(iextensions<1u>{dm_size(true)},
                                                shared_allocator<ComplexType>{local_group_comm}));
        }
        if(last_task_index < 0 || last_task_index > nextra)
          APP_ABORT("Error: Problems in NOMSD<devPsiT>::Overlap(WSet,Ov)");
        {
          boost::multi::array_ref<ComplexType,2> GA2D_2(to_address(shmbuff_for_G->origin()),
                               {GAdims.first,GAdims.second});
          boost::multi::array_ref<ComplexType,2> GB2D_2(to_address(shmbuff_for_G->origin())+
                               GAdims.first*GAdims.second,{GBdims.first,GBdims.second});
          boost::multi::array_ref<ComplexType,1> GA1D_2(to_address(shmbuff_for_G->origin()),
                               iextensions<1u>{GAdims.first*GAdims.second});
          boost::multi::array_ref<ComplexType,1> GB1D_2(to_address(shmbuff_for_G->origin())+
                               GAdims.first*GAdims.second,iextensions<1u>{GBdims.first*GBdims.second});
          int task = (last_task_index+ntasks_total_serial);
          int iw = task/ndet;
          int nd = task%ndet;
          ComplexType ov = SDetOp.MixedDensityMatrix(OrbMats[2*nd],wset[iw].SlaterMatrix(Alpha),
                             GA2D_2,LogOverlapFactor,local_group_comm,compact);
          ov *= SDetOp.MixedDensityMatrix(OrbMats[2*nd+1],wset[iw].SlaterMatrix(Beta),
                             GB2D_2,LogOverlapFactor,local_group_comm,compact);
          ov *= ma::conj(ci[nd]);
          if(local_group_comm.root()) {
            if(transpose) {
              std::lock_guard<shared_mutex> guard(*mutex);
              ma::axpy(ov,GA1D_2,G[iw].sliced(0,GAdims.first*GAdims.second));
              ma::axpy(ov,GB1D_2,G[iw].sliced(GAdims.first*GAdims.second,G.size(1)));
            } else {
              std::lock_guard<shared_mutex> guard(*mutex);
              ma::axpy(ov,GA1D_2,G({0,GAdims.first*GAdims.second},iw));
              ma::axpy(ov,GB1D_2,G({GAdims.first*GAdims.second,G.size(0)},iw));
            }
            Ov[iw] += ov;
          }
        }
      }
    }
    // normalize G
    if(TG.TG_local().size() > 1 )
      TG.TG_local().all_reduce_in_place_n(to_address(Ov.origin()),nw,std::plus<>());
    if(transpose) {
      for(size_t iw=0; iw<G.size(0); ++iw) 
        if( iw%TG.TG_local().size() == TG.TG_local().rank() ) {
          auto ov = ComplexType(1.0,0.0)/Ov[iw];
// to GPU: write routine to do y[i,j] = y[i,j] op x[i], where op can be +,-,*,/
// operation_over_rows???, scale_rows???
          ma::scal(ov,G[iw]);
        }
    } else {
      int ik0, ikN;
      std::tie(ik0,ikN) = FairDivideBoundary(TG.TG_local().rank(),int(G.size(0)),TG.TG_local().size());
      using ma::term_by_term_matrix_vector;
      term_by_term_matrix_vector(ma::TOp_DIV,1,ikN-ik0,G.size(1),
                                 make_device_ptr(G[ik0].origin()),G.stride(0),
                                 make_device_ptr(Ov.origin()),Ov.stride(0));
    }
    TG.local_barrier();
  } 

  // Batched version of MixedDensityMatrix
  template<class devPsiT>  
  template<class WlkSet, class MatG, class TVec>
  void NOMSD<devPsiT>::MixedDensityMatrix_batched(const WlkSet& wset, MatG&& G, TVec&& Ov, bool compact, bool transpose)
  {
    if(TG.TG_local().size() > 1) 
      APP_ABORT(" Error: Batched routine called with TG.TG_local().size() > 1 \n");
    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr_(make_device_ptr(Ov.origin()));
      auto dev_ptr(make_device_ptr(G.origin()));
    }

    assert(G.stride(1)==1);
    assert(Ov.stride(0)==1);
    if(transpose)
      assert(G.size(0) == wset.size() && G.size(1) == size_t(dm_size(not compact)));
    else
      assert(G.size(1) == wset.size() && G.size(0) == size_t(dm_size(not compact)));
    const int nw = wset.size(); 
    const int ndet = ci.size();
    int nbatch__ = std::min(nw*ndet,(nbatch<0?ndet*nw:nbatch));
    // do batches of determinants to simplify code
    double LogOverlapFactor(wset.getLogOverlapFactor());
    assert(Ov.size() >= nw);  
    fill_n(Ov.origin(),nw,0);
    if(ovlp2.size(0) < 2*nbatch__) ovlp2.reextent(iextensions<1u>{2*nbatch__});
    fill_n(ovlp2.origin(),ovlp2.num_elements(),ComplexType(0.0));
    // need strided fill_n????
    if(G.size(1) != G.stride(0))
      APP_ABORT(" Error: FIX FIX FIX need strided fill_n\n"); 
    fill_n(G.origin(),G.num_elements(),ComplexType(0.0));
    stdCVector hvec(iextensions<1u>{2*nbatch__});
    stdCVector Ovl(iextensions<1u>{nw});
    fill_n(Ovl.origin(),nw,ComplexType(0.0));
    TG.local_barrier();
    auto Gsize = size_t(dm_size(not compact));
    if(localGbuff.size() < nbatch__*Gsize+1)
      localGbuff.reextent(iextensions<1u>{nbatch__*Gsize+1});
    std::vector<CMatrix_ref> Ai;
    Ai.reserve(nbatch__);
    std::vector<devPsiT*> Oi;
    Oi.reserve(nbatch__);

    if(walker_type != COLLINEAR) {

      auto GAdims = dm_dims(not compact,Alpha); 
      CTensor_ref G3D_(localGbuff.origin(),{nbatch__,GAdims.first,GAdims.second});
      CMatrix_ref G2D_(G3D_.origin(),iextensions<2u>{nbatch__,GAdims.first*GAdims.second});
      std::vector<pointer> Gx;
      std::vector<pointer> Gy;
      Gx.reserve(nbatch__);
      Gy.reserve(nbatch__);

      // b = idet*nw + iw
      // iw = (b0+ni)%nw
      // idet = (b0+ni)/nw
      for(int b0=0; b0<ndet*nw; b0+=nbatch__) {
        int nb = std::min(nbatch__,nw*ndet-b0);
        Ai.clear();
        Oi.clear();
        for(int ni=0; ni<nb; ni++) 
          Ai.emplace_back(wset[(b0+ni)%nw].SlaterMatrix(Alpha));
        for(int ni=0; ni<nb; ni++) 
          Oi.emplace_back(std::addressof(OrbMats[(b0+ni)/nw]));
        SDetOp.BatchedMixedDensityMatrix(Oi,Ai,
              G3D_.sliced(0,nb),LogOverlapFactor,ovlp2.sliced(0,nb),compact);
        copy_n(ovlp2.origin(),nb,hvec.origin());
        Gx.clear();
        Gy.clear();
        for(int ib=0; ib<nb; ++ib) {
          int idet = (b0+ib)/nw;
          int iw = (b0+ib)%nw;
          if(walker_type==CLOSED) hvec[ib] *= hvec[ib];
          hvec[ib] *= ma::conj(ci[idet]);
          Gx.emplace_back(G2D_[ib].origin());
          if(transpose) 
            Gy.emplace_back(G[iw].origin());
          else 
            Gy.emplace_back(G.origin()+iw);
          Ovl[iw] += hvec[ib];
        }
        using ma::sumGwBatched;
        if(transpose) 
          sumGwBatched(GAdims.first*GAdims.second,hvec.data(),Gx.data(),1,Gy.data(),1,b0,nw,nb);
        else 
          sumGwBatched(GAdims.first*GAdims.second,hvec.data(),Gx.data(),1,Gy.data(),nw,b0,nw,nb);
      }  

    } else {

      auto GAdims = dm_dims(not compact,Alpha); 
      auto GBdims = dm_dims(not compact,Beta);
      CTensor_ref GA3D_(localGbuff.origin(),{nbatch__,GAdims.first,GAdims.second});
      CMatrix_ref GA2D_(GA3D_.origin(),iextensions<2u>{nbatch__,GAdims.first*GAdims.second});
      CTensor_ref GB3D_(GA3D_.origin()+GA3D_.num_elements(),{nbatch__,GBdims.first,GBdims.second});
      CMatrix_ref GB2D_(GB3D_.origin(),iextensions<2u>{nbatch__,GBdims.first*GBdims.second});
      std::vector<pointer> Gxa;
      std::vector<pointer> Gya;
      std::vector<pointer> Gxb;
      std::vector<pointer> Gyb;
      Gxa.reserve(nbatch__);
      Gya.reserve(nbatch__);
      Gxb.reserve(nbatch__);
      Gyb.reserve(nbatch__);

      // b = idet*nw + iw
      // iw = (b0+ni)%nw
      // idet = (b0+ni)/nw
      for(int b0=0; b0<ndet*nw; b0+=nbatch__) {
        int nb = std::min(nbatch__,nw*ndet-b0);
        Ai.clear();
        Oi.clear();
        for(int ni=0; ni<nb; ni++)
          Ai.emplace_back(wset[(b0+ni)%nw].SlaterMatrix(Alpha));
        for(int ni=0; ni<nb; ni++)
          Oi.emplace_back(std::addressof(OrbMats[2*((b0+ni)/nw)]));
        SDetOp.BatchedMixedDensityMatrix(Oi,Ai,
              GA3D_.sliced(0,nb),LogOverlapFactor,ovlp2.sliced(0,nb),compact);
        Ai.clear();
        Oi.clear();
        for(int ni=0; ni<nb; ni++)
          Ai.emplace_back(wset[(b0+ni)%nw].SlaterMatrix(Beta));
        for(int ni=0; ni<nb; ni++)
          Oi.emplace_back(std::addressof(OrbMats[2*((b0+ni)/nw)+1]));
        SDetOp.BatchedMixedDensityMatrix(Oi,Ai,
              GB3D_.sliced(0,nb),LogOverlapFactor,ovlp2.sliced(nb,2*nb),compact);
        copy_n(ovlp2.origin(),2*nb,hvec.origin());
        Gxa.clear();
        Gya.clear();
        Gxb.clear();
        Gyb.clear();
        for(int ib=0; ib<nb; ++ib) {
          int idet = (b0+ib)/nw;
          int iw = (b0+ib)%nw;
          hvec[ib] *= hvec[nb+ib]*ma::conj(ci[idet]);
          Gxa.emplace_back(GA2D_[ib].origin()); 
          Gxb.emplace_back(GB2D_[ib].origin()); 
          if(transpose) {
            Gya.emplace_back(G[iw].origin());
            Gyb.emplace_back(G[iw].origin()+GAdims.first*GAdims.second);
          } else {
            Gya.emplace_back(G.origin()+iw);
            Gyb.emplace_back(G[GAdims.first*GAdims.second].origin()+iw);
          }
          Ovl[iw] += hvec[ib];
        }
        using ma::sumGwBatched;
        if(transpose) {
          sumGwBatched(GAdims.first*GAdims.second,hvec.data(),Gxa.data(),1,Gya.data(),1,b0,nw,nb);
          sumGwBatched(GBdims.first*GBdims.second,hvec.data(),Gxb.data(),1,Gyb.data(),1,b0,nw,nb);
        } else {
          sumGwBatched(GAdims.first*GAdims.second,hvec.data(),Gxa.data(),1,Gya.data(),nw,b0,nw,nb);
          sumGwBatched(GBdims.first*GBdims.second,hvec.data(),Gxb.data(),1,Gyb.data(),nw,b0,nw,nb);
        }
      }
    }
    copy_n(Ovl.origin(),nw,Ov.origin());
    // normalize G
    if(transpose) {
      for(size_t iw=0; iw<G.size(0); ++iw) { 
        auto ov = ComplexType(1.0,0.0)/Ov[iw];
        ma::scal(ov,G[iw]);
      }
    } else {
      using ma::term_by_term_matrix_vector;
      term_by_term_matrix_vector(ma::TOp_DIV,1,G.size(0),G.size(1),
                                 make_device_ptr(G.origin()),G.stride(0),
                                 make_device_ptr(Ov.origin()),Ov.stride(0));
    }
    TG.local_barrier();
  } 

  // Batched version of Overlap
  template<class devPsiT>  
  template<class WlkSet, class TVec>
  void NOMSD<devPsiT>::Overlap_batched(const WlkSet& wset, TVec&& Ov)
  {
    if(TG.TG_local().size() > 1) 
      APP_ABORT(" Error: Batched routine called with TG.TG_local().size() > 1 \n");
    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr_(make_device_ptr(Ov.origin()));
    }

    // since the memory usage is low, all walkers are always done together
    assert(Ov.stride(0)==1);
    const int nw = wset.size(); 
    const int ndet = ci.size();
    double LogOverlapFactor(wset.getLogOverlapFactor());
    assert(Ov.size() >= nw);  
    fill_n(Ov.origin(),nw,0);
    if(ovlp2.size(0) < 2*ndet*nw) ovlp2.reextent(iextensions<1u>{2*ndet*nw});
    fill_n(ovlp2.origin(),ovlp2.num_elements(),ComplexType(0.0));
    stdCVector hvec(iextensions<1u>{(2*ndet+1)*nw});
    fill_n(hvec.origin(),hvec.num_elements(),ComplexType(0.0));
    int i0(2*nw*ndet);  
    TG.local_barrier();
    std::vector<CMatrix_ref> Ai;
    std::vector<devPsiT*> Oi;
    Ai.reserve(2*ndet*nw);
    Oi.reserve(2*ndet*nw);

    if(walker_type != COLLINEAR) {
      Ai.clear();
      Oi.clear();
      for(int idet=0; idet<ndet; ++idet) {
        for(int ni=0; ni<nw; ni++) Ai.emplace_back(wset[ni].SlaterMatrix(Alpha));
        for(int ni=0; ni<nw; ni++) Oi.emplace_back(std::addressof(OrbMats[idet]));
      }  
      SDetOp.BatchedOverlap(Oi,Ai,LogOverlapFactor,ovlp2.sliced(0,ndet*nw));
      copy_n(ovlp2.origin(),nw*ndet,hvec.origin());
      for(int idet=0, idb=0; idet<ndet; ++idet) {
        for(int ib=0; ib<nw; ++ib, ++idb) 
            hvec[i0+ib] += ma::conj(ci[idet])*hvec[idb]*
                             ((walker_type==CLOSED)?(hvec[idb]):(ComplexType(1.0,0.0)));
      }
    } else {
      Ai.clear();
      Oi.clear();
      for(int idet=0; idet<ndet; ++idet) {
        for(int ni=0; ni<nw; ni++) Ai.emplace_back(wset[ni].SlaterMatrix(Alpha));
        for(int ni=0; ni<nw; ni++) Oi.emplace_back(std::addressof(OrbMats[2*idet]));
      }
      SDetOp.BatchedOverlap(Oi,Ai,LogOverlapFactor,ovlp2.sliced(0,ndet*nw));
      Ai.clear();
      Oi.clear();
      for(int idet=0; idet<ndet; ++idet) {
        for(int ni=0; ni<nw; ni++) Ai.emplace_back(wset[ni].SlaterMatrix(Beta));
        for(int ni=0; ni<nw; ni++) Oi.emplace_back(std::addressof(OrbMats[2*idet+1]));
      }
      SDetOp.BatchedOverlap(Oi,Ai,LogOverlapFactor,ovlp2.sliced(ndet*nw,2*ndet*nw));
      copy_n(ovlp2.origin(),2*nw*ndet,hvec.origin());
      for(int idet=0, idb=0, idb2=nw*ndet; idet<ndet; ++idet) {
        for(int ib=0; ib<nw; ++ib, ++idb, ++idb2)
            hvec[i0+ib] += ma::conj(ci[idet])*hvec[idb]*hvec[idb2];
      }
    }
    copy_n(hvec.origin()+i0,nw,Ov.origin());
  } 

  /*
   * Calculates the overlaps of all walkers in the set. Returns values in arrays. 
   * Ov is assumed to be local to the core
   */
  template<class devPsiT>  
  template<class WlkSet, class TVec>
  void NOMSD<devPsiT>::Overlap_shared(const WlkSet& wset, TVec&& Ov)
  {
    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr_(make_device_ptr(Ov.origin()));
    }
    const int nw = wset.size(); 
    const int ndet = ci.size();
    double LogOverlapFactor(wset.getLogOverlapFactor());
    assert(Ov.size() >= nw);  
    fill_n(Ov.origin(),nw,0);
    if(localGbuff.size() < 1)
      localGbuff.reextent(iextensions<1u>{1});
    if(walker_type != COLLINEAR) {

      const int ntasks_percore = (nw*ndet)/TG.getNCoresPerTG();
      const int ntasks_total_serial = ntasks_percore*TG.getNCoresPerTG();
      const int nextra = nw*ndet - ntasks_total_serial; 

      // each processor does ntasks_percore_serial overlaps serially
      const int tk0 = TG.getLocalTGRank()*ntasks_percore; 
      const int tkN = (TG.getLocalTGRank()+1)*ntasks_percore; 

      // task_w_d = = wlk_w*ndet + d 
      for(int tk=tk0; tk<tkN; ++tk) {
        int iw = tk/ndet;
        int nd = tk%ndet;
        ComplexType ov = SDetOp.Overlap(OrbMats[nd],wset[iw].SlaterMatrix(Alpha),LogOverlapFactor);
        Ov[iw] += ma::conj(ci[nd])*ov*((walker_type==CLOSED)?(ov):(ComplexType(1.0,0.0)));
      }

      // all remaining overlaps are performed in parallel with blocks of cores
      // partition processors in nextra groups
      if(nextra > 0) {
        // check if new communicator is necessary
        if( last_number_extra_tasks != nextra ) {
          last_number_extra_tasks = nextra;
          for(int n=0; n<nextra; n++) {
            int n0,n1;
            std::tie(n0,n1) = FairDivideBoundary(n,TG.getNCoresPerTG(),nextra);
            if(TG.getLocalTGRank()>=n0 && TG.getLocalTGRank()<n1) {
              last_task_index = n;
              break;
            }
          }   
          // first setup
          local_group_comm = std::move(shared_communicator(TG.TG_local().split(last_task_index,TG.TG_local().rank()))); 
          shmbuff_for_G = std::move(std::make_unique<mpi3CVector>(iextensions<1u>{dm_size(true)},
                                                shared_allocator<ComplexType>{local_group_comm}));
        } 
        if(last_task_index < 0 || last_task_index > nextra) 
          APP_ABORT("Error: Problems in NOMSD<devPsiT>::Overlap(WSet,Ov)");
        {
          int iw = (last_task_index+ntasks_total_serial)/ndet;
          int nd = (last_task_index+ntasks_total_serial)%ndet;
          ComplexType ov = SDetOp.Overlap(OrbMats[nd],wset[iw].SlaterMatrix(Alpha),LogOverlapFactor,local_group_comm);
          if(local_group_comm.rank()==0)
            Ov[iw] += ma::conj(ci[nd])*ov*((walker_type==CLOSED)?(ov):(ComplexType(1.0,0.0)));
        }
      }

    } else {

      const int ntasks_percore = (nw*ndet)/TG.getNCoresPerTG();
      const int ntasks_total_serial = ntasks_percore*TG.getNCoresPerTG();
      const int nextra = nw*ndet - ntasks_total_serial;

      // each processor does ntasks_percore_serial overlaps serially
      const int tk0 = TG.getLocalTGRank()*ntasks_percore;
      const int tkN = (TG.getLocalTGRank()+1)*ntasks_percore;

      // task_w_d = = wlk_w*ndet + d 
      for(int tk=tk0; tk<tkN; ++tk) {
        int iw = tk/ndet;
        int nd = tk%ndet;
        ComplexType ov = SDetOp.Overlap(OrbMats[2*nd],wset[iw].SlaterMatrix(Alpha),LogOverlapFactor);
        ov *= SDetOp.Overlap(OrbMats[2*nd+1],wset[iw].SlaterMatrix(Beta),LogOverlapFactor);
        Ov[iw] += ma::conj(ci[nd])*ov; 
      }

      // all remaining overlaps are performed in parallel with blocks of cores
      // partition processors in nextra groups
      if(nextra > 0) {
        // check if new communicator is necessary
        if( last_number_extra_tasks != nextra ) {
          last_number_extra_tasks = nextra;
          for(int n=0; n<nextra; n++) {
            int n0,n1;
            std::tie(n0,n1) = FairDivideBoundary(n,TG.getNCoresPerTG(),nextra);
            if(TG.getLocalTGRank()>=n0 && TG.getLocalTGRank()<n1) {
              last_task_index = n;
              break;
            }
          }
          // first setup
          local_group_comm = std::move(shared_communicator(TG.TG_local().split(last_task_index,TG.TG_local().rank()))); 
          shmbuff_for_G = std::move(std::make_unique<mpi3CVector>(iextensions<1u>{dm_size(true)},
                                                shared_allocator<ComplexType>{local_group_comm}));
        }
        if(last_task_index < 0 || last_task_index > nextra)
          APP_ABORT("Error: Problems in NOMSD<devPsiT>::Overlap(WSet,Ov)");
        {
          int task = (last_task_index+ntasks_total_serial);
          int iw = task/ndet;
          int nd = task%ndet;
          ComplexType ov = SDetOp.Overlap(OrbMats[2*nd],wset[iw].SlaterMatrix(Alpha),LogOverlapFactor,local_group_comm);
          ov *= SDetOp.Overlap(OrbMats[2*nd+1],wset[iw].SlaterMatrix(Beta),LogOverlapFactor,local_group_comm);
          if(local_group_comm.root())
            Ov[iw] += ma::conj(ci[nd])*ov; 
        }
      }
    }
    if(TG.TG_local().size() > 1) 
      TG.TG_local().all_reduce_in_place_n(to_address(Ov.origin()),nw,std::plus<>());
  }

  // Computes walker averaged density matrix:
  //   G(j,l) = \sum_iw wfac[iw] * G(iw,jl) / \sum_iw wfac[iw],
  // where wfac is the walker's weight (given as an argument) 
  // potentially multiplied by various factors if doing
  // back propagation or free projection.
  // if Refs != nullptr, we assume that back propagation is being performed
  template<class devPsiT>  
  template<class WlkSet, class MatG, class CVec1, class CVec2, class Mat1, class Mat2>
  void NOMSD<devPsiT>::WalkerAveragedDensityMatrix_shared(const WlkSet& wset, CVec1& wgt, MatG& G, CVec2& denom, Mat1 &&Ovlp, Mat2&& DMsum, bool free_projection, boost::multi::array_ref<ComplexType,3>* Refs, boost::multi::array<ComplexType,2>* detR) 
  {
    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr_(make_device_ptr(G.origin()));
    }
    const int nwalk = wset.size();
    const int ndet = ci.size();
    bool compact = false;
    auto Gsize = size_t(dm_size(not compact));
    int nrow = NMO*((walker_type==NONCOLLINEAR)?2:1);  
    int ncol = NAEA+((walker_type==CLOSED)?0:NAEB);  
    assert(wgt.size(0) >= nwalk);
    if(localGbuff.size() < 2*Gsize+1)
      localGbuff.reextent(iextensions<1u>{2*Gsize+1});
    double LogOverlapFactor(wset.getLogOverlapFactor());
    auto wlk_dims = wset.walker_dims();
    // Transposed temporaries for back propagation.
    if(G.size(1) != G.stride(0))
      APP_ABORT(" Error: FIX FIX FIX need strided fill_n\n");
    using std::fill_n;
    if(TG.TG_local().root())
      fill_n(G.origin(),G.num_elements(),ComplexType(0.0));
    TG.TG_local().barrier();
    if(Refs!=nullptr) {
      assert(detR!=nullptr);
      assert(wset.size() <= (*Refs).size(0));
      assert(ci.size() <= (*Refs).size(1));
      assert(nrow*ncol == (*Refs).size(2));
      assert(wset.size() == (*detR).size(0));
      assert(OrbMats.size() == (*detR).size(1));
    }
    if(walker_type != COLLINEAR) {
      auto Gdims = dm_dims(not compact,Alpha);
      // Temporaries for determinant component of walker rdm function.
      CMatrix_ref G2D_(localGbuff.origin(), {Gdims.first,Gdims.second});
      CVector_ref G1D_(localGbuff.origin(), iextensions<1u>{Gsize});
      // Accumulator for trial wavefunction sum.
      CVector_ref G21D_(localGbuff.origin()+Gsize,iextensions<1u>{Gsize});
      // 1D view of walker averaged RDM
      CVector_ref G1D(make_device_ptr(G.origin()),iextensions<1u>{Gsize});
      for(int iw=0; iw<nwalk; iw++) {
        if( iw%TG.TG_local().size() != TG.TG_local().rank() ) continue;
        if( std::abs(wgt[iw]) < 1e-10 ) continue;
        fill_n(G21D_.origin(), G21D_.num_elements(), ComplexType(0.0));
        ComplexType overlap = ComplexType(0.0,0.0);
        for(int id=0; id<ndet; id++) {
          // Mixed density matrix.
          ComplexType ov(1.0,0.0);
          if(Refs!=nullptr) {
            stdCMatrix_ref Orb(to_address((*Refs)[iw][id].origin()),{nrow,ncol});  
            auto&& SM(wset[iw].SlaterMatrixAux(Alpha));
            using std::copy_n; 
            copy_n(Orb.origin(),Orb.num_elements(),SM.origin());
            ComplexType ov_ = SDetOp.MixedDensityMatrix_noHerm(SM,wset[iw].SlaterMatrixN(Alpha),
                                      G2D_,LogOverlapFactor,compact,useSVD_in_Gfull);
            ov *= ov_ * ((*detR)[iw][id]);
          } else {
            ov *= SDetOp.MixedDensityMatrix(OrbMats[id],wset[iw].SlaterMatrix(Alpha),
                                      G2D_,LogOverlapFactor,compact);
          }  
          if(walker_type==CLOSED) 
            ov *= ov;
          ov *= ma::conj(ci[id]);
          overlap += ov;
          ma::axpy(ov,G1D_,G21D_);
        }
        ComplexType ov_denom;
// NOTE: 1/overlap has a risk of producing 0.0 (underflow) if the projection time is long.
//       FIX THIS!!! 
        if(free_projection) {
          ov_denom = wgt[iw]; 
          denom[0] += wgt[iw]*overlap;
        } else {
          ov_denom = wgt[iw]/overlap;
          denom[0] += wgt[iw];
        }
        ma::axpy(ov_denom, G21D_, G1D);
      }
    } else {
      auto GAdims = dm_dims(not compact,Alpha);
      auto GBdims = dm_dims(not compact,Beta);
      CMatrix_ref GA2D_(localGbuff.origin(),
                                   {GAdims.first,GAdims.second});
      CMatrix_ref GB2D_(localGbuff.origin()+GAdims.first*GAdims.second,
                                   {GBdims.first,GBdims.second});
      CVector_ref GA1D_(localGbuff.origin(),
                                   iextensions<1u>{GAdims.first*GAdims.second});
      CVector_ref GB1D_(localGbuff.origin()+GAdims.first*GAdims.second,
                                   iextensions<1u>{GBdims.first*GBdims.second});
      CVector_ref GA21D_(localGbuff.origin()+Gsize,
                                   iextensions<1u>{GAdims.first*GAdims.second});
      CVector_ref GB21D_(localGbuff.origin()+Gsize+GAdims.first*GAdims.second,
                                   iextensions<1u>{GBdims.first*GBdims.second});
      CVector_ref GA1D(make_device_ptr(G.origin()), iextensions<1u>{GAdims.first*GAdims.second});
      CVector_ref GB1D(make_device_ptr(G.origin())+GAdims.first*GAdims.second,
                       iextensions<1u>{GBdims.first*GBdims.second});
      for(int iw=0; iw<nwalk; iw++) {
        if( iw%TG.TG_local().size() != TG.TG_local().rank() ) continue;
        if( std::abs(wgt[iw]) < 1e-10 ) continue;
        fill_n(GA21D_.origin(), GA21D_.num_elements(), ComplexType(0.0));
        fill_n(GB21D_.origin(), GB21D_.num_elements(), ComplexType(0.0));
        ComplexType overlap = ComplexType(0.0,0.0);
        for(int id=0; id<ndet; id++) {
          ComplexType ov(1.0,0.0);
          // Mixed density matrix.
          if(Refs!=nullptr) {
            stdCMatrix_ref OrbA(to_address((*Refs)[iw][id].origin()),{NMO,NAEA});     
            stdCMatrix_ref OrbB(OrbA.origin()+OrbA.num_elements(),{NMO,NAEB});     
            auto&& SMA(wset[iw].SlaterMatrixAux(Alpha));
            auto&& SMB(wset[iw].SlaterMatrixAux(Beta));
            using std::copy_n;
            copy_n(OrbA.origin(),OrbA.num_elements(),SMA.origin());
            copy_n(OrbB.origin(),OrbB.num_elements(),SMB.origin());
            ComplexType ov_ = SDetOp.MixedDensityMatrix_noHerm(SMA,wset[iw].SlaterMatrixN(Alpha),
                                      GA2D_,LogOverlapFactor,compact,useSVD_in_Gfull);
            ov *= ov_ * ((*detR)[iw][2*id]);
            ov_ = SDetOp.MixedDensityMatrix_noHerm(SMB,wset[iw].SlaterMatrixN(Beta),
                                      GB2D_,LogOverlapFactor,compact,useSVD_in_Gfull);
            ov *= ov_ * ((*detR)[iw][2*id+1]);
          } else {
            ov *= SDetOp.MixedDensityMatrix(OrbMats[2*id],wset[iw].SlaterMatrix(Alpha),
                                             GA2D_,LogOverlapFactor,compact);
  
            ov *= SDetOp.MixedDensityMatrix(OrbMats[2*id+1],wset[iw].SlaterMatrix(Beta),
                                             GB2D_,LogOverlapFactor,compact);
          }
          ov *= ma::conj(ci[id]);
          overlap += ov;
          ma::axpy(ov,GA1D_,GA21D_);
          ma::axpy(ov,GB1D_,GB21D_);
        }
        ComplexType ov_denom;
// NOTE: 1/overlap has a risk of producing 0.0 (underflow) if the projection time is long.
//       FIX THIS!!! 
        if(free_projection) {
          ov_denom = wgt[iw];
          denom[0] += wgt[iw]*overlap;
        } else {
          ov_denom = wgt[iw]/overlap;
          denom[0] += wgt[iw];
        }
        ma::axpy(ov_denom, GA21D_, GA1D);
        ma::axpy(ov_denom, GB21D_, GB1D);
      }
    }
    // Average over walker's
    if(TG.Global().size() > 1) {
      TG.Global().all_reduce_in_place_n(to_address(denom.origin()),1,std::plus<>());
      TG.Global().all_reduce_in_place_n(to_address(G.origin()),Gsize,std::plus<>());
    }
    TG.local_barrier();
  }

  // this version is customized for single determinant runs and doesn't have underflow problem
  template<class devPsiT>  
  template<class WlkSet, class MatG, class CVec1, class CVec2, class Mat1, class Mat2>
  void NOMSD<devPsiT>::WalkerAveragedDensityMatrix_shared_single_det(const WlkSet& wset, CVec1& wgt, MatG& G, CVec2& denom, Mat1 &&Ovlp, Mat2&& DMsum, bool free_projection, boost::multi::array_ref<ComplexType,3>* Refs, boost::multi::array<ComplexType,2>* detR) 
  {
    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr_(make_device_ptr(G.origin()));
    }
    const int nwalk = wset.size();
    const int ndet = ci.size();
    assert(ndet==1);
    bool compact = false;
    auto Gsize = size_t(dm_size(not compact));
    int nrow = NMO*((walker_type==NONCOLLINEAR)?2:1);  
    int ncol = NAEA+((walker_type==CLOSED)?0:NAEB);  
    assert(wgt.size(0) >= nwalk);
    if(localGbuff.size() < Gsize+1)
      localGbuff.reextent(iextensions<1u>{Gsize+1});
    auto wlk_dims = wset.walker_dims();
    double LogOverlapFactor(wset.getLogOverlapFactor());
    // Transposed temporaries for back propagation.
    if(G.size(1) != G.stride(0))
      APP_ABORT(" Error: FIX FIX FIX need strided fill_n\n");
    using std::fill_n;
    if(TG.TG_local().root()) 
      fill_n(G.origin(),G.num_elements(),ComplexType(0.0));
    TG.TG_local().barrier();
    if(Refs!=nullptr) {
      assert(detR!=nullptr);
      assert(wset.size() <= (*Refs).size(0));
      assert(ci.size() <= (*Refs).size(1));
      assert(nrow*ncol == (*Refs).size(2));
      assert(wset.size() == (*detR).size(0));
      assert(OrbMats.size() == (*detR).size(1));
    }

    fill_n(Ovlp.origin(),Ovlp.num_elements(),ComplexType(0.0));
    fill_n(DMsum.origin(),DMsum.num_elements(),ComplexType(0.0));

    if(walker_type != COLLINEAR) {
      auto Gdims = dm_dims(not compact,Alpha);
      // Temporaries for determinant component of walker rdm function.
      CMatrix_ref G2D_(localGbuff.origin(), {Gdims.first,Gdims.second});
      CVector_ref G1D_(localGbuff.origin(), iextensions<1u>{Gsize});
      // 1D view of walker averaged RDM
      CVector_ref G1D(make_device_ptr(G.origin()),iextensions<1u>{Gsize});
      for(int iw=0; iw<nwalk; iw++) {
        if( iw%TG.TG_local().size() != TG.TG_local().rank() ) continue;
        if( std::abs(wgt[iw]) < 1e-10 ) continue;
        ComplexType overlap = ComplexType(1.0,0.0);
        fill_n(G1D_.origin(),G1D_.num_elements(),ComplexType(0.0));

        // Mixed density matrix.
        if(Refs!=nullptr) {
          stdCMatrix_ref Orb(to_address((*Refs)[iw][0].origin()),{nrow,ncol});  
          auto&& SM(wset[iw].SlaterMatrixAux(Alpha));
          using std::copy_n; 
          copy_n(Orb.origin(),Orb.num_elements(),SM.origin());
          ComplexType ov_ = SDetOp.MixedDensityMatrix_noHerm(SM,wset[iw].SlaterMatrixN(Alpha),
                                    G2D_,LogOverlapFactor,compact,useSVD_in_Gfull);
          overlap *= ov_ * ((*detR)[iw][0]);
        } else {
          overlap *= SDetOp.MixedDensityMatrix(OrbMats[0],wset[iw].SlaterMatrix(Alpha),
                                     G2D_,LogOverlapFactor,compact);
        }  
        if(walker_type==CLOSED) 
          overlap *= overlap;
        overlap *= ma::conj(ci[0]);

        ComplexType ov_denom;
        if(free_projection) {
          // has overflow problem, remove a constant factor from both numerator and denominator???
          ov_denom = wgt[iw]*overlap; 
          denom[0] += wgt[iw]*overlap;
        } else {
          ov_denom = wgt[iw];
          denom[0] += wgt[iw];
        }
        ma::axpy(ov_denom, G1D_, G1D);
      }
    } else {
      auto GAdims = dm_dims(not compact,Alpha);
      auto GBdims = dm_dims(not compact,Beta);
      CMatrix_ref GA2D_(localGbuff.origin(),
                                   {GAdims.first,GAdims.second});
      CMatrix_ref GB2D_(localGbuff.origin()+GAdims.first*GAdims.second,
                                   {GBdims.first,GBdims.second});
      CVector_ref GA1D_(localGbuff.origin(),
                                   iextensions<1u>{GAdims.first*GAdims.second});
      CVector_ref GB1D_(localGbuff.origin()+GAdims.first*GAdims.second,
                                   iextensions<1u>{GBdims.first*GBdims.second});
      CVector_ref GA1D(make_device_ptr(G.origin()), iextensions<1u>{GAdims.first*GAdims.second});
      CVector_ref GB1D(make_device_ptr(G.origin())+GAdims.first*GAdims.second,
                       iextensions<1u>{GBdims.first*GBdims.second});
      for(int iw=0; iw<nwalk; iw++) {
        if( iw%TG.TG_local().size() != TG.TG_local().rank() ) continue;
        if( std::abs(wgt[iw]) < 1e-10 ) continue;
        ComplexType overlap = ComplexType(1.0,0.0);
        fill_n(GA1D_.origin(),GA1D_.num_elements(),ComplexType(0.0));
        fill_n(GB1D_.origin(),GB1D_.num_elements(),ComplexType(0.0));

        // Mixed density matrix.
        if(Refs!=nullptr) {
          stdCMatrix_ref OrbA(to_address((*Refs)[iw][0].origin()),{NMO,NAEA});     
          stdCMatrix_ref OrbB(OrbA.origin()+OrbA.num_elements(),{NMO,NAEB});     
          auto&& SMA(wset[iw].SlaterMatrixAux(Alpha));
          auto&& SMB(wset[iw].SlaterMatrixAux(Beta));
          using std::copy_n;
          copy_n(OrbA.origin(),OrbA.num_elements(),SMA.origin());
          copy_n(OrbB.origin(),OrbB.num_elements(),SMB.origin());
          ComplexType ov1 = SDetOp.MixedDensityMatrix_noHerm(SMA,wset[iw].SlaterMatrixN(Alpha),
                                    GA2D_,LogOverlapFactor,compact,useSVD_in_Gfull);
          Ovlp[iw][0] = ov1; 
          DMsum[iw][0] = ma::sum(GA1D_);
          overlap *= ov1 * ((*detR)[iw][0]);
          ComplexType ov2 = SDetOp.MixedDensityMatrix_noHerm(SMB,wset[iw].SlaterMatrixN(Beta),
                                    GB2D_,LogOverlapFactor,compact,useSVD_in_Gfull);
          Ovlp[iw][1] = ov2; 
          DMsum[iw][1] = ma::sum(GB1D_);
          overlap *= ov2 * ((*detR)[iw][1]);
        } else {
          ComplexType ov1 = SDetOp.MixedDensityMatrix(OrbMats[0],wset[iw].SlaterMatrix(Alpha),
                                         GA2D_,LogOverlapFactor,compact);
          Ovlp[iw][0] = ov1; 
          DMsum[iw][0] = ma::sum(GA1D_);
          overlap *= ov1; 

          ComplexType ov2 = SDetOp.MixedDensityMatrix(OrbMats[1],wset[iw].SlaterMatrix(Beta),
                                           GB2D_,LogOverlapFactor,compact);

          Ovlp[iw][1] = ov2; 
          DMsum[iw][1] = ma::sum(GB1D_);
          overlap *= ov2; 
        }
        overlap *= ma::conj(ci[0]);

//        if( not ( std::isfinite(real(Ovlp[iw][0])) && std::isfinite(imag(Ovlp[iw][0])) && 
//                  std::isfinite(real(Ovlp[iw][1])) && std::isfinite(imag(Ovlp[iw][1])) &&
        if( not ( std::isfinite(real(DMsum[iw][0])) && std::isfinite(imag(DMsum[iw][0])) &&
                  std::isfinite(real(DMsum[iw][1])) && std::isfinite(imag(DMsum[iw][1]))
                                     ) ) continue;
        if( std::isnan(real(DMsum[iw][0])) || std::isnan(imag(DMsum[iw][0]))||
                  std::isnan(real(DMsum[iw][1])) || std::isnan(imag(DMsum[iw][1]))
                                     )  continue;
        if( std::isinf(real(DMsum[iw][0])) || std::isinf(imag(DMsum[iw][0]))||
                  std::isinf(real(DMsum[iw][1])) || std::isinf(imag(DMsum[iw][1]))
                                     )  continue;

        ComplexType ov_denom;
        if(free_projection) {
          // has overflow problem, remove a constant factor from both numerator and denominator???
          ov_denom = wgt[iw]*overlap;
          denom[0] += wgt[iw]*overlap;
        } else {
          ov_denom = wgt[iw];
          denom[0] += wgt[iw];
        }
        ma::axpy(ov_denom, GA1D_, GA1D);
        ma::axpy(ov_denom, GB1D_, GB1D);
      }
    }
    // Average over walker's
    if(TG.Global().size() > 1) {
      TG.Global().all_reduce_in_place_n(to_address(denom.origin()),1,std::plus<>());
      TG.Global().all_reduce_in_place_n(to_address(G.origin()),Gsize,std::plus<>());
    }
    TG.local_barrier();
  }

  // Computes walker averaged density matrix:
  //   G(j,l) = \sum_iw wfac[iw] * G(iw,jl) / \sum_iw wfac[iw],
  // where wfac is the walker's weight (given as an argument) 
  // potentially multiplied by various factors if doing
  // back propagation or free projection.
  // if Refs != nullptr, we assume that back propagation is being performed
  template<class devPsiT>  
  template<class WlkSet, class MatG, class CVec1, class CVec2, class Mat1, class Mat2>
  void NOMSD<devPsiT>::WalkerAveragedDensityMatrix_batched(const WlkSet& wset, CVec1& wgt, MatG& G, CVec2& denom, Mat1 &&Ovlp, Mat2&& DMsum, bool free_projection, boost::multi::array_ref<ComplexType,3>* Refs, boost::multi::array<ComplexType,2>* detR) 
  {
/*
    if(TG.TG_local().size() > 1) 
      APP_ABORT(" Error: Batched routine called with TG.TG_local().size() > 1 \n");
    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr_(make_device_ptr(Ov.origin()));
      auto dev_ptr(make_device_ptr(G.origin()));
    }
    assert(G.stride(1)==1);
    const int nw = wset.size(); 
    int nbatch__ = std::min(nw,(nbatch<0?nw:nbatch)); 
    const int ndet = ci.size();
    int nrow = NMO*((walker_type==NONCOLLINEAR)?2:1);
    int ncol = NAEA+((walker_type==CLOSED)?0:NAEB);
    assert(wgt.size(0) >= nw);
    assert(wgt.size(1) >= ndet);
    if(G.size(1) != G.stride(0))
      APP_ABORT(" Error: FIX FIX FIX need strided fill_n\n");
    fill_n(G.origin(),G.num_elements(),ComplexType(0.0));
    if(Refs!=nullptr) {
      assert(detR!=nullptr);
      assert(wset.size() <= (*Refs).size(0));
      assert(ci.size() <= (*Refs).size(1));
      assert(nrow*ncol == (*Refs).size(2));
      assert(wset.size() == (*detR).size(0));
      assert(OrbMats.size() == (*detR).size(1));
    }
    stdCVector hvec(iextensions<1u>{2*nbatch__});
    TG.local_barrier();
    auto Gsize = size_t(dm_size(not compact));
    if(localGbuff.size() < nbatch__*Gsize+1)
      localGbuff.reextent(iextensions<1u>{nbatch__*Gsize+1});
    pointer ov_(localGbuff.origin()+nbatch__*Gsize);  
    std::vector<CMatrix_ref> Ai;
    Ai.reserve(nbatch__);

    if(walker_type != COLLINEAR) {

      auto GAdims = dm_dims(not compact,Alpha); 
      CTensor_ref G3D_(localGbuff.origin(),{nbatch__,GAdims.first,GAdims.second});
      CMatrix_ref G2D_(G3D_.origin(),iextensions<2u>{nbatch__,GAdims.first*GAdims.second});

      for(int idet=0; idet<ndet; ++idet) {
        for(int iw=0; iw<nw; iw+=nbatch__) {
          int nb = std::min(nbatch__,nw-iw);
          Ai.clear();
          for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Alpha));
          SDetOp.BatchedMixedDensityMatrix_noherm(OrbMats[idet],Ai,
                                G3D_.sliced(0,nb),ovlp2.sliced(0,nb),compact);
          copy_n(ovlp2.origin(),nb,hvec.origin());
          for(int ib=0; ib<nb; ++ib) {
            ComplexType ov(hvec[ib]);
            if(walker_type==CLOSED) ov *= ov;
            ov *= ma::conj(ci[idet]);
            if(transpose) {
              ma::axpy(ov,G2D_[ib],G[iw+ib]);   
            } else {
              ma::axpy(ov,G2D_[ib],G(G.extension(0),iw+ib));   
            } 
            Ov[iw+ib] += ov;
          }
        }
      }

    } else {

      auto GAdims = dm_dims(not compact,Alpha); 
      auto GBdims = dm_dims(not compact,Beta);
      CTensor_ref GA3D_(localGbuff.origin(),{nbatch__,GAdims.first,GAdims.second});
      CMatrix_ref GA2D_(GA3D_.origin(),iextensions<2u>{nbatch__,GAdims.first*GAdims.second});
      CTensor_ref GB3D_(GA3D_.origin()+GA3D_.num_elements(),{nbatch__,GBdims.first,GBdims.second});
      CMatrix_ref GB2D_(GB3D_.origin(),iextensions<2u>{nbatch__,GBdims.first*GBdims.second});

      for(int idet=0; idet<ndet; ++idet) {
        for(int iw=0; iw<nw; iw+=nbatch__) {
          int nb = std::min(nbatch__,nw-iw);
          Ai.clear();
          for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Alpha));
          SDetOp.BatchedMixedDensityMatrix_noherm(OrbMats[2*idet],Ai,
                                GA3D_.sliced(0,nb),ovlp2.sliced(0,nb),compact);
          Ai.clear();
          for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Beta));
          SDetOp.BatchedMixedDensityMatrix_noherm(OrbMats[2*idet+1],Ai,
                                GB3D_.sliced(0,nb),ovlp2.sliced(nb,2*nb),compact);
          copy_n(ovlp2.origin(),2*nb,hvec.origin());
          for(int ib=0; ib<nb; ++ib) {
            ComplexType ov = hvec[ib]*hvec[nb+ib]*ma::conj(ci[idet]);
            if(transpose) {
              ma::axpy(ov,GA2D_[ib],G[iw+ib].sliced(0,GAdims.first*GAdims.second));
              ma::axpy(ov,GB2D_[ib],G[iw+ib].sliced(GAdims.first*GAdims.second,G.size(1)));
            } else {
              ma::axpy(ov,GA2D_[ib],G({0,GAdims.first*GAdims.second},iw+ib));
              ma::axpy(ov,GB2D_[ib],G({GAdims.first*GAdims.second,G.size(0)},iw+ib));
            }
            Ov[iw+ib] += ov;
          }
        }
      }
    }
    TG.local_barrier();
*/

  }

  /*
   * Orthogonalizes the Slater matrices of all walkers in the set.  
   * Options:
   *  - bool importanceSamplingt(default=true): use algorithm appropriate for importance sampling. 
   *         This means that the determinant of the R matrix in the QR decomposition is ignored.
   *         If false, add the determinant of R to the weight of the walker. 
   */
  template<class devPsiT>  
  template<class WlkSet>
  void NOMSD<devPsiT>::Orthogonalize_shared(WlkSet& wset, bool impSamp) {
    if(localGbuff.size() < 1)
      localGbuff.reextent(iextensions<1u>{1});
    double LogOverlapFactor(wset.getLogOverlapFactor());
    ComplexType detR(1.0,0.0);
    if(walker_type != COLLINEAR) {
      int cnt=0;
      for(typename WlkSet::iterator it=wset.begin(); it!=wset.end(); ++it) {
        if( (cnt++)%TG.getNCoresPerTG() == TG.getLocalTGRank() ) {
          if(excitedState && numExcitations.first>0)
            OrthogonalizeExcited(it->SlaterMatrix(Alpha),Alpha,LogOverlapFactor);
          else { 
            detR = SDetOp.Orthogonalize(it->SlaterMatrix(Alpha),LogOverlapFactor);
          }
          if(!impSamp) {
            if(walker_type==CLOSED)
              *it->weight() *= (detR*detR);
            else  
              *it->weight() *= detR;
          }
        }
      } 
    } else {
      int cnt=0;
      for(typename WlkSet::iterator it=wset.begin(); it!=wset.end(); ++it) {
        if( (cnt++)%TG.getNCoresPerTG() == TG.getLocalTGRank() ) {
          if(excitedState && numExcitations.first>0)
            OrthogonalizeExcited(it->SlaterMatrix(Alpha),Alpha,LogOverlapFactor);
          else {
            detR = SDetOp.Orthogonalize(it->SlaterMatrix(Alpha),LogOverlapFactor);
          }
          if(!impSamp) {
            std::lock_guard<shared_mutex> guard(*mutex);
            *it->weight() *= detR;
          }
        }
        if( (cnt++)%TG.getNCoresPerTG() == TG.getLocalTGRank() ) {
          if(excitedState && numExcitations.second>0)
            OrthogonalizeExcited(it->SlaterMatrix(Beta),Beta,LogOverlapFactor);
          else {
            detR = SDetOp.Orthogonalize(it->SlaterMatrix(Beta),LogOverlapFactor);
          }
          if(!impSamp) {
            std::lock_guard<shared_mutex> guard(*mutex);
            *it->weight() *= detR;
          }
        }
      }
    }   
    TG.local_barrier();    
    // recalculate overlaps
    Overlap(wset);
  }

  /*
   * Orthogonalizes the Slater matrices of all walkers in the set.  
   * Options:
   *  - bool importanceSamplingt(default=true): use algorithm appropriate for importance sampling. 
   *         This means that the determinant of the R matrix in the QR decomposition is ignored.
   *         If false, add the determinant of R to the weight of the walker. 
   */
  template<class devPsiT>  
  template<class WlkSet>
  void NOMSD<devPsiT>::Orthogonalize_batched(WlkSet& wset, bool impSamp) {
    if(TG.TG_local().size() > 1)
      APP_ABORT(" Error: Batched routine called with TG.TG_local().size() > 1 \n");
    using std::fill_n;
    using std::copy_n;
    const int nw = wset.size();
    double LogOverlapFactor(wset.getLogOverlapFactor());
    int nbatch__ = std::min(nw,(nbatch_qr<0?nw:nbatch_qr));
    if(!impSamp) {
      if(localGbuff.size() < nbatch__)
        localGbuff.reextent(iextensions<1u>{nbatch__});
    }
    stdCVector detR(iextensions<1u>{2*nw});
    std::vector<CMatrix_ref> Ai;
    Ai.reserve(nbatch__);
    if(walker_type != COLLINEAR) {
      for(int iw=0; iw<nw; iw+=nbatch__) {
        int nb = std::min(nbatch__,nw-iw);
        Ai.clear();
        for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Alpha));
        if(impSamp) {
          SDetOp.BatchedOrthogonalize(Ai,LogOverlapFactor);
        } else {
          SDetOp.BatchedOrthogonalize(Ai,LogOverlapFactor,localGbuff.origin());
          using std::copy_n;
          copy_n(localGbuff.origin(),nb,detR.origin()+iw);
        }
      }
    } else {
      for(int iw=0; iw<nw; iw+=nbatch__) {
        int nb = std::min(nbatch__,nw-iw);
        Ai.clear();
        for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Alpha));
        if(impSamp) {
          SDetOp.BatchedOrthogonalize(Ai,LogOverlapFactor);
        } else {
          SDetOp.BatchedOrthogonalize(Ai,LogOverlapFactor,localGbuff.origin());
          using std::copy_n;
          copy_n(localGbuff.origin(),nb,detR.origin()+iw);
        }
        Ai.clear();
        for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Beta));
        if(impSamp) {
          SDetOp.BatchedOrthogonalize(Ai,LogOverlapFactor);
        } else {
          SDetOp.BatchedOrthogonalize(Ai,LogOverlapFactor,localGbuff.origin());
          using std::copy_n;
          copy_n(localGbuff.origin(),nb,detR.origin()+nw+iw);
        }
      }
    }
    if(!impSamp) {
      if(walker_type==CLOSED)
        for(int iw=0; iw<nw; iw++) 
          detR[iw] = detR[iw]*detR[iw];
      else if(walker_type==COLLINEAR)
        for(int iw=0; iw<nw; iw++) 
          detR[iw] *= detR[iw+nw];
      wset.getProperty(WEIGHT,detR.sliced(nw,2*nw));
      for(int iw=0; iw<nw; iw++)
        detR[nw+iw] *= detR[iw];  
      wset.setProperty(WEIGHT,detR.sliced(nw,2*nw));
    }
    TG.local_barrier();
    // recalculate overlaps
    Overlap(wset);
  }

  /*
   * Orthogonalize extended Slater Matrix for excited states calculation
   * Ret 
   */
  template<class devPsiT>  
  template<class Mat>
  void NOMSD<devPsiT>::OrthogonalizeExcited(Mat&& A, SpinTypes spin, double LogOverlapFactor)
  {
    if(localGbuff.size() < 1)
      localGbuff.reextent(iextensions<1u>{1});
    if(walker_type == NONCOLLINEAR)  
      APP_ABORT(" Error: OrthogonalizeExcited not implemented with NONCOLLINEAR.\n");
    if(spin==Alpha) {
      if(extendedMatAlpha.size(0) != NMO || extendedMatAlpha.size(1) != maxOccupExtendedMat.first)
        extendedMatAlpha.reextent({NMO,maxOccupExtendedMat.first});
      extendedMatAlpha(extendedMatAlpha.extension(0),{0,NAEA}) = A;
      extendedMatAlpha(extendedMatAlpha.extension(0),{NAEA+1,maxOccupExtendedMat.first}) =
                excitedOrbMat[0](excitedOrbMat.extension(1),{NAEA+1,maxOccupExtendedMat.first}); 
      // move i->a, copy trial orb i  
      for(auto& i:excitations) 
        if(i.first < NMO && i.second < NMO ) {
          extendedMatAlpha(extendedMatAlpha.extension(0),i.second) = extendedMatAlpha(extendedMatAlpha.extension(0),i.first);
          extendedMatAlpha(extendedMatAlpha.extension(0),i.first) = excitedOrbMat[0](excitedOrbMat.extension(1),i.first);
        }
      ComplexType detR = SDetOp.Orthogonalize(extendedMatAlpha,LogOverlapFactor);
      A(A.extension(0),{0,NAEA}) = extendedMatAlpha(extendedMatAlpha.extension(0),{0,NAEA});  
      for(auto& i:excitations) 
        if(i.first < NMO && i.second < NMO ) 
          A(A.extension(0),i.first) = extendedMatAlpha(extendedMatAlpha.extension(0),i.second);
    } else {
      if(extendedMatBeta.size(0) != NMO || extendedMatBeta.size(1) != maxOccupExtendedMat.second)
        extendedMatBeta.reextent({NMO,maxOccupExtendedMat.second});
      extendedMatBeta(extendedMatBeta.extension(0),{0,NAEB}) = A;
      extendedMatBeta(extendedMatBeta.extension(0),{NAEB+1,maxOccupExtendedMat.second}) = 
                excitedOrbMat[1](excitedOrbMat.extension(1),{NAEB+1,maxOccupExtendedMat.second});
      // move i->a, copy trial orb i  
      for(auto& i:excitations) 
        if(i.first >= NMO && i.second >= NMO ) {
          extendedMatBeta(extendedMatBeta.extension(0),i.second) = extendedMatBeta(extendedMatBeta.extension(0),i.first);
          extendedMatBeta(extendedMatBeta.extension(0),i.first) = excitedOrbMat[1](excitedOrbMat.extension(1),i.first);
        }
      ComplexType detR = SDetOp.Orthogonalize(extendedMatBeta,LogOverlapFactor);
      A(A.extension(0),{0,NAEB}) = extendedMatBeta(extendedMatBeta.extension(0),{0,NAEB});
      for(auto& i:excitations) 
        if(i.first >= NMO && i.second >= NMO ) 
          A(A.extension(0),i.first) = extendedMatBeta(extendedMatBeta.extension(0),i.second);
    }
  }  

  /*
   * Calculate mean field expectation value of Cholesky potentials
   */
  template<class devPsiT>  
  template<class Vec>
  void NOMSD<devPsiT>::vMF(Vec&& v) {
    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr_(make_device_ptr(v.origin()));
    }

    assert(v.num_elements() == local_number_of_cholesky_vectors());
    fill_n(v.origin(),v.num_elements(),ComplexType(0));  

    int ndets = ci.size(); 
    CMatrix PsiT,PsiTB;
    if(localGbuff.size() < 1)
      localGbuff.reextent(iextensions<1u>{1});
 
    bool found=false;
    using ma::conj;
    if(ndets == 1) {
      CMatrix G;
      size_t Gsize = dm_size(false);
      if(walker_type != COLLINEAR) {
        auto Gdims = dm_dims(false,Alpha);
        G.reextent({Gdims.first,Gdims.second});
        ma::Matrix2MA('H',OrbMats[0],PsiT); 
        SDetOp.MixedDensityMatrix(OrbMats[0],PsiT,G,0.0,true);
      } else {
        G.reextent({NAEA+NAEB,NMO});
        ma::Matrix2MA('H',OrbMats[0],PsiT);
        SDetOp.MixedDensityMatrix(OrbMats[0],PsiT,
                                       G.sliced(0,NAEA),0.0,true);
        ma::Matrix2MA('H',OrbMats[1],PsiT);
        SDetOp.MixedDensityMatrix(OrbMats[1],PsiT,
                                       G.sliced(NAEA,NAEA+NAEB),0.0,true);
      }    
      CVector_ref G1D(G.origin(),iextensions<1u>{G.num_elements()});
      HamOp.vbias(G1D,std::forward<Vec>(v));
    } else {
      size_t Gsize = dm_size(true);
      auto Gdims = dm_dims(true,Alpha);
      CVector G1D(iextensions<1u>{Gsize});
      fill_n(G1D.origin(),G1D.num_elements(),ComplexType(0));
      ComplexType ov(0);  
      if(walker_type != COLLINEAR) {
        CMatrix G_({Gdims.first,Gdims.second});
        CVector_ref G1D_(G_.origin(),iextensions<1u>{Gdims.first*Gdims.second});
        int n0,n1;
        std::tie(n0,n1) = FairDivideBoundary(TG.getGlobalRank(),ndets*(ndets+1)/2,
                                             TG.getGlobalSize());
        int last_p=-1;
        for(int p=0, pq=0; p<ndets; p++) {
          for(int q=p; q<ndets; q++, pq++) {
            if(pq < n0) continue;
            if(pq >= n1) break;
            if(last_p != p) {
              last_p=p;
              ma::Matrix2MA('H',OrbMats[p],PsiT);
            }
            ComplexType ov1 = SDetOp.MixedDensityMatrix(OrbMats[q],PsiT,G_,0.0,false);
            if(walker_type==CLOSED) ov1 *= ov1;
            ov += real(ma::conj(ci[q])*ci[p]*ov1);
// !!! GPU !!!
            for(int i=0; i<G1D.num_elements(); i++)
              G1D[i] += real(ma::conj(ci[q])*ci[p]*ov1*G1D_[i]);
          }
        }
      } else {
        CMatrix G_({2*NMO,NMO});
        CVector_ref G1D_(G_.origin(),iextensions<1u>{2*NMO*NMO});
        int n0,n1;
        std::tie(n0,n1) = FairDivideBoundary(TG.getGlobalRank(),ndets*(ndets+1)/2,
                                             TG.getGlobalSize());
        int last_p=-1;
        for(int p=0, pq=0; p<ndets; p++) {
          for(int q=p; q<ndets; q++, pq++) {
            if(pq < n0) continue;
            if(pq >= n1) break;
            if(last_p != p) {
              last_p=p;
              ma::Matrix2MA('H',OrbMats[2*p],PsiT);
              ma::Matrix2MA('H',OrbMats[2*p+1],PsiTB);
            }
            ComplexType ov1 = SDetOp.MixedDensityMatrix(OrbMats[2*q],PsiT,
                                G_.sliced(0,NMO),0.0,false);
            if(std::abs(ov1) == ComplexType(0.0) && not found) {
              found=true;
              app_warning()<<" WARNING: Found orthogonal determinants in trial wave function of NOMSD. The mean-field substraction potential is potentially wrong. ! \n";   
//              SDetOp.OrthogonalUnnormalizedMixedDensityMatrix(OrbMats[2*q],PsiT,
//                                G_.sliced(0,NMO),false); 
            }
            ComplexType ov2 = SDetOp.MixedDensityMatrix(OrbMats[2*q+1],PsiTB,
                                G_.sliced(NMO,2*NMO),0.0,false);
            if(std::abs(ov2) == ComplexType(0.0) && not found) {
              found=true;
              app_warning()<<" WARNING: Found orthogonal determinants in trial wave function of NOMSD. The mean-field substraction potential is potentially wrong. ! \n";   
//              SDetOp.OrthogonalUnnormalizedMixedDensityMatrix(OrbMats[2*q+1],PsiTB,
//                                G_.sliced(NMO,2*NMO),false); 
            }
            ov += real(ma::conj(ci[q])*ci[p]*ov1*ov2);
            // 
// !!! GPU !!!
            for(int i=0; i<G1D.num_elements(); i++)
              G1D[i] += real(ma::conj(ci[q])*ci[p]*ov1*ov2*G1D_[i]);
          }
        }
      }  
      if(TG.Global().size() > 1) 
        TG.Global().all_reduce_in_place_n(to_address(G1D.origin()),G1D.num_elements(),std::plus<>());  
      ComplexType ov1 = ( TG.Global() += ov );
      ma::scal(ComplexType(1.0,0.0)/ov1,G1D);
      HamOp.vbias(G1D.sliced(0,Gdims.first*Gdims.second),
                  std::forward<Vec>(v));
      if(walker_type==COLLINEAR)
        HamOp.vbias(G1D.sliced(NMO*NMO,2*NMO*NMO),
                  std::forward<Vec>(v),1.0,1.0);
    }
    // since v is not in shared memory, we need to reduce
    if(TG.TG_local().size() > 1) 
      TG.TG_local().all_reduce_in_place_n(to_address(v.origin()),v.num_elements(),std::plus<>());
    // NOTE: since SpvnT is a truncated structure the complex part of vMF, 
    //       which should be exactly zero, suffers from truncation errors.
    //       Set it to zero.
    for(int i=0; i<v.num_elements(); i++)
      v[i] = ComplexType(real(v[i]),0.0); 
  }

  template< class devPsiT>
  inline void NOMSD<devPsiT>::recompute_ci()
  {
    double LogOverlapFactor(0.0);
    int ndets = ci.size(); 
    int nspin(1);
    if(walker_type==COLLINEAR) nspin=2;
    if(ndets == 1) {
      ci[0] = ComplexType(1.0,0.0);
      return;   
    }

    if(TG.getNGroupsPerTG() > 1) {
      APP_ABORT(" Error: rediag not implemented with distributed wavefunction.\n");
    } else {

      size_t nt = (1+dm_size(false)+NMO*NAEA);
      if(shmbuff_for_E.num_elements() < nt)
        shmbuff_for_E.reextent(iextensions<1u>{nt});

      boost::multi::array<ComplexType,2,shared_allocator<ComplexType>> H({ndets,ndets},TG.Node());
      boost::multi::array<ComplexType,2,shared_allocator<ComplexType>> S({ndets,ndets},TG.Node());
      boost::multi::array<ComplexType,1> energy(iextensions<1u>{2});
      CMatrix Psia({0,0});
      CMatrix Psib({0,0});
      if(TG.TG_local().root()) {
        // only TG_local().root() calculates G for now
        Psia.reextent({NMO,NAEA});
        if(walker_type == COLLINEAR)
          Psib.reextent({NMO,NAEB});
      }  
      using std::fill_n;
      fill_n(H.origin(),H.num_elements(),ComplexType(0.0));
      fill_n(S.origin(),S.num_elements(),ComplexType(0.0));
      fill_n(energy.origin(),energy.num_elements(),ComplexType(0.0));
    
      ComplexType zero(0.0);
      auto Gsize = dm_size(false);
      int nr=Gsize,nc=1;
      if(transposed_G_for_E_) std::swap(nr,nc);
      CVector_ref ov_(make_device_ptr(shmbuff_for_E.origin()),iextensions<1u>{1});
      CMatrix_ref G(ov_.origin()+1,{nr,nc});
      eloc2.reextent({1,3});
    
      for(int jdet=0, ji=0; jdet<ndets; jdet++) {
        ComplexType cjdet = ci[jdet];
        for(int idet=jdet;  idet<ndets; idet++, ji++) {
          if(ji%TG.getNumberOfTGs() == TG.getTGNumber()) {
            ComplexType cidet = ci[idet];
            if(TG.TG_local().root()) {
              CMatrix_ref Ga(G.origin(),{NAEA,NMO});
              ma::Matrix2MA('H',OrbMats[nspin*jdet],Psia);
              ov_[0] = SDetOp.MixedDensityMatrix( OrbMats[nspin*idet], Psia, Ga,
                                           LogOverlapFactor,true);
              if(walker_type == COLLINEAR) { 
                CMatrix_ref Gb(Ga.origin()+Ga.num_elements(),{NAEB,NMO});
                ma::Matrix2MA('H',OrbMats[nspin*jdet+1],Psib);
                ov_[0] *= SDetOp.MixedDensityMatrix( OrbMats[nspin*idet+1], Psib, Gb,
                                           LogOverlapFactor,true);
              } else if(walker_type==CLOSED) {
                ov_[0] *= ov_[0];
              }
            }
            TG.local_barrier();
            HamOp.energy(eloc2,G,idet,TG.TG_local().root());
            if(TG.TG_local().size() > 1)
              TG.TG_local().all_reduce_in_place_n(to_address(eloc2.origin()),3,std::plus<>());
            if(TG.TG_local().root()) {
              if(idet != jdet) {
                H[idet][jdet] = ov_[0]*(eloc2[0][0]+eloc2[0][1]+eloc2[0][2]);
                S[idet][jdet] = ov_[0];
                H[jdet][idet] = ma::conj(H[idet][jdet]);   
                S[jdet][idet] = ma::conj(ov_[0]);   
                energy[0] += ma::conj(cidet)*cjdet*H[idet][jdet] +
                             ma::conj(cjdet)*cidet*H[jdet][idet];
                energy[1] += ma::conj(cidet)*cjdet*S[idet][jdet] +
                             ma::conj(cjdet)*cidet*S[jdet][idet];
              } else { 
                H[idet][idet] = ComplexType(real(ov_[0])*real(eloc2[0][0]+eloc2[0][1]+eloc2[0][2]),0.0);
                S[idet][idet] = ComplexType(real(ov_[0]),0.0);
                energy[0] += ma::conj(cidet)*cjdet*H[idet][jdet]; 
                energy[1] += ma::conj(cidet)*cjdet*S[idet][jdet]; 
              } 
            }
            TG.local_barrier();
          }
        }
      }
      TG.Global().barrier();
      if(TG.Node().root())
        TG.Cores().all_reduce_in_place_n(to_address(H.origin()),H.num_elements(),std::plus<>());
      if(TG.Node().root())
        TG.Cores().all_reduce_in_place_n(to_address(S.origin()),S.num_elements(),std::plus<>());
      TG.Global().all_reduce_in_place_n(energy.origin(),2,std::plus<>());

      app_log() << " - Variational energy of trial wavefunction: " << std::setprecision(16) << energy[0] / energy[1] << "\n";
      app_log() << " - Diagonalizing CI matrix.\n";
      using RVector = boost::multi::array<RealType,1>;
      using CMatrix = boost::multi::array<ComplexType,2>;
      // Want a "unique" solution for all cores/nodes.
      if(TG.Global().rank() == 0) {
        boost::multi::array_ref<ComplexType,2> H_(to_address(H.origin()),{ndets,ndets});
        boost::multi::array_ref<ComplexType,2> S_(to_address(S.origin()),{ndets,ndets});
        std::pair<RVector,CMatrix> Sol = ma::genEigSelect<RVector,CMatrix>(H_,S_,1);
        app_log() << " - Updating CI coefficients. \n";
        app_log() << " - Recomputed coefficient of first determinant: " << Sol.second[0][0] << "\n";
        for(int idet=0; idet < ndets; idet++) {
          ComplexType ci_ = Sol.second[0][idet];
          // Do we want this much output?
          app_log() << idet << " old: " << ci[idet] << " new: " << ci_ << "\n";
          ci[idet] = ci_;
        } 
        app_log() << " - Recomputed variational energy of trial wavefunction: " << Sol.first[0] << "\n";
      } 
      if(TG.Global().size() > 1) 
        TG.Global().broadcast_n(to_address(ci.data()), ci.size(), 0);
    }
  }

}

}
