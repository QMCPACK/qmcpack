//////////////////////////////////////////////////////////////////////
// This file is distributed under the University of Illinois/NCSA Open Source
// License.  See LICENSE file in top directory for details.
//
// Copyright (c) 2016 Jeongnim Kim and QMCPACK developers.
//
// File developed by:
// Miguel A. Morales, moralessilva2@llnl.gov 
//    Lawrence Livermore National Laboratory 
//
// File created by:
// Miguel A. Morales, moralessilva2@llnl.gov 
//    Lawrence Livermore National Laboratory 
////////////////////////////////////////////////////////////////////////////////

#include <vector>
#include <map>
#include <string>
#include <iostream>
#include <tuple>

#include "AFQMC/config.h"
#include "AFQMC/Utilities/Utils.hpp"
#include "AFQMC/Propagators/WalkerSetUpdate.hpp"
#include "AFQMC/Walkers/WalkerConfig.hpp"
#include "AFQMC/Numerics/ma_blas.hpp"

#include "Utilities/Timer.h"

namespace qmcplusplus
{

namespace afqmc
{

/*
 * Propagates the walker population nsteps forward with a fixed vbias (from the initial 
 * configuration).   
 */
template<class WlkSet>
void AFQMCDistributedPropagatorDistCV::step(int nsteps_, WlkSet& wset, RealType Eshift, RealType dt) 
{
  using ma::axpy;
  using std::fill_n;
  using std::copy_n;
  AFQMCTimers[setup_timer]->start();
  const ComplexType one(1.),zero(0.);
  auto walker_type = wset.getWalkerType();
  int nsteps= nsteps_;
  int nwalk = wset.size();
  RealType sqrtdt = std::sqrt(dt);  
  long Gsize = wfn.size_of_G_for_vbias();
  const int globalnCV = wfn.global_number_of_cholesky_vectors();
  const int localnCV = wfn.local_number_of_cholesky_vectors();
  const int global_origin = wfn.global_origin_cholesky_vector();
  const int nnodes = TG.getNGroupsPerTG();  
  const int node_number = TG.getLocalGroupNumber();
  // if transposed_XXX_=true  --> XXX[nwalk][...], 
  // if transposed_XXX_=false --> XXX[...][nwalk]
  int vhs_nr = NMO*NMO, vhs_nc = nwalk*nsteps;
  if(transposed_vHS_) std::swap(vhs_nr,vhs_nc);
  int vhs3d_n1 = NMO, vhs3d_n2 = NMO, vhs3d_n3 = nwalk*nsteps;
  if(transposed_vHS_) {
    vhs3d_n1 = nwalk*nsteps;
    vhs3d_n2 = vhs3d_n3 = NMO;
  }
  int G_nr = Gsize, G_nc = nwalk;
  if(transposed_G_) std::swap(G_nr,G_nc);

  //  Shared buffer used for:
  //  G_for_vbias:     [ Gsize * nwalk ] (2 copies)
  //  vbias:           [ localnCV * nwalk ] 
  //  X:               [ localnCV * nwalk * nstep ]
  //  vHS:             [ NMO*NMO * nwalk * nstep ] (3 copies)     
  // memory_needs: nwalk * ( 2*nsteps + Gsize + localnCV*(nstep+1) + NMO*NMO*nstep )
  size_t memory_needs = nwalk * ( 2*Gsize + size_t(localnCV)*(nsteps+1) + 3*NMO*NMO*nsteps );

  // 0. Allocate memory and set shared memory structures
  if(buffer.num_elements() < memory_needs ) {
    {
      buffer = std::move(sharedCVector(iextensions<1u>{memory_needs},aux_alloc_));
    }
    using std::fill_n;
    fill_n(buffer.origin(),buffer.num_elements(),ComplexType(0.0));
    buffer_reallocated=true;
    buffer_reallocated_bp=true;

    app_log()<<" Resizing buffer space in AFQMCDistributedPropagatorDistCV::step to " <<memory_needs*sizeof(ComplexType)/1024.0/1024.0 <<" MBs. \n";
    memory_report();
  } else  
    fill_n(buffer.origin(),buffer.num_elements(),ComplexType(0.0));
  TG.local_barrier();

  // convert array to a basic_array<element,1,pointer>. This generates a view of buffer 
  // with type basic_array<ComplexType,1,pointer> which eliminates the need to cast origin()  
  //auto mem_pool(boost::multi::static_array_cast<ComplexType, pointer>(buffer));
  CVector_ref mem_pool(make_device_ptr(buffer.origin()),buffer.extensions());

  size_t displ=0;
  // Mixed Density Matrix for walkers at original configuration
  CMatrix_ref Gwork(mem_pool.origin()+displ, {G_nr,G_nc}); 
    displ+=G_nr*G_nc; 
  // Mixed Density Matrix used for communication 
  CMatrix_ref Grecv(mem_pool.origin()+displ, {G_nr,G_nc}); 
    displ+=G_nr*G_nc; 
  // vias potential for walkers at original configuration
  CMatrix_ref vbias(mem_pool.origin()+displ, {long(localnCV),nwalk}); 
    displ+=localnCV*nwalk;
  // right hand side matrix in calculation of HS potential for all steps: ~ sigma + (vbias-vMF)
  // The same vbias is used in all steps
  CMatrix_ref X(mem_pool.origin()+displ, {long(localnCV),nwalk*nsteps}); 
    displ+=localnCV*nwalk*nsteps;
  // HS potential for all steps.
  CMatrix_ref vHS(mem_pool.origin()+displ, {vhs_nr,vhs_nc}); 
    displ+=vhs_nr*vhs_nc;
  // HS potential for communication 
  CMatrix_ref vsend(mem_pool.origin()+displ, {vhs_nr,vhs_nc}); 
    displ+=vhs_nr*vhs_nc;
  // HS potential for communication
  CMatrix_ref vrecv(mem_pool.origin()+displ, {vhs_nr,vhs_nc}); 
  // second view of vHS matrix for use in propagation step
  // notice that the final vHS matrix will be in vrecv, received in the last step
  C3Tensor_ref vHS3D(mem_pool.origin()+displ,{vhs3d_n1,vhs3d_n2,vhs3d_n3}); 

  // partition G and v for communications: all cores communicate a piece of the matrix
  int vak0,vakN;
  int Gak0,GakN;
  std::tie(Gak0,GakN) = FairDivideBoundary(TG.getLocalTGRank(),int(Gwork.num_elements()),TG.getNCoresPerTG());
  std::tie(vak0,vakN) = FairDivideBoundary(TG.getLocalTGRank(),int(vHS.num_elements()),TG.getNCoresPerTG());
  if(buffer_reallocated) {
    // use mpi3 when ready
    if(req_Grecv!=MPI_REQUEST_NULL)
        MPI_Request_free(&req_Grecv);
    if(req_Gsend!=MPI_REQUEST_NULL)
        MPI_Request_free(&req_Gsend);
    if(req_vrecv!=MPI_REQUEST_NULL)
        MPI_Request_free(&req_vrecv);
    if(req_vsend!=MPI_REQUEST_NULL)
        MPI_Request_free(&req_vsend);
    MPI_Send_init(to_address(Gwork.origin())+Gak0,(GakN-Gak0)*sizeof(ComplexType),MPI_CHAR,
                  TG.prev_core(),3456,&TG.TG(),&req_Gsend);
    MPI_Recv_init(to_address(Grecv.origin())+Gak0,(GakN-Gak0)*sizeof(ComplexType),MPI_CHAR,
                  TG.next_core(),3456,&TG.TG(),&req_Grecv);
    MPI_Send_init(to_address(vsend.origin())+vak0,(vakN-vak0)*sizeof(ComplexType),MPI_CHAR,
                  TG.prev_core(),5678,&TG.TG(),&req_vsend);
    MPI_Recv_init(to_address(vrecv.origin())+vak0,(vakN-vak0)*sizeof(ComplexType),MPI_CHAR,
                  TG.next_core(),5678,&TG.TG(),&req_vrecv);
    buffer_reallocated=false;
  }

  // local matrices for temporary accumulation
  if(MFfactor.size(0) != nnodes*nsteps || MFfactor.size(1) != nwalk)
    MFfactor = std::move(CMatrix({nnodes*nsteps,nwalk}));
  if(hybrid_weight.size(0) != nnodes*nsteps || hybrid_weight.size(1) != nwalk)
    hybrid_weight = std::move(CMatrix({nnodes*nsteps,nwalk}));
  if(new_overlaps.size(0) != nwalk) new_overlaps = std::move(CVector(iextensions<1u>{nwalk}));
  if(new_energies.size(0) != nwalk || new_energies.size(1) != 3) 
    new_energies = std::move(CMatrix({nwalk,3}));

  // if timestep changed, recalculate one body propagator
  if( std::abs(dt-old_dt) > 1e-6 )
    generateP1(dt,walker_type);
  TG.local_barrier();  

  fill_n(vsend.origin()+vak0,(vakN-vak0),zero);

  // are we back propagating?
  int bp_step=wset.getBPPos(), bp_max=wset.NumBackProp();
  bool realloc(false);
  int xx(0);
  if(bp_step >= 0 && bp_step<bp_max) {
    xx=1; 
    size_t m_(globalnCV*nwalk*nsteps*2);
    if(bpX.num_elements() < m_) {
      realloc=true;
      bpX = std::move(mpi3CVector(iextensions<1u>{m_},
                                  shared_allocator<ComplexType>{TG.TG_local()})); 
      if(TG.TG_local().root()) fill_n(bpX.origin(),bpX.num_elements(),ComplexType(0.0));
    }
  }
  stdCMatrix_ref Xsend(to_address(bpX.origin()), {long(globalnCV*xx),nwalk*nsteps});
  stdCMatrix_ref Xrecv(Xsend.origin()+Xsend.num_elements(), 
                                                 {long(globalnCV*xx),nwalk*nsteps});
  int Xg0,XgN;
  std::tie(Xg0,XgN) = FairDivideBoundary(TG.getLocalTGRank(),globalnCV*nwalk*nsteps,TG.getNCoresPerTG());
  int Xl0,XlN;
  std::tie(Xl0,XlN) = FairDivideBoundary(TG.getLocalTGRank(),localnCV*nwalk*nsteps,TG.getNCoresPerTG());
  int cv0,cvN;
  std::tie(cv0,cvN) = FairDivideBoundary(TG.getLocalTGRank(),localnCV,TG.getNCoresPerTG());
  if(realloc) {
    if(req_X2recv!=MPI_REQUEST_NULL)
      MPI_Request_free(&req_X2recv);
    if(req_X2send!=MPI_REQUEST_NULL)
      MPI_Request_free(&req_X2send);
    MPI_Send_init(to_address(Xsend.origin())+Xg0,(XgN-Xg0)*sizeof(ComplexType),MPI_CHAR,
                TG.prev_core(),3456,&TG.TG(),&req_X2send);
    MPI_Recv_init(to_address(Xrecv.origin())+Xg0,(XgN-Xg0)*sizeof(ComplexType),MPI_CHAR,
                TG.next_core(),3456,&TG.TG(),&req_X2recv);
  }

  TG.local_barrier();
  AFQMCTimers[setup_timer]->stop();

  MPI_Status st;

  // 1. Calculate Green function for all (local) walkers
  AFQMCTimers[G_for_vbias_timer]->start();
  wfn.MixedDensityMatrix_for_vbias(wset,Gwork);
  AFQMCTimers[G_for_vbias_timer]->stop();

  for(int k=0; k<nnodes; ++k) { 
  
    // 2. wait for communication of previous step 
    AFQMCTimers[vHS_comm_overhead_timer]->start();
    if(k>0) {
      MPI_Wait(&req_Grecv,&st);
      MPI_Wait(&req_Gsend,&st);     // need to wait for Gsend in order to overwrite Gwork  
      copy_n(Grecv.origin()+Gak0,GakN-Gak0,Gwork.origin()+Gak0);
      TG.local_barrier();
    }    

    // 3. setup next communication
    if(k < nnodes-1) {
      MPI_Start(&req_Gsend);
      MPI_Start(&req_Grecv);
    }
    AFQMCTimers[vHS_comm_overhead_timer]->stop();

    // calculate vHS contribution from this node
    // 4a. Calculate vbias for initial configuration
    AFQMCTimers[vbias_timer]->start();
    wfn.vbias(Gwork,vbias,sqrtdt);
    AFQMCTimers[vbias_timer]->stop();

    // 4b. Assemble X(nCV,nsteps,nwalk)
    AFQMCTimers[assemble_X_timer]->start();
    int q = (node_number+k)%nnodes;
    CMatrix_ref mf_(MFfactor[q*nsteps].origin(), {long(nsteps),long(nwalk)});
    CMatrix_ref hw_(hybrid_weight[q*nsteps].origin(), {long(nsteps),long(nwalk)});
    assemble_X(nsteps,nwalk,sqrtdt,X,vbias,mf_,hw_);
    TG.local_barrier();
    AFQMCTimers[assemble_X_timer]->stop();
    if(bp_step >= 0 && bp_step<bp_max) {
      // receive X 
      if(k > 0) {
        MPI_Wait(&req_X2recv,&st);
        MPI_Wait(&req_X2send,&st);
        copy_n(Xrecv.origin()+Xg0,XgN-Xg0,Xsend.origin()+Xg0);
        TG.local_barrier();
      }
      // accumulate 
      copy_n(X[cv0].origin(),nwalk*nsteps*(cvN-cv0),Xsend[global_origin+cv0].origin());
      TG.local_barrier();
      // start X communication
      MPI_Start(&req_X2send);
      MPI_Start(&req_X2recv);
    }

    // 4c. Calculate vHS(M*M,nsteps,nwalk)
    AFQMCTimers[vHS_timer]->start();
    wfn.vHS(X,vHS,sqrtdt);
    TG.local_barrier();
    AFQMCTimers[vHS_timer]->stop();

    AFQMCTimers[vHS_comm_overhead_timer]->start();
    // 5. receive v 
    if(k > 0) {
      MPI_Wait(&req_vrecv,&st);
      MPI_Wait(&req_vsend,&st);
      copy_n(vrecv.origin()+vak0,vakN-vak0,vsend.origin()+vak0);
    }

    // 6. add local contribution to vsend
    axpy(vakN-vak0,one,vHS.origin()+vak0,1,vsend.origin()+vak0,1);

    // 7. start v communication
    MPI_Start(&req_vsend);
    MPI_Start(&req_vrecv);
    TG.local_barrier();
    AFQMCTimers[vHS_comm_overhead_timer]->stop();
  }

  // after the wait, vrecv ( and by extention vHS3D ) has the final vHS for the local walkers
  AFQMCTimers[vHS_comm_overhead_timer]->start();
  MPI_Wait(&req_vrecv,&st);
  MPI_Wait(&req_vsend,&st);
  MPI_Wait(&req_X2recv,&st);
  MPI_Wait(&req_X2send,&st);

  // store fields in walker
  if(bp_step >= 0 && bp_step<bp_max) {
    int cvg0,cvgN;
    std::tie(cvg0,cvgN) = FairDivideBoundary(TG.getLocalTGRank(),globalnCV,TG.getNCoresPerTG());
    for(int ni=0; ni<nsteps; ni++) {
      if(bp_step<bp_max) {
        auto&& V(wset.getFields(bp_step));
        if(nsteps==1) {
          copy_n(Xrecv[cvg0].origin(),nwalk*(cvgN-cvg0),V[cvg0].origin());
          ma::scal(sqrtdt,V.sliced(cvg0,cvgN));
        } else {
          ma::add(ComplexType(0.0),   V.sliced(cvg0,cvgN),
                  ComplexType(sqrtdt),Xrecv( {cvg0,cvgN}, {ni*nwalk,(ni+1)*nwalk} ),
                                      V.sliced(cvg0,cvgN));
        }
        bp_step++;
      }
    }
    TG.TG_local().barrier();
  }

  // reduce MF and HWs
  if(TG.TG().size() > 1) {
    TG.TG().all_reduce_in_place_n(to_address(MFfactor.origin()),MFfactor.num_elements(),
                                  std::plus<>());
    TG.TG().all_reduce_in_place_n(to_address(hybrid_weight.origin()),
                                  hybrid_weight.num_elements(),std::plus<>());
  }  
  TG.local_barrier();
  AFQMCTimers[vHS_comm_overhead_timer]->stop();

  // From here on is similar to Shared 
  int nx = 1;
  if(walker_type == COLLINEAR) nx=2;

  // from now on, individual work on each walker/step
  const int ntasks_per_core = int(nx*nwalk)/TG.getNCoresPerTG();
  const int ntasks_total_serial = ntasks_per_core*TG.getNCoresPerTG();
  const int nextra = int(nx*nwalk) - ntasks_total_serial;

  // each processor does ntasks_percore_serial overlaps serially
  const int tk0 = TG.getLocalTGRank()*ntasks_per_core;
  const int tkN = (TG.getLocalTGRank()+1)*ntasks_per_core;

  // make new communicator if nextra changed from last setting
  reset_nextra(nextra);

  int n0 = node_number*nsteps_; 
  for(int ni=0; ni<nsteps_; ni++) {

    // 5. Propagate walkers
    AFQMCTimers[propagate_timer]->start();
    if(nbatched_propagation != 0) {
      apply_propagators_batched('N',wset,ni,vHS3D);
    } else {
      apply_propagators('N',wset,ni,tk0,tkN,ntasks_total_serial,vHS3D);
    }
    AFQMCTimers[propagate_timer]->stop();

    // 6. Calculate local energy/overlap
    AFQMCTimers[pseudo_energy_timer]->start();
    if(hybrid) {
      wfn.Overlap(wset,new_overlaps);
    } else {
      wfn.Energy(wset,new_energies,new_overlaps);
    }
    TG.local_barrier();
    AFQMCTimers[pseudo_energy_timer]->stop();

    // 7. update weights/energy/etc, apply constrains/bounds/etc 
    AFQMCTimers[extra_timer]->start();
    if(TG.TG_local().root()) {                           
      if(free_projection) { 
        free_projection_walker_update(wset,dt,new_overlaps,
                         MFfactor[n0+ni],Eshift,hybrid_weight[n0+ni],work);
      } else if(hybrid) {
        hybrid_walker_update(wset,dt,apply_constrain,importance_sampling,
                         Eshift,new_overlaps,MFfactor[n0+ni],hybrid_weight[n0+ni],work);
      } else {
        local_energy_walker_update(wset,dt,apply_constrain,Eshift,
                                   new_overlaps,new_energies,
                                   MFfactor[n0+ni],hybrid_weight[n0+ni],work);
      }
      if(wset.getBPPos() >=0 && wset.getBPPos()<wset.NumBackProp()) wset.advanceBPPos();
    }
    TG.local_barrier();
    AFQMCTimers[extra_timer]->stop();
  }  
}

// Distributed propagation based on collective communication
/*
 * Propagates the walker population nsteps forward with a fixed vbias (from the initial 
 * configuration).   
 */
template<class WlkSet>
void AFQMCDistributedPropagatorDistCV::step_collective(int nsteps_, WlkSet& wset, RealType Eshift, RealType dt)
{
  using ma::axpy;
  using std::fill_n;
  using std::copy_n;
  AFQMCTimers[setup_timer]->start();
  const ComplexType one(1.),zero(0.);
  auto walker_type = wset.getWalkerType();
  int nsteps= nsteps_;
  int nwalk = wset.size();
  RealType sqrtdt = std::sqrt(dt);
  long Gsize = wfn.size_of_G_for_vbias();
  const int globalnCV = wfn.global_number_of_cholesky_vectors();
  const int localnCV = wfn.local_number_of_cholesky_vectors();
  const int global_origin = wfn.global_origin_cholesky_vector();
  const int nnodes = TG.getNGroupsPerTG();
  const int ncores = TG.getNCoresPerTG();
  const int node_number = TG.getLocalGroupNumber();
  // if transposed_XXX_=true  --> XXX[nwalk][...], 
  // if transposed_XXX_=false --> XXX[...][nwalk]
  int vhs_nr = NMO*NMO, vhs_nc = nwalk*nsteps;
  if(transposed_vHS_) std::swap(vhs_nr,vhs_nc);
  int vhs3d_n1 = NMO, vhs3d_n2 = NMO, vhs3d_n3 = nwalk*nsteps;
  if(transposed_vHS_) {
    vhs3d_n1 = nwalk*nsteps;
    vhs3d_n2 = vhs3d_n3 = NMO;
  }
  int G_nr = Gsize, G_nc = nwalk;
  if(transposed_G_) std::swap(G_nr,G_nc);

  //  Shared buffer used for:
  //  G_for_vbias:     [ Gsize * nwalk ] (2 copies)
  //  vbias:           [ localnCV * nwalk ] 
  //  X:               [ localnCV * nwalk * nstep ]
  //  vHS:             [ NMO*NMO * nwalk * nstep ] (2 copies)     
  // memory_needs: nwalk * ( 2*nsteps + Gsize + localnCV*(nstep+1) + NMO*NMO*nstep )
  size_t memory_needs = nwalk * ( 2*Gsize + size_t(localnCV)*(nsteps+1) + 2*NMO*NMO*nsteps );

  // 0. Allocate memory and set shared memory structures
  if(buffer.num_elements() < memory_needs ) {
    {
      buffer = std::move(sharedCVector(iextensions<1u>{memory_needs},aux_alloc_));
    }
    if(TG.TG_local().root()) fill_n(buffer.origin(),buffer.num_elements(),ComplexType(0.0));
    buffer_reallocated=true;
    buffer_reallocated_bp=true;
    app_log()<<" Resizing buffer space in AFQMCDistributedPropagatorDistCV::step_collective to " <<memory_needs*sizeof(ComplexType)/1024.0/1024.0 <<" MBs. \n";
    memory_report();
  } else
    if(TG.TG_local().root()) fill_n(buffer.origin(),buffer.num_elements(),ComplexType(0.0));
  TG.local_barrier();

  // convert array to a basic_array<element,1,pointer>. This generates a view of buffer 
  // with type basic_array<ComplexType,1,pointer> which eliminates the need to cast origin()  
  //auto mem_pool(boost::multi::static_array_cast<ComplexType, pointer>(buffer));
  CVector_ref mem_pool(make_device_ptr(buffer.origin()),buffer.extensions());

  // partition G and v for communications: all cores communicate a piece of the matrix
  int vak0,vakN;
  int Gak0,GakN;

// NOTE: keep vHS, vbias and G as single precision numbers
// in mixe precision case, no need to keep them here in double precision.
// cast it using inplace_cast to/from double precision when needed.
  size_t displ=0;
  // HS potential for communication
  CMatrix_ref vrecv(mem_pool.origin(), {vhs_nr,vhs_nc});
  displ+=vrecv.num_elements();
  // second view of vHS matrix for use in propagation step
  // notice that the final vHS matrix will be in vrecv, received in the last step
  C3Tensor_ref vHS3D(vrecv.origin(),{vhs3d_n1,vhs3d_n2,vhs3d_n3});

  // scope controlling lifetime of temporary arrays
  {

    // Mixed Density Matrix for walkers at original configuration
    CMatrix_ref Gwork(mem_pool.origin()+displ, {G_nr,G_nc});
    displ+=G_nr*G_nc;
    // Mixed Density Matrix used for communication 
    CMatrix_ref Gstore(mem_pool.origin()+displ, {G_nr,G_nc});
      displ+=G_nr*G_nc;
    // vias potential for walkers at original configuration
    CMatrix_ref vbias(mem_pool.origin()+displ, {long(localnCV),nwalk});
      displ+=localnCV*nwalk;
    // right hand side matrix in calculation of HS potential for all steps: ~ sigma + (vbias-vMF)
    // The same vbias is used in all steps
    CMatrix_ref X(mem_pool.origin()+displ, {long(localnCV),nwalk*nsteps});
      displ+=localnCV*nwalk*nsteps;
    // HS potential for all steps.
    CMatrix_ref vHS(mem_pool.origin()+displ, {vhs_nr,vhs_nc});
      displ+=vhs_nr*vhs_nc;

    std::tie(Gak0,GakN) = FairDivideBoundary(TG.getLocalTGRank(),int(Gwork.num_elements()),TG.getNCoresPerTG());
    std::tie(vak0,vakN) = FairDivideBoundary(TG.getLocalTGRank(),int(vHS.num_elements()),TG.getNCoresPerTG());

    // local matrices for temporary accumulation
    if(MFfactor.size(0) != nnodes*nsteps || MFfactor.size(1) != nwalk)
      MFfactor = std::move(CMatrix({nnodes*nsteps,nwalk}));
    if(hybrid_weight.size(0) != nnodes*nsteps || hybrid_weight.size(1) != nwalk)
      hybrid_weight = std::move(CMatrix({nnodes*nsteps,nwalk}));
    if(new_overlaps.size(0) != nwalk) new_overlaps = std::move(CVector(iextensions<1u>{nwalk}));
    if(new_energies.size(0) != nwalk || new_energies.size(1) != 3)
      new_energies = std::move(CMatrix({nwalk,3}));

    // if timestep changed, recalculate one body propagator
    if( std::abs(dt-old_dt) > 1e-6 )
      generateP1(dt,walker_type);
    TG.local_barrier();

    // are we back propagating?
    int bp_step=wset.getBPPos(), bp_max=wset.NumBackProp();
    bool realloc(false);
    int xx(0);
    if(bp_step >= 0 && bp_step<bp_max) {
      xx=1;
      size_t m_(globalnCV*nwalk*nsteps);
      if(bpX.num_elements() < m_) {
        realloc=true;
        bpX = std::move(mpi3CVector(iextensions<1u>{m_},
                                  shared_allocator<ComplexType>{TG.TG_local()}));
        if(TG.TG_local().root()) fill_n(bpX.origin(),bpX.num_elements(),ComplexType(0.0));
      }
    }
    stdCMatrix_ref Xrecv(to_address(bpX.origin()), {long(globalnCV*xx),nwalk*nsteps});
    int Xg0,XgN;
    std::tie(Xg0,XgN) = FairDivideBoundary(TG.getLocalTGRank(),globalnCV*nwalk*nsteps,TG.getNCoresPerTG());
    int Xl0,XlN;
    std::tie(Xl0,XlN) = FairDivideBoundary(TG.getLocalTGRank(),localnCV*nwalk*nsteps,TG.getNCoresPerTG());
    int cv0,cvN;
    std::tie(cv0,cvN) = FairDivideBoundary(TG.getLocalTGRank(),localnCV,TG.getNCoresPerTG());

    // setup counts, it is possible to keep old one if nwalk*nsteps*(cvN-cv0) doesn't change.
    // what can I assume?
    if(bp_step >= 0 && bp_step<bp_max) {
      bpx_counts = TG.TG().all_gather_value(int(nwalk*nsteps*(cvN-cv0)));
      if(TG.TG_local().root()) {
        bpx_displ.reserve(bpx_counts.size());
        bpx_displ.clear();
        for(int i=0, s=0; i<bpx_counts.size(); i++) {
          bpx_displ.push_back(s);
          s+=bpx_counts[i];
        }
      }
    }

    TG.local_barrier();
    AFQMCTimers[setup_timer]->stop();

    MPI_Status st;

    // 1. Calculate Green function for all (local) walkers
    AFQMCTimers[G_for_vbias_timer]->start();
    wfn.MixedDensityMatrix_for_vbias(wset,Gstore);
    AFQMCTimers[G_for_vbias_timer]->stop();

    for(int k=0; k<nnodes; ++k) {

      // 2. bcast G 
      AFQMCTimers[vHS_comm_overhead_timer]->start();
#ifdef BUILD_AFQMC_WITH_NCCL
      if(k == TG.getLocalTGRank()) 
        copy_n(Gstore.origin()+Gak0,GakN-Gak0,Gwork.origin()+Gak0);
      NCCLCHECK(ncclBcast(to_address(Gwork.origin())+Gak0,2*(GakN-Gak0),ncclDouble,k,
                          TG.ncclTG(),TG.ncclStream()));
      qmc_cuda::cuda_check(cudaStreamSynchronize(TG.ncclStream()),"cudaStreamSynchronize(s)");
      //TG.TG_Cores().broadcast_n(to_address(Gwork.origin())+Gak0,GakN-Gak0,k);
#else
      if(k == TG.getLocalTGRank()) 
        copy_n(Gstore.origin()+Gak0,GakN-Gak0,Gwork.origin()+Gak0);
      TG.TG_Cores().broadcast_n(to_address(Gwork.origin())+Gak0,GakN-Gak0,k);
#endif
      TG.local_barrier();
      AFQMCTimers[vHS_comm_overhead_timer]->stop();

      // calculate vHS contribution from this node
      // 3a. Calculate vbias for initial configuration
      AFQMCTimers[vbias_timer]->start();
      wfn.vbias(Gwork,vbias,sqrtdt);
      AFQMCTimers[vbias_timer]->stop();

      // 3b. Assemble X(nCV,nsteps,nwalk)
      AFQMCTimers[assemble_X_timer]->start();
      CMatrix_ref mf_(MFfactor[k*nsteps].origin(), {long(nsteps),long(nwalk)});
      CMatrix_ref hw_(hybrid_weight[k*nsteps].origin(), {long(nsteps),long(nwalk)});
      assemble_X(nsteps,nwalk,sqrtdt,X,vbias,mf_,hw_);
      TG.local_barrier();
      AFQMCTimers[assemble_X_timer]->stop();
      if(bp_step >= 0 && bp_step<bp_max) {
        APP_ABORT("Finish");
        // accumulate 
        //copy_n(X[cv0].origin(),nwalk*nsteps*(cvN-cv0),Xsend[global_origin+cv0].origin());
        // how do I know if these change???
        // possible if 2 execute blocks use different ncores
        // maybe store last ncores and change if this number changes!!!
        TG.TG().gatherv_n(to_address(X[cv0].origin()),nwalk*nsteps*(cvN-cv0),
                          to_address(Xrecv.origin()), bpx_counts.begin(), bpx_displ.begin(),
                          k*ncores);
        TG.local_barrier();
      }

      // 3c. Calculate vHS(M*M,nsteps,nwalk)
      AFQMCTimers[vHS_timer]->start();
      wfn.vHS(X,vHS,sqrtdt);
      TG.local_barrier();
      AFQMCTimers[vHS_timer]->stop();

      AFQMCTimers[vHS_comm_overhead_timer]->start();
      // 4. Reduce vHS
#ifdef BUILD_AFQMC_WITH_NCCL
      NCCLCHECK(ncclReduce((const void*)to_address(vHS.origin())+vak0,
                    (void*)to_address(vrecv.origin())+vak0, 2*(vakN-vak0),
                     ncclDouble, ncclSum,k,TG.ncclTG(),TG.ncclStream()));
      qmc_cuda::cuda_check(cudaStreamSynchronize(TG.ncclStream()),"cudaStreamSynchronize(s)");
#else
      TG.TG_Cores().reduce_n(to_address(vHS.origin())+vak0,vakN-vak0,
                             to_address(vrecv.origin())+vak0,std::plus<>(),k);
#endif

      TG.local_barrier();
      AFQMCTimers[vHS_comm_overhead_timer]->stop();
    }

    // after the wait, vrecv ( and by extention vHS3D ) has the final vHS for the local walkers
    AFQMCTimers[vHS_comm_overhead_timer]->start();

    // store fields in walker
    if(bp_step >= 0 && bp_step<bp_max) {
      int cvg0,cvgN;
      std::tie(cvg0,cvgN) = FairDivideBoundary(TG.getLocalTGRank(),globalnCV,TG.getNCoresPerTG());
      for(int ni=0; ni<nsteps; ni++) {
        if(bp_step<bp_max) {
          auto&& V(wset.getFields(bp_step));
          if(nsteps==1) {
            copy_n(Xrecv[cvg0].origin(),nwalk*(cvgN-cvg0),V[cvg0].origin());
            ma::scal(sqrtdt,V.sliced(cvg0,cvgN));
          } else {
            ma::add(ComplexType(0.0),   V.sliced(cvg0,cvgN),
                  ComplexType(sqrtdt),Xrecv( {cvg0,cvgN}, {ni*nwalk,(ni+1)*nwalk} ),
                                      V.sliced(cvg0,cvgN));
          }
          bp_step++;
        }
      }
      TG.TG_local().barrier();
    }
    // reduce MF and HWs
    if(TG.TG().size() > 1) {
#ifdef BUILD_AFQMC_WITH_NCCL
      NCCLCHECK(ncclAllReduce((const void*)to_address(MFfactor.origin()),
                    (void*)to_address(MFfactor.origin()), 2*MFfactor.num_elements(),
                     ncclDouble, ncclSum,TG.ncclTG(),TG.ncclStream()));
      NCCLCHECK(ncclAllReduce((const void*)to_address(hybrid_weight.origin()),
                     (void*)to_address(hybrid_weight.origin()), 2*hybrid_weight.num_elements(),
                     ncclDouble, ncclSum,TG.ncclTG(),TG.ncclStream()));
      qmc_cuda::cuda_check(cudaStreamSynchronize(TG.ncclStream()),"cudaStreamSynchronize(s)");
#else
      TG.TG().all_reduce_in_place_n(to_address(MFfactor.origin()),MFfactor.num_elements(),
                                  std::plus<>());
      TG.TG().all_reduce_in_place_n(to_address(hybrid_weight.origin()),
                                    hybrid_weight.num_elements(),std::plus<>());
#endif
    }
    TG.local_barrier();
    AFQMCTimers[vHS_comm_overhead_timer]->stop();
  } // scope controlling lifetime of temporary arrays

  // From here on is similar to Shared 
  int nx = 1;
  if(walker_type == COLLINEAR) nx=2;

  // from now on, individual work on each walker/step
  const int ntasks_per_core = int(nx*nwalk)/TG.getNCoresPerTG();
  const int ntasks_total_serial = ntasks_per_core*TG.getNCoresPerTG();
  const int nextra = int(nx*nwalk) - ntasks_total_serial;

  // each processor does ntasks_percore_serial overlaps serially
  const int tk0 = TG.getLocalTGRank()*ntasks_per_core;
  const int tkN = (TG.getLocalTGRank()+1)*ntasks_per_core;

  // make new communicator if nextra changed from last setting
  reset_nextra(nextra);

  int n0 = node_number*nsteps_;
  for(int ni=0; ni<nsteps_; ni++) {

    // 5. Propagate walkers
    AFQMCTimers[propagate_timer]->start();
    if(nbatched_propagation != 0) {
      apply_propagators_batched('N',wset,ni,vHS3D);
    } else {
      apply_propagators('N',wset,ni,tk0,tkN,ntasks_total_serial,vHS3D);
    }
    AFQMCTimers[propagate_timer]->stop();

    // 6. Calculate local energy/overlap
    AFQMCTimers[pseudo_energy_timer]->start();
    if(hybrid) {
      wfn.Overlap(wset,new_overlaps);
    } else {
      wfn.Energy(wset,new_energies,new_overlaps);
    }
    TG.local_barrier();
    AFQMCTimers[pseudo_energy_timer]->stop();

    // 7. update weights/energy/etc, apply constrains/bounds/etc 
    AFQMCTimers[extra_timer]->start();
    if(TG.TG_local().root()) {
      if(free_projection) {
        free_projection_walker_update(wset,dt,new_overlaps,
                         MFfactor[n0+ni],Eshift,hybrid_weight[n0+ni],work);
      } else if(hybrid) {
        hybrid_walker_update(wset,dt,apply_constrain,importance_sampling,
                         Eshift,new_overlaps,MFfactor[n0+ni],hybrid_weight[n0+ni],work);
      } else {
        local_energy_walker_update(wset,dt,apply_constrain,Eshift,
                                   new_overlaps,new_energies,
                                   MFfactor[n0+ni],hybrid_weight[n0+ni],work);
      }
      if(wset.getBPPos() >=0 && wset.getBPPos()<wset.NumBackProp()) wset.advanceBPPos();
    }
    TG.local_barrier();
    AFQMCTimers[extra_timer]->stop();
  }
}

/*
 * This routine assumes that the 1 body propagator does not need updating
 */
template<class WlkSet, class CTens, class CMat>
void AFQMCDistributedPropagatorDistCV::BackPropagate(int nbpsteps, int nStabalize, WlkSet& wset, CTens&& Refs, CMat&& detR)
{
  using std::copy_n;
  const ComplexType one(1.),zero(0.);
  auto walker_type = wset.getWalkerType();
  const int nwalk = wset.size();
  const int globalnCV = wfn.global_number_of_cholesky_vectors();
  const int localnCV = wfn.local_number_of_cholesky_vectors();
  const int global_origin = wfn.global_origin_cholesky_vector();
  const int nnodes = TG.getNGroupsPerTG();  

  int vhs_nr = NMO*NMO, vhs_nc = nwalk;
  if(transposed_vHS_) std::swap(vhs_nr,vhs_nc);
  int vhs3d_n1 = NMO, vhs3d_n2 = NMO, vhs3d_n3 = nwalk;
  if(transposed_vHS_) {
    vhs3d_n1 = nwalk;
    vhs3d_n2 = vhs3d_n3 = NMO;
  }

  //  Shared buffer used for:
  //  X:               [ (localnCV + 2*globalnCV) * nwalk ] 
  //  vHS:             [ NMO*NMO * nwalk ] (3 copies)     
  // memory_needs: nwalk * ( localnCV + NMO*NMO )
  size_t memory_needs = nwalk * ( 3*globalnCV + 3*NMO*NMO );

  // 0. Allocate memory and set shared memory structures
  using std::fill_n;
  using std::copy_n;
  // this should never be true
  if(buffer.num_elements() < memory_needs ) { 
    buffer = std::move(sharedCVector(iextensions<1u>{memory_needs},aux_alloc_));
    using std::fill_n;
    fill_n(buffer.origin(),buffer.num_elements(),ComplexType(0.0));
    buffer_reallocated=true;
    buffer_reallocated_bp=true;
  }  else
    fill_n(buffer.origin(),buffer.num_elements(),ComplexType(0.0));
  TG.local_barrier();

  // convert array to a basic_array<element,1,pointer>. This generates a view of buffer 
  // with type basic_array<ComplexType,1,pointer> which eliminates the need to cast origin()  
  //auto mem_pool(boost::multi::static_array_cast<ComplexType, pointer>(buffer));
  CVector_ref mem_pool(make_device_ptr(buffer.origin()),buffer.extensions());

  size_t displ=0;
  CMatrix_ref X(mem_pool.origin()+displ, {long(localnCV),long(nwalk)}); 
    displ+=X.num_elements();
  CMatrix_ref Xsend(mem_pool.origin()+displ, {long(globalnCV),long(nwalk)}); 
    displ+=Xsend.num_elements();
  CMatrix_ref Xrecv(mem_pool.origin()+displ, {long(globalnCV),long(nwalk)}); 
    displ+=Xrecv.num_elements();
  // HS potential for all steps.
  CMatrix_ref vHS(mem_pool.origin()+displ, {vhs_nr,vhs_nc}); 
    displ+=vhs_nr*vhs_nc;
  // HS potential for communication 
  CMatrix_ref vsend(mem_pool.origin()+displ, {vhs_nr,vhs_nc}); 
    displ+=vhs_nr*vhs_nc;
  // HS potential for communication
  CMatrix_ref vrecv(mem_pool.origin()+displ, {vhs_nr,vhs_nc}); 
  // second view of vHS matrix for use in propagation step
  // notice that the final vHS matrix will be in vrecv, received in the last step
  C3Tensor_ref vHS3D(mem_pool.origin()+displ,{vhs3d_n1,vhs3d_n2,vhs3d_n3}); 

  // partition G and v for communications: all cores communicate a piece of the matrix
  int vak0,vakN;
  int X0,XN;
  std::tie(X0,XN) = FairDivideBoundary(TG.getLocalTGRank(),int(Xsend.num_elements()),TG.getNCoresPerTG());
  std::tie(vak0,vakN) = FairDivideBoundary(TG.getLocalTGRank(),int(vHS.num_elements()),TG.getNCoresPerTG());
  if(buffer_reallocated_bp) {
    // use mpi3 when ready
    if(req_Xrecv!=MPI_REQUEST_NULL)
        MPI_Request_free(&req_Xrecv);
    if(req_Xsend!=MPI_REQUEST_NULL)
        MPI_Request_free(&req_Xsend);
    if(req_bpvrecv!=MPI_REQUEST_NULL)
        MPI_Request_free(&req_bpvrecv);
    if(req_bpvsend!=MPI_REQUEST_NULL)
        MPI_Request_free(&req_bpvsend);
// MAM: should be sent as ComplexType, so get larger message upper bound size
    MPI_Send_init(to_address(Xsend.origin())+X0,(XN-X0)*sizeof(ComplexType),MPI_CHAR,
                  TG.prev_core(),2345,&TG.TG(),&req_Xsend);
    MPI_Recv_init(to_address(Xrecv.origin())+X0,(XN-X0)*sizeof(ComplexType),MPI_CHAR,
                  TG.next_core(),2345,&TG.TG(),&req_Xrecv);
    MPI_Send_init(to_address(vsend.origin())+vak0,(vakN-vak0)*sizeof(ComplexType),MPI_CHAR,
                  TG.prev_core(),6789,&TG.TG(),&req_bpvsend);
    MPI_Recv_init(to_address(vrecv.origin())+vak0,(vakN-vak0)*sizeof(ComplexType),MPI_CHAR,
                  TG.next_core(),6789,&TG.TG(),&req_bpvrecv);
    buffer_reallocated_bp=false;
  }
  TG.local_barrier();

  auto&& Fields(wset.getFields());
  assert(Fields.size(0) >= nbpsteps);
  assert(Fields.size(1) == globalnCV);
  assert(Fields.size(2) == nwalk);

  int nrow(NMO*((walker_type==NONCOLLINEAR)?2:1));
  int ncol(NAEA+((walker_type==CLOSED)?0:NAEB));
  assert(Refs.size(0) == nwalk);
  int nrefs = Refs.size(1);
  assert(Refs.size(2) == nrow*ncol);

  int cv0,cvN;
  std::tie(cv0,cvN) = FairDivideBoundary(TG.getLocalTGRank(),localnCV,TG.getNCoresPerTG());
  int r0,rN;
  std::tie(r0,rN) = FairDivideBoundary(TG.getLocalTGRank(),nrow*ncol,TG.getNCoresPerTG());

  MPI_Status st;

  int nx = 1;
  if(walker_type == COLLINEAR) nx=2;

  assert(detR.size(0) == nwalk);
  assert(detR.size(1) == nrefs*nx);
  std::fill_n(detR.origin(),detR.num_elements(),ComplexType(1.0,0.0));

  // from now on, individual work on each walker/step
  const int ntasks_per_core = int(nx*nwalk)/TG.getNCoresPerTG();
  const int ntasks_total_serial = ntasks_per_core*TG.getNCoresPerTG();
  const int nextra = int(nx*nwalk) - ntasks_total_serial;

  // each processor does ntasks_percore_serial overlaps serially
  const int tk0 = TG.getLocalTGRank()*ntasks_per_core;
  const int tkN = (TG.getLocalTGRank()+1)*ntasks_per_core;

  // make new communicator if nextra changed from last setting
  reset_nextra(nextra);

  // to avoid having to modify the existing routines, 
  // I'm storing the walkers SlaterMatrix on SlaterMatrixAux
  // and copying the back propagated references into SlaterMatrix
  // 0. copy SlaterMatrix to SlaterMatrixAux 
  for(int i=0; i<nwalk; i++) {
    copy_n(wset[i].SlaterMatrix(Alpha).origin()+r0,rN-r0,
           wset[i].SlaterMatrixAux(Alpha).origin()+r0);
    // optimize for the single reference case
    if(nrefs==1)
      copy_n(Refs[i][0].origin()+r0,rN-r0,
             wset[i].SlaterMatrix(Alpha).origin()+r0);
  }
  TG.TG_local().barrier();

  for(int ni=nbpsteps-1; ni>=0; --ni) {

    // 1. Get X(nCV,nwalk) from wset
    fill_n(vsend.origin()+vak0,(vakN-vak0),zero);
    copy_n(Fields[ni].origin()+X0,(XN-X0),Xsend.origin()+X0);
    TG.TG_local().barrier();
    copy_n(Xsend[global_origin+cv0].origin(),nwalk*(cvN-cv0),X[cv0].origin());
    TG.TG_local().barrier();
//std::cout<<" X0: " <<TG.Global().rank() <<" " <<ma::dot(Xsend(Xsend.extension(0),0),Xsend(Xsend.extension(0),0)) <<" " <<ma::dot(X(X.extension(0),0),X(X.extension(0),0)) <<"\n\n" <<std::endl;
    // 2. Calculate vHS(M*M,nwalk)/vHS(nwalk,M*M) using distributed algorithm 
    for(int k=0; k<nnodes; ++k) {

      // 2.1 wait for communication of previous step 
      if(k>0) {
        MPI_Wait(&req_Xrecv,&st);
        MPI_Wait(&req_Xsend,&st);     // need to wait for Gsend in order to overwrite Gwork  
        copy_n(Xrecv.origin()+X0,XN-X0,Xsend.origin()+X0);
        TG.local_barrier();
        copy_n(Xsend[global_origin+cv0].origin(),nwalk*(cvN-cv0),X[cv0].origin());
        TG.local_barrier();
      }

      // 2.2 setup next communication
      if(k < nnodes-1) {
        MPI_Start(&req_Xsend);
        MPI_Start(&req_Xrecv);
      }

      // 2.3 Calculate vHS
//std::cout<<" k, X: " <<TG.Global().rank() <<" " <<k <<" " <<ma::dot(X(X.extension(0),0),X(X.extension(0),0)) <<"\n\n" <<std::endl;
      wfn.vHS(X,vHS);
//std::cout<<" k, vHS: " <<TG.Global().rank() <<" " <<k <<" " <<ma::dot(vHS[0],vHS[0]) <<"\n\n" <<std::endl;

      // 2.4 receive v 
      if(k > 0) {
        MPI_Wait(&req_bpvrecv,&st);
        MPI_Wait(&req_bpvsend,&st);
        copy_n(vrecv.origin()+vak0,vakN-vak0,vsend.origin()+vak0);
      }
      TG.local_barrier();

      // 2.5 add local contribution to vsend
      using ma::axpy;
      axpy(vakN-vak0,one,vHS.origin()+vak0,1,vsend.origin()+vak0,1);
//std::cout<<" k vsend: " <<TG.Global().rank() <<" " <<k <<" " <<ma::dot(vsend[0],vsend[0]) <<"\n\n" <<std::endl;

      // 2.6 start v communication
      MPI_Start(&req_bpvsend);
      MPI_Start(&req_bpvrecv);
      TG.local_barrier();
    }

    MPI_Wait(&req_bpvrecv,&st);
    MPI_Wait(&req_bpvsend,&st);
    TG.local_barrier();
//std::cout<<" vrecv: " <<TG.Global().rank() <<" " <<ma::dot(vrecv[0],vrecv[0]) <<"\n\n" <<std::endl;

/**/
    for(int nr=0; nr<nrefs; ++nr) {

      // 3. copy reference to SlaterMatrix
      if(nrefs>1)
        for(int i=0; i<nwalk; i++)
          copy_n(Refs[i][nr].origin()+r0,rN-r0,
                 wset[i].SlaterMatrix(Alpha).origin()+r0);
      TG.TG_local().barrier();

      // 4. Propagate walkers
      if(nbatched_propagation != 0)
        apply_propagators_batched('H',wset,0,vHS3D);
      else
        apply_propagators('H',wset,0,tk0,tkN,ntasks_total_serial,vHS3D);
      TG.local_barrier();

      // always end (ni==0) with orthogonalization
      if(ni==0 || ni%nStabalize==0) {
        // orthogonalize
        if(nbatched_qr != 0) {
          if(walker_type!=COLLINEAR)
            Orthogonalize_batched(wset,detR(detR.extension(0),{nr,nr+1}));
          else
            Orthogonalize_batched(wset,detR(detR.extension(0),{2*nr,2*nr+2}));
        } else {
          if(walker_type!=COLLINEAR)
            Orthogonalize_shared(wset,detR(detR.extension(0),{nr,nr+1}));
          else
            Orthogonalize_shared(wset,detR(detR.extension(0),{2*nr,2*nr+2}));
        }
      }

      // 5. copy reference to back 
      if(nrefs>1)
        for(int i=0; i<nwalk; i++)
          copy_n(wset[i].SlaterMatrix(Alpha).origin()+r0,rN-r0,
                 Refs[i][nr].origin()+r0);
      TG.TG_local().barrier();

    }

  }

  // 6. restore the Slater Matrix 
  for(int i=0; i<nwalk; i++) {
    if(nrefs==1)
      copy_n(wset[i].SlaterMatrix(Alpha).origin()+r0,rN-r0,
             Refs[i][0].origin()+r0);
    copy_n(wset[i].SlaterMatrixAux(Alpha).origin()+r0,rN-r0,
           wset[i].SlaterMatrix(Alpha).origin()+r0);
  }
  TG.TG_local().barrier();
}


}

}

