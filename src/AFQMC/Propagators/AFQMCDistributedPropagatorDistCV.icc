//////////////////////////////////////////////////////////////////////
// This file is distributed under the University of Illinois/NCSA Open Source
// License.  See LICENSE file in top directory for details.
//
// Copyright (c) 2016 Jeongnim Kim and QMCPACK developers.
//
// File developed by:
// Miguel A. Morales, moralessilva2@llnl.gov
//    Lawrence Livermore National Laboratory
//
// File created by:
// Miguel A. Morales, moralessilva2@llnl.gov
//    Lawrence Livermore National Laboratory
////////////////////////////////////////////////////////////////////////////////

#include <vector>
#include <map>
#include <string>
#include <iostream>
#include <tuple>

#include "AFQMC/config.h"
#include "AFQMC/Utilities/Utils.hpp"
#include "AFQMC/Propagators/WalkerSetUpdate.hpp"
#include "AFQMC/Walkers/WalkerConfig.hpp"
#include "AFQMC/Numerics/ma_blas.hpp"

#include "Utilities/Timer.h"

namespace qmcplusplus
{
namespace afqmc
{
/*
 * Propagates the walker population nsteps forward with a fixed vbias (from the initial 
 * configuration).   
 */
template<class WlkSet>
void AFQMCDistributedPropagatorDistCV::step(int nsteps_, WlkSet& wset, RealType Eshift, RealType dt)
{
  using ma::axpy;
  using std::copy_n;
  using std::fill_n;
  AFQMCTimers[setup_timer]->start();
  const SPComplexType one(1.), zero(0.);
  auto walker_type        = wset.getWalkerType();
  int nsteps              = nsteps_;
  int nwalk               = wset.size();
  RealType sqrtdt         = std::sqrt(dt);
  long Gsize              = wfn.size_of_G_for_vbias();
  const int globalnCV     = wfn.global_number_of_cholesky_vectors();
  const int localnCV      = wfn.local_number_of_cholesky_vectors();
  const int global_origin = wfn.global_origin_cholesky_vector();
  const int nnodes        = TG.getNGroupsPerTG();
  const int node_number   = TG.getLocalGroupNumber();
  // if transposed_XXX_=true  --> XXX[nwalk][...],
  // if transposed_XXX_=false --> XXX[...][nwalk]
  auto vhs_ext   = iextensions<2u>{NMO * NMO, nwalk * nsteps};
  auto vhs3d_ext = iextensions<3u>{NMO, NMO, nwalk * nsteps};
  auto G_ext     = iextensions<2u>{Gsize, nwalk};
  if (transposed_vHS_)
  {
    vhs_ext   = iextensions<2u>{nwalk * nsteps, NMO * NMO};
    vhs3d_ext = iextensions<3u>{nwalk * nsteps, NMO, NMO};
  }
  if (transposed_G_)
    G_ext = iextensions<2u>{nwalk, Gsize};

  if (MFfactor.size(0) != nsteps || MFfactor.size(1) != nwalk)
    MFfactor = CMatrix({long(nsteps), long(nwalk)});
  if (hybrid_weight.size(0) != nsteps || hybrid_weight.size(1) != nwalk)
    hybrid_weight = CMatrix({long(nsteps), long(nwalk)});
  if (new_overlaps.size(0) != nwalk)
    new_overlaps = CVector(iextensions<1u>{nwalk});
  if (new_energies.size(0) != nwalk || new_energies.size(1) != 3)
    new_energies = CMatrix({long(nwalk), 3});

  //  Temporary memory usage summary:
  //  G_for_vbias:     [ Gsize * nwalk ] (2 copies)
  //  vbias:           [ localnCV * nwalk ]
  //  X:               [ localnCV * nwalk * nstep ]
  //  vHS:             [ NMO*NMO * nwalk * nstep ] (3 copies)
  // memory_needs: nwalk * ( 2*nsteps + Gsize + localnCV*(nstep+1) + NMO*NMO*nstep )

  // if timestep changed, recalculate one body propagator
  if (std::abs(dt - old_dt) > 1e-6)
    generateP1(dt, walker_type);
  TG.local_barrier();

  StaticMatrix vrecv_buff(vhs_ext, buffer_manager.get_generator().template get_allocator<ComplexType>());
  SPCMatrix_ref vrecv(sp_pointer(make_device_ptr(vrecv_buff.origin())), vhs_ext);

  { // using scope to control lifetime of StaticArrays, avoiding unnecesary buffer space

    Static3Tensor globalMFfactor({nnodes, nsteps, nwalk},
                                 buffer_manager.get_generator().template get_allocator<ComplexType>());
    Static3Tensor globalhybrid_weight({nnodes, nsteps, nwalk},
                                      buffer_manager.get_generator().template get_allocator<ComplexType>());
    StaticSPMatrix Gwork(G_ext, buffer_manager.get_generator().template get_allocator<SPComplexType>());

    // 1. Calculate Green function for all (local) walkers
    AFQMCTimers[G_for_vbias_timer]->start();
#if defined(MIXED_PRECISION)
    { // control scope of Gc
      int Gak0, GakN;
      std::tie(Gak0, GakN) = FairDivideBoundary(TG.getLocalTGRank(), int(Gwork.num_elements()), TG.getNCoresPerTG());
      StaticMatrix Gc(G_ext, buffer_manager.get_generator().template get_allocator<ComplexType>());
      wfn.MixedDensityMatrix_for_vbias(wset, Gc);
      copy_n_cast(make_device_ptr(Gc.origin()) + Gak0, GakN - Gak0, make_device_ptr(Gwork.origin()) + Gak0);
    }
    TG.local_barrier();
#else
    wfn.MixedDensityMatrix_for_vbias(wset, Gwork);
#endif
    AFQMCTimers[G_for_vbias_timer]->stop();


    StaticSPMatrix Grecv(G_ext, buffer_manager.get_generator().template get_allocator<SPComplexType>());
    StaticSPMatrix vbias({long(localnCV), long(nwalk)},
                         buffer_manager.get_generator().template get_allocator<SPComplexType>());
    StaticSPMatrix X({long(localnCV), long(nwalk * nsteps)},
                     buffer_manager.get_generator().template get_allocator<SPComplexType>());
#if defined(MIXED_PRECISION)
    // in MIXED_PRECISION, use second half of vrecv_buff for vsend
    SPCMatrix_ref vsend(sp_pointer(make_device_ptr(vrecv_buff.origin())) + vrecv_buff.num_elements(), vhs_ext);
#else
    StaticSPMatrix vsend(vhs_ext, buffer_manager.get_generator().template get_allocator<SPComplexType>());
#endif
    StaticSPMatrix vHS(vhs_ext, buffer_manager.get_generator().template get_allocator<SPComplexType>());

    // partition G and v for communications: all cores communicate a piece of the matrix
    int vak0, vakN;
    int Gak0, GakN;
    std::tie(Gak0, GakN) = FairDivideBoundary(TG.getLocalTGRank(), int(Gwork.num_elements()), TG.getNCoresPerTG());
    std::tie(vak0, vakN) = FairDivideBoundary(TG.getLocalTGRank(), int(vHS.num_elements()), TG.getNCoresPerTG());
    MPI_Send_init(to_address(Gwork.origin()) + Gak0, (GakN - Gak0) * sizeof(SPComplexType), MPI_CHAR, TG.prev_core(),
                  3456, &TG.TG(), &req_Gsend);
    MPI_Recv_init(to_address(Grecv.origin()) + Gak0, (GakN - Gak0) * sizeof(SPComplexType), MPI_CHAR, TG.next_core(),
                  3456, &TG.TG(), &req_Grecv);
    MPI_Send_init(to_address(vsend.origin()) + vak0, (vakN - vak0) * sizeof(SPComplexType), MPI_CHAR, TG.prev_core(),
                  5678, &TG.TG(), &req_vsend);
    MPI_Recv_init(to_address(vrecv.origin()) + vak0, (vakN - vak0) * sizeof(SPComplexType), MPI_CHAR, TG.next_core(),
                  5678, &TG.TG(), &req_vrecv);

    fill_n(make_device_ptr(vsend.origin()) + vak0, (vakN - vak0), zero);

    // are we back propagating?
    int bp_step = wset.getBPPos(), bp_max = wset.NumBackProp();
    bool realloc(false);
    int xx(0);
    if (bp_step >= 0 && bp_step < bp_max)
    {
      xx = 1;
      size_t m_(globalnCV * nwalk * nsteps * 2);
      if (bpX.num_elements() < m_)
      {
        realloc = true;
        bpX     = mpi3SPCVector(iextensions<1u>{m_}, shared_allocator<SPComplexType>{TG.TG_local()});
        if (TG.TG_local().root())
          fill_n(bpX.origin(), bpX.num_elements(), SPComplexType(0.0));
      }
    }
    stdSPCMatrix_ref Xsend(to_address(bpX.origin()), {long(globalnCV * xx), nwalk * nsteps});
    stdSPCMatrix_ref Xrecv(Xsend.origin() + Xsend.num_elements(), {long(globalnCV * xx), nwalk * nsteps});
    int Xg0, XgN;
    std::tie(Xg0, XgN) = FairDivideBoundary(TG.getLocalTGRank(), globalnCV * nwalk * nsteps, TG.getNCoresPerTG());
    int Xl0, XlN;
    std::tie(Xl0, XlN) = FairDivideBoundary(TG.getLocalTGRank(), localnCV * nwalk * nsteps, TG.getNCoresPerTG());
    int cv0, cvN;
    std::tie(cv0, cvN) = FairDivideBoundary(TG.getLocalTGRank(), localnCV, TG.getNCoresPerTG());

    if (bp_step >= 0 && bp_step < bp_max)
    {
      MPI_Send_init(Xsend.origin() + Xg0, (XgN - Xg0) * sizeof(SPComplexType), MPI_CHAR, TG.prev_core(), 3456, &TG.TG(),
                    &req_X2send);
      MPI_Recv_init(Xrecv.origin() + Xg0, (XgN - Xg0) * sizeof(SPComplexType), MPI_CHAR, TG.next_core(), 3456, &TG.TG(),
                    &req_X2recv);
    }

    TG.local_barrier();
    AFQMCTimers[setup_timer]->stop();

    MPI_Status st;

    for (int k = 0; k < nnodes; ++k)
    {
      // 2. wait for communication of previous step
      AFQMCTimers[vHS_comm_overhead_timer]->start();
      if (k > 0)
      {
        MPI_Wait(&req_Grecv, &st);
        MPI_Wait(&req_Gsend, &st); // need to wait for Gsend in order to overwrite Gwork
        copy_n(make_device_ptr(Grecv.origin()) + Gak0, GakN - Gak0, make_device_ptr(Gwork.origin()) + Gak0);
        TG.local_barrier();
      }

      // 3. setup next communication
      if (k < nnodes - 1)
      {
        MPI_Start(&req_Gsend);
        MPI_Start(&req_Grecv);
      }
      AFQMCTimers[vHS_comm_overhead_timer]->stop();

      // calculate vHS contribution from this node
      // 4a. Calculate vbias for initial configuration
      AFQMCTimers[vbias_timer]->start();
      wfn.vbias(Gwork, vbias, sqrtdt);
      AFQMCTimers[vbias_timer]->stop();

      // 4b. Assemble X(nCV,nsteps,nwalk)
      AFQMCTimers[assemble_X_timer]->start();
      int q = (node_number + k) % nnodes;
      assemble_X(nsteps, nwalk, sqrtdt, X, vbias, MFfactor, hybrid_weight);
      copy_n(make_device_ptr(MFfactor.origin()), MFfactor.num_elements(), make_device_ptr(globalMFfactor[q].origin()));
      copy_n(make_device_ptr(hybrid_weight.origin()), hybrid_weight.num_elements(),
             make_device_ptr(globalhybrid_weight[q].origin()));
      TG.local_barrier();
      AFQMCTimers[assemble_X_timer]->stop();
      if (bp_step >= 0 && bp_step < bp_max)
      {
        // receive X
        if (k > 0)
        {
          MPI_Wait(&req_X2recv, &st);
          MPI_Wait(&req_X2send, &st);
          copy_n(Xrecv.origin() + Xg0, XgN - Xg0, Xsend.origin() + Xg0);
          TG.local_barrier();
        }
        // accumulate
        copy_n(make_device_ptr(X[cv0].origin()), nwalk * nsteps * (cvN - cv0), Xsend[global_origin + cv0].origin());
        TG.local_barrier();
        // start X communication
        MPI_Start(&req_X2send);
        MPI_Start(&req_X2recv);
      }

      // 4c. Calculate vHS(M*M,nsteps,nwalk)
      AFQMCTimers[vHS_timer]->start();
      wfn.vHS(X, vHS, sqrtdt);
      TG.local_barrier();
      AFQMCTimers[vHS_timer]->stop();

      AFQMCTimers[vHS_comm_overhead_timer]->start();
      // 5. receive v
      if (k > 0)
      {
        MPI_Wait(&req_vrecv, &st);
        MPI_Wait(&req_vsend, &st);
        copy_n(make_device_ptr(vrecv.origin()) + vak0, vakN - vak0, make_device_ptr(vsend.origin()) + vak0);
      }

      // 6. add local contribution to vsend
      axpy(vakN - vak0, one, make_device_ptr(vHS.origin()) + vak0, 1, make_device_ptr(vsend.origin()) + vak0, 1);

      // 7. start v communication
      MPI_Start(&req_vsend);
      MPI_Start(&req_vrecv);
      TG.local_barrier();
      AFQMCTimers[vHS_comm_overhead_timer]->stop();
    }

    // after the wait, vrecv ( and by extention vHS3D ) has the final vHS for the local walkers
    AFQMCTimers[vHS_comm_overhead_timer]->start();
    MPI_Wait(&req_vrecv, &st);
    MPI_Wait(&req_vsend, &st);
    MPI_Wait(&req_X2recv, &st);
    MPI_Wait(&req_X2send, &st);

    MPI_Request_free(&req_Grecv);
    MPI_Request_free(&req_Gsend);
    MPI_Request_free(&req_vrecv);
    MPI_Request_free(&req_vsend);

    // store fields in walker
    if (bp_step >= 0 && bp_step < bp_max)
    {
      MPI_Request_free(&req_X2recv);
      MPI_Request_free(&req_X2send);

      int cvg0, cvgN;
      std::tie(cvg0, cvgN) = FairDivideBoundary(TG.getLocalTGRank(), globalnCV, TG.getNCoresPerTG());
      for (int ni = 0; ni < nsteps; ni++)
      {
        if (bp_step < bp_max)
        {
          auto&& V(*wset.getFields(bp_step));
          if (nsteps == 1)
          {
            copy_n(Xrecv[cvg0].origin(), nwalk * (cvgN - cvg0), V[cvg0].origin());
            ma::scal(SPComplexType(sqrtdt), V.sliced(cvg0, cvgN));
          }
          else
          {
            ma::add(SPComplexType(0.0), V.sliced(cvg0, cvgN), SPComplexType(sqrtdt),
                    Xrecv({cvg0, cvgN}, {ni * nwalk, (ni + 1) * nwalk}), V.sliced(cvg0, cvgN));
          }
          bp_step++;
        }
      }
      TG.TG_local().barrier();
    }
    // reduce MF and HWs
    if (TG.TG().size() > 1)
    {
      TG.TG().all_reduce_in_place_n(to_address(globalMFfactor.origin()), globalMFfactor.num_elements(), std::plus<>());
      TG.TG().all_reduce_in_place_n(to_address(globalhybrid_weight.origin()), globalhybrid_weight.num_elements(),
                                    std::plus<>());
    }

    // copy from global to local array
    copy_n(make_device_ptr(globalMFfactor[node_number].origin()), MFfactor.num_elements(),
           make_device_ptr(MFfactor.origin()));
    copy_n(make_device_ptr(globalhybrid_weight[node_number].origin()), hybrid_weight.num_elements(),
           make_device_ptr(hybrid_weight.origin()));

    TG.local_barrier();
    AFQMCTimers[vHS_comm_overhead_timer]->stop();
  }

#if defined(MIXED_PRECISION)
  // is this clever or dirsty? seems to work well and saves memory!
  TG.local_barrier();
  using qmcplusplus::afqmc::inplace_cast;
  if (TG.TG_local().root())
    inplace_cast<SPComplexType, ComplexType>(vrecv.origin(), vrecv.num_elements());
  TG.local_barrier();
#endif
  C3Tensor_ref vHS3D(make_device_ptr(vrecv_buff.origin()), vhs3d_ext);

  // From here on is similar to Shared
  int nx = 1;
  if (walker_type == COLLINEAR)
    nx = 2;

  // from now on, individual work on each walker/step
  const int ntasks_per_core     = int(nx * nwalk) / TG.getNCoresPerTG();
  const int ntasks_total_serial = ntasks_per_core * TG.getNCoresPerTG();
  const int nextra              = int(nx * nwalk) - ntasks_total_serial;

  // each processor does ntasks_percore_serial overlaps serially
  const int tk0 = TG.getLocalTGRank() * ntasks_per_core;
  const int tkN = (TG.getLocalTGRank() + 1) * ntasks_per_core;

  // make new communicator if nextra changed from last setting
  reset_nextra(nextra);

  for (int ni = 0; ni < nsteps_; ni++)
  {
    // 5. Propagate walkers
    AFQMCTimers[propagate_timer]->start();
    if (nbatched_propagation != 0)
    {
      apply_propagators_batched('N', wset, ni, vHS3D);
    }
    else
    {
      apply_propagators('N', wset, ni, tk0, tkN, ntasks_total_serial, vHS3D);
    }
    AFQMCTimers[propagate_timer]->stop();

    // 6. Calculate local energy/overlap
    AFQMCTimers[pseudo_energy_timer]->start();
    if (hybrid)
    {
      wfn.Overlap(wset, new_overlaps);
    }
    else
    {
      wfn.Energy(wset, new_energies, new_overlaps);
    }
    TG.local_barrier();
    AFQMCTimers[pseudo_energy_timer]->stop();

    // 7. update weights/energy/etc, apply constrains/bounds/etc
    AFQMCTimers[extra_timer]->start();
    if (TG.TG_local().root())
    {
      if (free_projection)
      {
        free_projection_walker_update(wset, dt, new_overlaps, MFfactor[ni], Eshift, hybrid_weight[ni], work);
      }
      else if (hybrid)
      {
        hybrid_walker_update(wset, dt, apply_constrain, importance_sampling, Eshift, new_overlaps, MFfactor[ni],
                             hybrid_weight[ni], work);
      }
      else
      {
        local_energy_walker_update(wset, dt, apply_constrain, Eshift, new_overlaps, new_energies, MFfactor[ni],
                                   hybrid_weight[ni], work);
      }
      if (wset.getBPPos() >= 0 && wset.getBPPos() < wset.NumBackProp())
        wset.advanceBPPos();
      if (wset.getBPPos() >= 0)
        wset.advanceHistoryPos();
    }
    TG.local_barrier();
    AFQMCTimers[extra_timer]->stop();
  }
}

// Distributed propagation based on collective communication
/*
 * Propagates the walker population nsteps forward with a fixed vbias (from the initial 
 * configuration).   
 */
template<class WlkSet>
void AFQMCDistributedPropagatorDistCV::step_collective(int nsteps_, WlkSet& wset, RealType Eshift, RealType dt)
{
  using ma::axpy;
  using std::copy_n;
  using std::fill_n;
  AFQMCTimers[setup_timer]->start();
  const SPComplexType one(1.), zero(0.);
  auto walker_type        = wset.getWalkerType();
  int nsteps              = nsteps_;
  int nwalk               = wset.size();
  RealType sqrtdt         = std::sqrt(dt);
  long Gsize              = wfn.size_of_G_for_vbias();
  const int globalnCV     = wfn.global_number_of_cholesky_vectors();
  const int localnCV      = wfn.local_number_of_cholesky_vectors();
  const int global_origin = wfn.global_origin_cholesky_vector();
  const int nnodes        = TG.getNGroupsPerTG();
  const int ncores        = TG.getNCoresPerTG();
  const int node_number   = TG.getLocalGroupNumber();
  // if transposed_XXX_=true  --> XXX[nwalk][...],
  // if transposed_XXX_=false --> XXX[...][nwalk]
  auto vhs_ext   = iextensions<2u>{NMO * NMO, nwalk * nsteps};
  auto vhs3d_ext = iextensions<3u>{NMO, NMO, nwalk * nsteps};
  auto G_ext     = iextensions<2u>{Gsize, nwalk};
  if (transposed_vHS_)
  {
    vhs_ext   = iextensions<2u>{nwalk * nsteps, NMO * NMO};
    vhs3d_ext = iextensions<3u>{nwalk * nsteps, NMO, NMO};
  }
  if (transposed_G_)
    G_ext = iextensions<2u>{nwalk, Gsize};

  if (MFfactor.size(0) != nsteps || MFfactor.size(1) != nwalk)
    MFfactor = CMatrix({long(nsteps), long(nwalk)});
  if (hybrid_weight.size(0) != nsteps || hybrid_weight.size(1) != nwalk)
    hybrid_weight = CMatrix({long(nsteps), long(nwalk)});
  if (new_overlaps.size(0) != nwalk)
    new_overlaps = CVector(iextensions<1u>{nwalk});
  if (new_energies.size(0) != nwalk || new_energies.size(1) != 3)
    new_energies = CMatrix({long(nwalk), 3});

  //  Temporary memory usage summary:
  //  G_for_vbias:     [ Gsize * nwalk ] (2 copies)
  //  vbias:           [ localnCV * nwalk ]
  //  X:               [ localnCV * nwalk * nstep ]
  //  vHS:             [ NMO*NMO * nwalk * nstep ] (2 copies)
  // memory_needs: nwalk * ( 2*nsteps + Gsize + localnCV*(nstep+1) + NMO*NMO*nstep )

  // if timestep changed, recalculate one body propagator
  if (std::abs(dt - old_dt) > 1e-6)
    generateP1(dt, walker_type);
  TG.local_barrier();

  StaticMatrix vrecv_buff(vhs_ext, buffer_manager.get_generator().template get_allocator<ComplexType>());
  C3Tensor_ref vHS3D(make_device_ptr(vrecv_buff.origin()), vhs3d_ext);
  SPCMatrix_ref vrecv(sp_pointer(make_device_ptr(vrecv_buff.origin())), vhs_ext);

  // scope controlling lifetime of temporary arrays
  {
    Static3Tensor globalMFfactor({nnodes, nsteps, nwalk},
                                 buffer_manager.get_generator().template get_allocator<ComplexType>());
    Static3Tensor globalhybrid_weight({nnodes, nsteps, nwalk},
                                      buffer_manager.get_generator().template get_allocator<ComplexType>());
    StaticSPMatrix Gstore(G_ext, buffer_manager.get_generator().template get_allocator<SPComplexType>());

    int Gak0, GakN;
    std::tie(Gak0, GakN) = FairDivideBoundary(TG.getLocalTGRank(), int(Gstore.num_elements()), TG.getNCoresPerTG());

    // 1. Calculate Green function for all (local) walkers
    AFQMCTimers[G_for_vbias_timer]->start();
#if defined(MIXED_PRECISION)
    {
      StaticMatrix Gstore_(G_ext, buffer_manager.get_generator().template get_allocator<ComplexType>());
      wfn.MixedDensityMatrix_for_vbias(wset, Gstore_);
      copy_n_cast(make_device_ptr(Gstore_.origin()) + Gak0, GakN - Gak0, make_device_ptr(Gstore.origin()) + Gak0);
    }
#else
    wfn.MixedDensityMatrix_for_vbias(wset, Gstore);
#endif
    TG.local_barrier();
    AFQMCTimers[G_for_vbias_timer]->stop();

    StaticSPMatrix Gwork(G_ext, buffer_manager.get_generator().template get_allocator<SPComplexType>());
    StaticSPMatrix vbias({long(localnCV), long(nwalk)},
                         buffer_manager.get_generator().template get_allocator<SPComplexType>());
    StaticSPMatrix X({long(localnCV), long(nwalk * nsteps)},
                     buffer_manager.get_generator().template get_allocator<SPComplexType>());
// reusing second half of vrecv buffer reinterpreted as a SPComplex Array
#if defined(MIXED_PRECISION)
    SPCMatrix_ref vHS(sp_pointer(make_device_ptr(vrecv_buff.origin())) + vrecv_buff.num_elements(), vhs_ext);
#else
    StaticMatrix vHS(vhs_ext, buffer_manager.get_generator().template get_allocator<ComplexType>());
#endif

    // partition G and v for communications: all cores communicate a piece of the matrix
    int vak0, vakN;
    std::tie(vak0, vakN) = FairDivideBoundary(TG.getLocalTGRank(), int(vHS.num_elements()), TG.getNCoresPerTG());

    // are we back propagating?
    int bp_step = wset.getBPPos(), bp_max = wset.NumBackProp();
    bool realloc(false);
    int xx(0);
    if (bp_step >= 0 && bp_step < bp_max)
    {
      xx = 1;
      size_t m_(globalnCV * nwalk * nsteps);
      if (bpX.num_elements() < m_)
      {
        realloc = true;
        bpX     = mpi3SPCVector(iextensions<1u>{m_}, shared_allocator<SPComplexType>{TG.TG_local()});
        if (TG.TG_local().root())
          fill_n(bpX.origin(), bpX.num_elements(), SPComplexType(0.0));
      }
    }
    stdSPCMatrix_ref Xrecv(to_address(bpX.origin()), {long(globalnCV * xx), nwalk * nsteps});
    int Xg0, XgN;
    std::tie(Xg0, XgN) = FairDivideBoundary(TG.getLocalTGRank(), globalnCV * nwalk * nsteps, TG.getNCoresPerTG());
    int Xl0, XlN;
    std::tie(Xl0, XlN) = FairDivideBoundary(TG.getLocalTGRank(), localnCV * nwalk * nsteps, TG.getNCoresPerTG());
    int cv0, cvN;
    std::tie(cv0, cvN) = FairDivideBoundary(TG.getLocalTGRank(), localnCV, TG.getNCoresPerTG());

    // setup counts, it is possible to keep old one if nwalk*nsteps*(cvN-cv0) doesn't change.
    // what can I assume?
    if (bp_step >= 0 && bp_step < bp_max)
    {
      bpx_counts = TG.TG().all_gather_value(int(nwalk * nsteps * (cvN - cv0)));
      if (TG.TG_local().root())
      {
        bpx_displ.reserve(bpx_counts.size());
        bpx_displ.clear();
        for (int i = 0, s = 0; i < bpx_counts.size(); i++)
        {
          bpx_displ.push_back(s);
          s += bpx_counts[i];
        }
      }
    }

    TG.local_barrier();
    AFQMCTimers[setup_timer]->stop();

    MPI_Status st;

    for (int k = 0; k < nnodes; ++k)
    {
      // 2. bcast G
      AFQMCTimers[vHS_comm_overhead_timer]->start();
      if (k == node_number)
        copy_n(make_device_ptr(Gstore.origin()) + Gak0, GakN - Gak0, make_device_ptr(Gwork.origin()) + Gak0);
#ifdef BUILD_AFQMC_WITH_NCCL
#ifdef ENABLE_CUDA
#if defined(MIXED_PRECISION)
      NCCLCHECK(
          ncclBcast(to_address(Gwork.origin() + Gak0), 2 * (GakN - Gak0), ncclFloat, k, TG.ncclTG(), TG.ncclStream()));
#else
      NCCLCHECK(
          ncclBcast(to_address(Gwork.origin() + Gak0), 2 * (GakN - Gak0), ncclDouble, k, TG.ncclTG(), TG.ncclStream()));
#endif
      qmc_cuda::cuda_check(cudaStreamSynchronize(TG.ncclStream()), "cudaStreamSynchronize(s)");
#else
#error "BUILD_AFQMC_WITH_NCCL only with ENABLE_CUDA"
#endif
#else
      TG.TG_Cores().broadcast_n(to_address(Gwork.origin()) + Gak0, GakN - Gak0, k);
#endif
      TG.local_barrier();
      AFQMCTimers[vHS_comm_overhead_timer]->stop();

      // calculate vHS contribution from this node
      // 3a. Calculate vbias for initial configuration
      AFQMCTimers[vbias_timer]->start();
      wfn.vbias(Gwork, vbias, sqrtdt);
      AFQMCTimers[vbias_timer]->stop();

      // 3b. Assemble X(nCV,nsteps,nwalk)
      AFQMCTimers[assemble_X_timer]->start();
      assemble_X(nsteps, nwalk, sqrtdt, X, vbias, MFfactor, hybrid_weight);
      copy_n(make_device_ptr(MFfactor.origin()), MFfactor.num_elements(), make_device_ptr(globalMFfactor[k].origin()));
      copy_n(make_device_ptr(hybrid_weight.origin()), hybrid_weight.num_elements(),
             make_device_ptr(globalhybrid_weight[k].origin()));
      TG.local_barrier();
      AFQMCTimers[assemble_X_timer]->stop();
      if (bp_step >= 0 && bp_step < bp_max)
      {
        APP_ABORT("Finish");
        // accumulate
        //copy_n(X[cv0].origin(),nwalk*nsteps*(cvN-cv0),Xsend[global_origin+cv0].origin());
        // how do I know if these change???
        // possible if 2 execute blocks use different ncores
        // maybe store last ncores and change if this number changes!!!
        TG.TG().gatherv_n(to_address(X[cv0].origin()), nwalk * nsteps * (cvN - cv0), to_address(Xrecv.origin()),
                          bpx_counts.begin(), bpx_displ.begin(), k * ncores);
        TG.local_barrier();
      }

      // 3c. Calculate vHS(M*M,nsteps,nwalk)
      AFQMCTimers[vHS_timer]->start();
      wfn.vHS(X, vHS, sqrtdt);
      TG.local_barrier();
      AFQMCTimers[vHS_timer]->stop();

      AFQMCTimers[vHS_comm_overhead_timer]->start();
      // 4. Reduce vHS
#ifdef BUILD_AFQMC_WITH_NCCL
#ifdef ENABLE_CUDA
#if defined(MIXED_PRECISION)
      NCCLCHECK(ncclReduce((const void*)to_address(vHS.origin() + vak0), (void*)to_address(vrecv.origin() + vak0),
                           2 * (vakN - vak0), ncclFloat, ncclSum, k, TG.ncclTG(), TG.ncclStream()));
#else
      NCCLCHECK(ncclReduce((const void*)to_address(vHS.origin() + vak0), (void*)to_address(vrecv.origin() + vak0),
                           2 * (vakN - vak0), ncclDouble, ncclSum, k, TG.ncclTG(), TG.ncclStream()));
#endif
      qmc_cuda::cuda_check(cudaStreamSynchronize(TG.ncclStream()), "cudaStreamSynchronize(s)");
#else
#error "BUILD_AFQMC_WITH_NCCL only with ENABLE_CUDA"
#endif
#else
      TG.TG_Cores().reduce_n(to_address(vHS.origin()) + vak0, vakN - vak0, to_address(vrecv.origin()) + vak0,
                             std::plus<>(), k);
#endif

      TG.local_barrier();
      AFQMCTimers[vHS_comm_overhead_timer]->stop();
    }

    // after the wait, vrecv ( and by extention vHS3D ) has the final vHS for the local walkers
    AFQMCTimers[vHS_comm_overhead_timer]->start();

    // store fields in walker
    if (bp_step >= 0 && bp_step < bp_max)
    {
      int cvg0, cvgN;
      std::tie(cvg0, cvgN) = FairDivideBoundary(TG.getLocalTGRank(), globalnCV, TG.getNCoresPerTG());
      for (int ni = 0; ni < nsteps; ni++)
      {
        if (bp_step < bp_max)
        {
          auto&& V(*wset.getFields(bp_step));
          if (nsteps == 1)
          {
            copy_n(Xrecv[cvg0].origin(), nwalk * (cvgN - cvg0), V[cvg0].origin());
            ma::scal(sqrtdt, V.sliced(cvg0, cvgN));
          }
          else
          {
            ma::add(SPComplexType(0.0), V.sliced(cvg0, cvgN), SPComplexType(sqrtdt),
                    Xrecv({cvg0, cvgN}, {ni * nwalk, (ni + 1) * nwalk}), V.sliced(cvg0, cvgN));
          }
          bp_step++;
        }
      }
      TG.TG_local().barrier();
    }
    // reduce MF and HWs, always in DP
    if (TG.TG().size() > 1)
    {
#ifdef BUILD_AFQMC_WITH_NCCL
#ifdef ENABLE_CUDA
      NCCLCHECK(ncclAllReduce((const void*)to_address(globalMFfactor.origin()),
                              (void*)to_address(globalMFfactor.origin()), 2 * globalMFfactor.num_elements(), ncclDouble,
                              ncclSum, TG.ncclTG(), TG.ncclStream()));
      NCCLCHECK(ncclAllReduce((const void*)to_address(globalhybrid_weight.origin()),
                              (void*)to_address(globalhybrid_weight.origin()), 2 * globalhybrid_weight.num_elements(),
                              ncclDouble, ncclSum, TG.ncclTG(), TG.ncclStream()));
      qmc_cuda::cuda_check(cudaStreamSynchronize(TG.ncclStream()), "cudaStreamSynchronize(s)");
#else
#error "BUILD_AFQMC_WITH_NCCL only with ENABLE_CUDA"
#endif
#else
      TG.TG().all_reduce_in_place_n(to_address(globalMFfactor.origin()), globalMFfactor.num_elements(), std::plus<>());
      TG.TG().all_reduce_in_place_n(to_address(globalhybrid_weight.origin()), globalhybrid_weight.num_elements(),
                                    std::plus<>());
#endif
    }
    TG.local_barrier();

    // copy from global to local array
    copy_n(make_device_ptr(globalMFfactor[node_number].origin()), MFfactor.num_elements(),
           make_device_ptr(MFfactor.origin()));
    copy_n(make_device_ptr(globalhybrid_weight[node_number].origin()), hybrid_weight.num_elements(),
           make_device_ptr(hybrid_weight.origin()));

    AFQMCTimers[vHS_comm_overhead_timer]->stop();
  } // scope controlling lifetime of temporary arrays

#if defined(MIXED_PRECISION)
  TG.local_barrier();
  using qmcplusplus::afqmc::inplace_cast;
  if (TG.TG_local().root())
    inplace_cast<SPComplexType, ComplexType>(make_device_ptr(vrecv.origin()), vrecv.num_elements());
  TG.local_barrier();
#endif

  // From here on is similar to Shared
  int nx = 1;
  if (walker_type == COLLINEAR)
    nx = 2;

  // from now on, individual work on each walker/step
  const int ntasks_per_core     = int(nx * nwalk) / TG.getNCoresPerTG();
  const int ntasks_total_serial = ntasks_per_core * TG.getNCoresPerTG();
  const int nextra              = int(nx * nwalk) - ntasks_total_serial;

  // each processor does ntasks_percore_serial overlaps serially
  const int tk0 = TG.getLocalTGRank() * ntasks_per_core;
  const int tkN = (TG.getLocalTGRank() + 1) * ntasks_per_core;

  // make new communicator if nextra changed from last setting
  reset_nextra(nextra);

  for (int ni = 0; ni < nsteps_; ni++)
  {
    // 5. Propagate walkers
    AFQMCTimers[propagate_timer]->start();
    if (nbatched_propagation != 0)
    {
      apply_propagators_batched('N', wset, ni, vHS3D);
    }
    else
    {
      apply_propagators('N', wset, ni, tk0, tkN, ntasks_total_serial, vHS3D);
    }
    AFQMCTimers[propagate_timer]->stop();

    // 6. Calculate local energy/overlap
    AFQMCTimers[pseudo_energy_timer]->start();
    if (hybrid)
    {
      wfn.Overlap(wset, new_overlaps);
    }
    else
    {
      wfn.Energy(wset, new_energies, new_overlaps);
    }
    TG.local_barrier();
    AFQMCTimers[pseudo_energy_timer]->stop();

    // 7. update weights/energy/etc, apply constrains/bounds/etc
    AFQMCTimers[extra_timer]->start();
    if (TG.TG_local().root())
    {
      if (free_projection)
      {
        free_projection_walker_update(wset, dt, new_overlaps, MFfactor[ni], Eshift, hybrid_weight[ni], work);
      }
      else if (hybrid)
      {
        hybrid_walker_update(wset, dt, apply_constrain, importance_sampling, Eshift, new_overlaps, MFfactor[ni],
                             hybrid_weight[ni], work);
      }
      else
      {
        local_energy_walker_update(wset, dt, apply_constrain, Eshift, new_overlaps, new_energies, MFfactor[ni],
                                   hybrid_weight[ni], work);
      }
      if (wset.getBPPos() >= 0 && wset.getBPPos() < wset.NumBackProp())
        wset.advanceBPPos();
      if (wset.getBPPos() >= 0)
        wset.advanceHistoryPos();
    }
    TG.local_barrier();
    AFQMCTimers[extra_timer]->stop();
  }
}

/*
 * This routine assumes that the 1 body propagator does not need updating
 */
template<class WlkSet, class CTens, class CMat>
void AFQMCDistributedPropagatorDistCV::BackPropagate(int nbpsteps,
                                                     int nStabalize,
                                                     WlkSet& wset,
                                                     CTens&& Refs,
                                                     CMat&& detR)
{
  using std::copy_n;
  using std::fill_n;
  const SPComplexType one(1.), zero(0.);
  auto walker_type        = wset.getWalkerType();
  const int nwalk         = wset.size();
  const int globalnCV     = wfn.global_number_of_cholesky_vectors();
  const int localnCV      = wfn.local_number_of_cholesky_vectors();
  const int global_origin = wfn.global_origin_cholesky_vector();
  const int nnodes        = TG.getNGroupsPerTG();

  auto vhs_ext   = iextensions<2u>{NMO * NMO, nwalk};
  auto vhs3d_ext = iextensions<3u>{NMO, NMO, nwalk};
  if (transposed_vHS_)
  {
    vhs_ext   = iextensions<2u>{nwalk, NMO * NMO};
    vhs3d_ext = iextensions<3u>{nwalk, NMO, NMO};
  }

  //  Shared buffer used for:
  //  X:               [ (localnCV + 2*globalnCV) * nwalk ]
  //  vHS:             [ NMO*NMO * nwalk ] (3 copies)
  // memory_needs: nwalk * ( localnCV + NMO*NMO )

  StaticSPMatrix X({long(localnCV), long(nwalk)},
                   buffer_manager.get_generator().template get_allocator<SPComplexType>());
  StaticSPMatrix Xsend({long(globalnCV), long(nwalk)},
                       buffer_manager.get_generator().template get_allocator<SPComplexType>());
  StaticSPMatrix Xrecv({long(globalnCV), long(nwalk)},
                       buffer_manager.get_generator().template get_allocator<SPComplexType>());
  StaticMatrix vrecv_buff(vhs_ext, buffer_manager.get_generator().template get_allocator<ComplexType>());
  SPCMatrix_ref vrecv(sp_pointer(make_device_ptr(vrecv_buff.origin())), vhs_ext);
#if defined(MIXED_PRECISION)
  SPCMatrix_ref vsend(sp_pointer(make_device_ptr(vrecv_buff.origin())) + vrecv_buff.num_elements(), vhs_ext);
#else
  StaticSPMatrix vsend(vhs_ext, buffer_manager.get_generator().template get_allocator<SPComplexType>());
#endif
  StaticSPMatrix vHS(vhs_ext, buffer_manager.get_generator().template get_allocator<SPComplexType>());

  // partition G and v for communications: all cores communicate a piece of the matrix
  int vak0, vakN;
  int X0, XN;
  std::tie(X0, XN)     = FairDivideBoundary(TG.getLocalTGRank(), int(Xsend.num_elements()), TG.getNCoresPerTG());
  std::tie(vak0, vakN) = FairDivideBoundary(TG.getLocalTGRank(), int(vHS.num_elements()), TG.getNCoresPerTG());
  MPI_Send_init(to_address(Xsend.origin()) + X0, (XN - X0) * sizeof(SPComplexType), MPI_CHAR, TG.prev_core(), 2345,
                &TG.TG(), &req_Xsend);
  MPI_Recv_init(to_address(Xrecv.origin()) + X0, (XN - X0) * sizeof(SPComplexType), MPI_CHAR, TG.next_core(), 2345,
                &TG.TG(), &req_Xrecv);
  MPI_Send_init(to_address(vsend.origin()) + vak0, (vakN - vak0) * sizeof(SPComplexType), MPI_CHAR, TG.prev_core(),
                6789, &TG.TG(), &req_bpvsend);
  MPI_Recv_init(to_address(vrecv.origin()) + vak0, (vakN - vak0) * sizeof(SPComplexType), MPI_CHAR, TG.next_core(),
                6789, &TG.TG(), &req_bpvrecv);
  TG.local_barrier();

  auto&& Fields(*wset.getFields());
  assert(Fields.size(0) >= nbpsteps);
  assert(Fields.size(1) == globalnCV);
  assert(Fields.size(2) == nwalk);

  int nrow(NMO * ((walker_type == NONCOLLINEAR) ? 2 : 1));
  int ncol(NAEA + ((walker_type == CLOSED) ? 0 : NAEB));
  assert(Refs.size(0) == nwalk);
  int nrefs = Refs.size(1);
  assert(Refs.size(2) == nrow * ncol);

  int cv0, cvN;
  std::tie(cv0, cvN) = FairDivideBoundary(TG.getLocalTGRank(), localnCV, TG.getNCoresPerTG());
  int r0, rN;
  std::tie(r0, rN) = FairDivideBoundary(TG.getLocalTGRank(), nrow * ncol, TG.getNCoresPerTG());

  MPI_Status st;

  int nx = 1;
  if (walker_type == COLLINEAR)
    nx = 2;

  assert(detR.size(0) == nwalk);
  assert(detR.size(1) == nrefs * nx);
  std::fill_n(detR.origin(), detR.num_elements(), SPComplexType(1.0, 0.0));

  // from now on, individual work on each walker/step
  const int ntasks_per_core     = int(nx * nwalk) / TG.getNCoresPerTG();
  const int ntasks_total_serial = ntasks_per_core * TG.getNCoresPerTG();
  const int nextra              = int(nx * nwalk) - ntasks_total_serial;

  // each processor does ntasks_percore_serial overlaps serially
  const int tk0 = TG.getLocalTGRank() * ntasks_per_core;
  const int tkN = (TG.getLocalTGRank() + 1) * ntasks_per_core;

  // make new communicator if nextra changed from last setting
  reset_nextra(nextra);

  // to avoid having to modify the existing routines,
  // I'm storing the walkers SlaterMatrix on SlaterMatrixAux
  // and copying the back propagated references into SlaterMatrix
  // 0. copy SlaterMatrix to SlaterMatrixAux
  for (int i = 0; i < nwalk; i++)
  {
    copy_n((*wset[i].SlaterMatrix(Alpha)).origin() + r0, rN - r0, (*wset[i].SlaterMatrixAux(Alpha)).origin() + r0);
    // optimize for the single reference case
    if (nrefs == 1)
      copy_n(Refs[i][0].origin() + r0, rN - r0, (*wset[i].SlaterMatrix(Alpha)).origin() + r0);
  }
  TG.TG_local().barrier();

  for (int ni = nbpsteps - 1; ni >= 0; --ni)
  {
    // 1. Get X(nCV,nwalk) from wset
    fill_n(make_device_ptr(vsend.origin()) + vak0, (vakN - vak0), zero);
    copy_n(Fields[ni].origin() + X0, (XN - X0), make_device_ptr(Xsend.origin()) + X0);
    TG.TG_local().barrier();
    copy_n(make_device_ptr(Xsend[global_origin + cv0].origin()), nwalk * (cvN - cv0), make_device_ptr(X[cv0].origin()));
    TG.TG_local().barrier();
    // 2. Calculate vHS(M*M,nwalk)/vHS(nwalk,M*M) using distributed algorithm
    for (int k = 0; k < nnodes; ++k)
    {
      // 2.1 wait for communication of previous step
      if (k > 0)
      {
        MPI_Wait(&req_Xrecv, &st);
        MPI_Wait(&req_Xsend, &st); // need to wait for Gsend in order to overwrite Gwork
        copy_n(make_device_ptr(Xrecv.origin()) + X0, XN - X0, make_device_ptr(Xsend.origin()) + X0);
        TG.local_barrier();
        copy_n(make_device_ptr(Xsend[global_origin + cv0].origin()), nwalk * (cvN - cv0),
               make_device_ptr(X[cv0].origin()));
        TG.local_barrier();
      }

      // 2.2 setup next communication
      if (k < nnodes - 1)
      {
        MPI_Start(&req_Xsend);
        MPI_Start(&req_Xrecv);
      }

      // 2.3 Calculate vHS
      //std::cout<<" k, X: " <<TG.Global().rank() <<" " <<k <<" " <<ma::dot(X(X.extension(0),0),X(X.extension(0),0)) <<"\n\n" <<std::endl;
      wfn.vHS(X, vHS);
      //std::cout<<" k, vHS: " <<TG.Global().rank() <<" " <<k <<" " <<ma::dot(vHS[0],vHS[0]) <<"\n\n" <<std::endl;

      // 2.4 receive v
      if (k > 0)
      {
        MPI_Wait(&req_bpvrecv, &st);
        MPI_Wait(&req_bpvsend, &st);
        copy_n(make_device_ptr(vrecv.origin()) + vak0, vakN - vak0, make_device_ptr(vsend.origin()) + vak0);
      }
      TG.local_barrier();

      // 2.5 add local contribution to vsend
      using ma::axpy;
      axpy(vakN - vak0, one, make_device_ptr(vHS.origin()) + vak0, 1, make_device_ptr(vsend.origin()) + vak0, 1);
      //std::cout<<" k vsend: " <<TG.Global().rank() <<" " <<k <<" " <<ma::dot(vsend[0],vsend[0]) <<"\n\n" <<std::endl;

      // 2.6 start v communication
      MPI_Start(&req_bpvsend);
      MPI_Start(&req_bpvrecv);
      TG.local_barrier();
    }

    MPI_Wait(&req_bpvrecv, &st);
    MPI_Wait(&req_bpvsend, &st);
    TG.local_barrier();
    //std::cout<<" vrecv: " <<TG.Global().rank() <<" " <<ma::dot(vrecv[0],vrecv[0]) <<"\n\n" <<std::endl;

#if defined(MIXED_PRECISION)
    TG.local_barrier();
    if (TG.TG_local().root())
      inplace_cast<SPComplexType, ComplexType>(vrecv.origin(), vrecv.num_elements());
    TG.local_barrier();
#endif
    C3Tensor_ref vHS3D(make_device_ptr(vrecv_buff.origin()), vhs3d_ext);

    for (int nr = 0; nr < nrefs; ++nr)
    {
      // 3. copy reference to SlaterMatrix
      if (nrefs > 1)
        for (int i = 0; i < nwalk; i++)
          copy_n(Refs[i][nr].origin() + r0, rN - r0, (*wset[i].SlaterMatrix(Alpha)).origin() + r0);
      TG.TG_local().barrier();

      // 4. Propagate walkers
      if (nbatched_propagation != 0)
        apply_propagators_batched('H', wset, 0, vHS3D);
      else
        apply_propagators('H', wset, 0, tk0, tkN, ntasks_total_serial, vHS3D);
      TG.local_barrier();

      // always end (ni==0) with orthogonalization
      if (ni == 0 || ni % nStabalize == 0)
      {
        // orthogonalize
        if (nbatched_qr != 0)
        {
          if (walker_type != COLLINEAR)
            Orthogonalize_batched(wset, detR(detR.extension(0), {nr, nr + 1}));
          else
            Orthogonalize_batched(wset, detR(detR.extension(0), {2 * nr, 2 * nr + 2}));
        }
        else
        {
          if (walker_type != COLLINEAR)
            Orthogonalize_shared(wset, detR(detR.extension(0), {nr, nr + 1}));
          else
            Orthogonalize_shared(wset, detR(detR.extension(0), {2 * nr, 2 * nr + 2}));
        }
      }

      // 5. copy reference to back
      if (nrefs > 1)
        for (int i = 0; i < nwalk; i++)
          copy_n((*wset[i].SlaterMatrix(Alpha)).origin() + r0, rN - r0, Refs[i][nr].origin() + r0);
      TG.TG_local().barrier();
    }
  }

  // 6. restore the Slater Matrix
  for (int i = 0; i < nwalk; i++)
  {
    if (nrefs == 1)
      copy_n((*wset[i].SlaterMatrix(Alpha)).origin() + r0, rN - r0, Refs[i][0].origin() + r0);
    copy_n((*wset[i].SlaterMatrixAux(Alpha)).origin() + r0, rN - r0, (*wset[i].SlaterMatrix(Alpha)).origin() + r0);
  }
  MPI_Request_free(&req_Xrecv);
  MPI_Request_free(&req_Xsend);
  MPI_Request_free(&req_bpvrecv);
  MPI_Request_free(&req_bpvsend);
  TG.TG_local().barrier();
}


} // namespace afqmc

} // namespace qmcplusplus
