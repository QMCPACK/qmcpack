//////////////////////////////////////////////////////////////////////
// This file is distributed under the University of Illinois/NCSA Open Source
// License.  See LICENSE file in top directory for details.
//
// Copyright (c) 2016 Jeongnim Kim and QMCPACK developers.
//
// File developed by:
// Miguel A. Morales, moralessilva2@llnl.gov 
//    Lawrence Livermore National Laboratory 
//
// File created by:
// Miguel A. Morales, moralessilva2@llnl.gov 
//    Lawrence Livermore National Laboratory 
////////////////////////////////////////////////////////////////////////////////

#include <vector>
#include <map>
#include <string>
#include <iostream>
#include <tuple>

#include "AFQMC/config.h"
#include "AFQMC/Propagators/generate1BodyPropagator.hpp"
#include "AFQMC/Propagators/WalkerUpdate.hpp"
#include "AFQMC/Walkers/WalkerConfig.hpp"

namespace qmcplusplus
{

namespace afqmc
{

/*
 * Propagates the walker population nsteps forward with a fixed vbias (from the initial 
 * configuration).   
 */
template<class WlkSet>
void AFQMCDistributedPropagatorDistCV::step(int nsteps_, WlkSet& wset, RealType Eshift, RealType dt) 
{
  const ComplexType one(1.),zero(0.);
  auto walker_type = wset.getWalkerType();
  size_t nsteps= size_t(nsteps_);
  size_t nwalk = wset.size();
  RealType sqrtdt = std::sqrt(dt);  
  size_t Gsize = wfn.size_of_G_for_vbias();
  size_t localnCV = wfn.local_number_of_cholesky_vectors();
  const int nnodes = TG.getNNodesPerTG();  
  const int node_number = TG.getLocalNodeNumber();
  // if transposed_XXX_=true  --> XXX[nwalk][...], 
  // if transposed_XXX_=false --> XXX[...][nwalk]
  int vhs_nr = NMO*NMO, vhs_nc = nwalk*nsteps;
  if(transposed_vHS_) std::swap(vhs_nr,vhs_nc);
  int vhs3d_n1 = NMO, vhs3d_n2 = NMO, vhs3d_n3 = nwalk*nsteps;
  if(transposed_vHS_) {
    vhs3d_n1 = nwalk*nsteps;
    vhs3d_n2 = vhs3d_n3 = NMO;
  }
  int G_nr = Gsize, G_nc = nwalk;
  if(transposed_G_) std::swap(G_nr,G_nc);

  //  Shared buffer used for:
  //  G_for_vbias:     [ Gsize * nwalk ] (2 copies)
  //  vbias:           [ localnCV * nwalk ] 
  //  X:               [ localnCV * nwalk * nstep ]
  //  vHS:             [ NMO*NMO * nwalk * nstep ] (3 copies)     
  // memory_needs: nwalk * ( 2*nsteps + Gsize + localnCV*(nstep+1) + NMO*NMO*nstep )
  size_t memory_needs = nwalk * ( 2*Gsize + localnCV*(nsteps+1) + 3*NMO*NMO*nsteps );
  bool new_shm_space=false;  

  // 0. Allocate memory and set shared memory structures
  if(not shmbuff || shmbuff->size() < memory_needs ) {
    shmbuff.reset(); // destroy current SHM_Buffer in case it is large
    shmbuff = std::move(std::make_unique<SHM_Buffer>(TG.TG_local(),memory_needs));  
    new_shm_space=true;
  }  
  size_t displ=0;
  // Mixed Density Matrix for walkers at original configuration
  CMatrix_ref Gwork(shmbuff->data()+displ, extents[G_nr][G_nc]); 
    displ+=G_nr*G_nc; 
  // Mixed Density Matrix used for communication 
  CMatrix_ref Grecv(shmbuff->data()+displ, extents[G_nr][G_nc]); 
    displ+=G_nr*G_nc; 
  // vias potential for walkers at original configuration
  CMatrix_ref vbias(shmbuff->data()+displ, extents[localnCV][nwalk]); 
    displ+=localnCV*nwalk;
  // right hand side matrix in calculation of HS potential for all steps: ~ sigma + (vbias-vMF)
  // The same vbias is used in all steps
  CMatrix_ref X(shmbuff->data()+displ, extents[localnCV][nwalk*nsteps]); 
    displ+=localnCV*nwalk*nsteps;
  // HS potential for all steps.
  CMatrix_ref vHS(shmbuff->data()+displ, extents[vhs_nr][vhs_nc]); 
    displ+=vhs_nr*vhs_nc;
  // HS potential for communication 
  CMatrix_ref vsend(shmbuff->data()+displ, extents[vhs_nr][vhs_nc]); 
    displ+=vhs_nr*vhs_nc;
  // HS potential for communication
  CMatrix_ref vrecv(shmbuff->data()+displ, extents[vhs_nr][vhs_nc]); 
  // second view of vHS matrix for use in propagation step
  // notice that the final vHS matrix will be in vrecv, received in the last step
  boost::multi_array_ref<ComplexType,3> vHS3D(shmbuff->data()+displ, 
                                              extents[vhs3d_n1][vhs3d_n2][vhs3d_n3]); 

  // partition G and v for communications: all cores communicate a piece of the matrix
  int vak0,vakN;
  int Gak0,GakN;
  std::tie(Gak0,GakN) = FairDivideBoundary(TG.getLocalTGRank(),int(Gwork.num_elements()),TG.getNCoresPerTG());
  std::tie(vak0,vakN) = FairDivideBoundary(TG.getLocalTGRank(),int(vHS.num_elements()),TG.getNCoresPerTG());
  if(new_shm_space) {
    // use mpi3 when ready
    if(req_Grecv!=MPI_REQUEST_NULL)
        MPI_Request_free(&req_Grecv);
    if(req_Gsend!=MPI_REQUEST_NULL)
        MPI_Request_free(&req_Gsend);
    if(req_vrecv!=MPI_REQUEST_NULL)
        MPI_Request_free(&req_vrecv);
    if(req_vsend!=MPI_REQUEST_NULL)
        MPI_Request_free(&req_vsend);
    MPI_Send_init(Gwork.origin()+Gak0,(GakN-Gak0)*sizeof(ComplexType),MPI_CHAR,
                  TG.prev_core(),3456,&TG.TG(),&req_Gsend);
    MPI_Recv_init(Grecv.origin()+Gak0,(GakN-Gak0)*sizeof(ComplexType),MPI_CHAR,
                  TG.next_core(),3456,&TG.TG(),&req_Grecv);
    MPI_Send_init(vsend.origin()+vak0,(vakN-vak0)*sizeof(ComplexType),MPI_CHAR,
                  TG.prev_core(),5678,&TG.TG(),&req_vsend);
    MPI_Recv_init(vrecv.origin()+vak0,(vakN-vak0)*sizeof(ComplexType),MPI_CHAR,
                  TG.next_core(),5678,&TG.TG(),&req_vrecv);
  }

  // local matrices for temporary accumulation
  if(MFfactor.shape()[0] != nnodes*nsteps || MFfactor.shape()[1] != nwalk)
    MFfactor.resize(extents[nnodes*nsteps][nwalk]);
  if(hybrid_weight.shape()[0] != nnodes*nsteps || hybrid_weight.shape()[1] != nwalk)
    hybrid_weight.resize(extents[nnodes*nsteps][nwalk]);
  if(new_overlaps.shape()[0] != nwalk) new_overlaps.resize(extents[nwalk]);
  if(new_energies.shape()[0] != nwalk || new_energies.shape()[1] != 3) 
    new_energies.resize(extents[nwalk][3]);
  if(not transposed_vHS_) {  
    if(walker_type==NONCOLLINEAR)
      if(local_vHS.shape()[0] != 2*NMO || local_vHS.shape()[1] != 2*NMO)
        local_vHS.resize(extents[2*NMO][2*NMO]);
    else
      if(local_vHS.shape()[0] != NMO || local_vHS.shape()[1] != NMO)
        local_vHS.resize(extents[NMO][NMO]);
  }

  // if timestep changed, recalculate one body propagator
  if( std::abs(dt-old_dt) > 1e-6 ) {
    old_dt = dt;
    P1 = std::move(generate1BodyPropagator(TG,1e-8,dt,H1)); 
  }

  std::fill_n(vsend.origin()+vak0,(vakN-vak0),zero);
  TG.local_barrier();

  MPI_Status st;

  // 1. Calculate Green function for all (local) walkers
  AFQMCTimers[G_for_vbias_timer]->start();
  wfn.MixedDensityMatrix_for_vbias(wset,Gwork);
  AFQMCTimers[G_for_vbias_timer]->stop();

  for(int k=0; k<nnodes; ++k) { 
  
    // 2. wait for communication of previous step 
    AFQMCTimers[vHS_comm_overhead_timer]->start();
    if(k>0) {
      MPI_Wait(&req_Grecv,&st);
      MPI_Wait(&req_Gsend,&st);     // need to wait for Gsend in order to overwrite Gwork  
      std::copy_n(Grecv.origin()+Gak0,GakN-Gak0,Gwork.origin()+Gak0);
      TG.local_barrier();
    }    

    // 3. setup next communication
    if(k < nnodes-1) {
      MPI_Start(&req_Gsend);
      MPI_Start(&req_Grecv);
    }
    AFQMCTimers[vHS_comm_overhead_timer]->stop();

    // calculate vHS contribution from this node
    // 4a. Calculate vbias for initial configuration
    AFQMCTimers[vbias_timer]->start();
    wfn.vbias(Gwork,vbias,sqrtdt);
    AFQMCTimers[vbias_timer]->stop();

    // 4b. Assemble X(nCV,nsteps,nwalk)
    int q = (node_number+k)%nnodes;
    CMatrix_ref mf_(MFfactor[q*nsteps].origin(), extents[nsteps][nwalk]);
    CMatrix_ref hw_(hybrid_weight[q*nsteps].origin(), extents[nsteps][nwalk]);
    assemble_X(nsteps,nwalk,sqrtdt,X,vbias,mf_,hw_);

    // 4c. Calculate vHS(M*M,nsteps,nwalk)
    AFQMCTimers[vHS_timer]->start();
    wfn.vHS(X,vHS,sqrtdt);
    TG.local_barrier();
    AFQMCTimers[vHS_timer]->stop();

    AFQMCTimers[vHS_comm_overhead_timer]->start();
    // 5. receive v 
    if(k > 0) {
      MPI_Wait(&req_vrecv,&st);
      MPI_Wait(&req_vsend,&st);
      std::copy_n(vrecv.origin()+vak0,vakN-vak0,vsend.origin()+vak0);
    }

    // 6. add local contribution to vsend
    BLAS::axpy(vakN-vak0,one,vHS.origin()+vak0,1,vsend.origin()+vak0,1);

    // 7. start v communication
    MPI_Start(&req_vsend);
    MPI_Start(&req_vrecv);
    TG.local_barrier();
    AFQMCTimers[vHS_comm_overhead_timer]->stop();
  }

  // after the wait, vrecv ( and by extention vHS3D ) has the final vHS for the local walkers
  AFQMCTimers[vHS_comm_overhead_timer]->start();
  MPI_Wait(&req_vrecv,&st);
  MPI_Wait(&req_vsend,&st);

  // reduce MF and HWs
  TG.TG().all_reduce_in_place_n(MFfactor.origin(),MFfactor.num_elements(),std::plus<>());
  TG.TG().all_reduce_in_place_n(hybrid_weight.origin(),hybrid_weight.num_elements(),std::plus<>());
  TG.local_barrier();
  AFQMCTimers[vHS_comm_overhead_timer]->stop();

  // From here on is similar to Shared 
  int nx = 1;
  if(walker_type == COLLINEAR) nx=2;

  // from now on, individual work on each walker/step
  const int ntasks_per_core = int(nx*nwalk)/TG.getNCoresPerTG();
  const int ntasks_total_serial = ntasks_per_core*TG.getNCoresPerTG();
  const int nextra = int(nx*nwalk) - ntasks_total_serial;

  // each processor does ntasks_percore_serial overlaps serially
  const int tk0 = TG.getLocalTGRank()*ntasks_per_core;
  const int tkN = (TG.getLocalTGRank()+1)*ntasks_per_core;

  // make new communicator if nextra changed from last setting
  reset_nextra(nextra);

  int n0 = node_number*nsteps_; 
  for(int ni=0; ni<nsteps_; ni++) {

    // 5. Propagate walkers
    AFQMCTimers[propagate_timer]->start();
    apply_propagators(wset,ni,tk0,tkN,ntasks_total_serial,vHS3D);
    AFQMCTimers[propagate_timer]->stop();

    // 6. Calculate local energy/overlap
    AFQMCTimers[pseudo_energy_timer]->start();
    if(hybrid) {
      wfn.Overlap(wset,new_overlaps);
    } else {
      wfn.Energy(wset,new_energies,new_overlaps);
    }
    TG.local_barrier();
    AFQMCTimers[pseudo_energy_timer]->stop();

    // 7. update weights/energy/etc, apply constrains/bounds/etc 
    // simple round-robin
    for(int iw=0; iw<nwalk; ++iw) { 
      if(iw%TG.TG_local().size() == TG.TG_local().rank()) { 
        if(free_projection) { 
          free_projection_walker_update(wset[iw],dt,new_overlaps[iw],
                           MFfactor[n0+ni][iw],hybrid_weight[n0+ni][iw]);
        } else if(hybrid) {
          hybrid_walker_update(wset[iw],dt,apply_constrain,importance_sampling,
                           Eshift,new_overlaps[iw],MFfactor[n0+ni][iw],hybrid_weight[n0+ni][iw]);
        } else {
          local_energy_walker_update(wset[iw],dt,apply_constrain,Eshift,
                                     new_overlaps[iw],new_energies[iw],
                                     MFfactor[n0+ni][iw],hybrid_weight[n0+ni][iw]);
        }
      }
    }
  }  
  TG.local_barrier();
}

}

}

