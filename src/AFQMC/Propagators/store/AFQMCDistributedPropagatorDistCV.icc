//////////////////////////////////////////////////////////////////////
// This file is distributed under the University of Illinois/NCSA Open Source
// License.  See LICENSE file in top directory for details.
//
// Copyright (c) 2016 Jeongnim Kim and QMCPACK developers.
//
// File developed by:
// Miguel A. Morales, moralessilva2@llnl.gov
//    Lawrence Livermore National Laboratory
//
// File created by:
// Miguel A. Morales, moralessilva2@llnl.gov
//    Lawrence Livermore National Laboratory
////////////////////////////////////////////////////////////////////////////////

#include <vector>
#include <map>
#include <string>
#include <iostream>
#include <tuple>

#include "AFQMC/config.h"
#include "AFQMC/Utilities/Utils.hpp"
#include "AFQMC/Propagators/generate1BodyPropagator.hpp"
#include "AFQMC/Propagators/WalkerSetUpdate.hpp"
#include "AFQMC/Walkers/WalkerConfig.hpp"
#include "AFQMC/Numerics/ma_blas.hpp"

#include "Utilities/Timer.h"

namespace qmcplusplus
{
namespace afqmc
{
/*
 * Propagates the walker population nsteps forward with a fixed vbias (from the initial 
 * configuration).   
 */
template<class WlkSet>
void AFQMCDistributedPropagatorDistCV::step(int nsteps_, WlkSet& wset, RealType Eshift, RealType dt)
{
  AFQMCTimers[setup_timer].get().start();
  const ComplexType one(1.), zero(0.);
  auto walker_type      = wset.getWalkerType();
  int nsteps            = nsteps_;
  int nwalk             = wset.size();
  RealType sqrtdt       = std::sqrt(dt);
  long Gsize            = wfn.size_of_G_for_vbias();
  long localnCV         = wfn.local_number_of_cholesky_vectors();
  const int nnodes      = TG.getNGroupsPerTG();
  const int node_number = TG.getLocalGroupNumber();
  // if transposed_XXX_=true  --> XXX[nwalk][...],
  // if transposed_XXX_=false --> XXX[...][nwalk]
  int vhs_nr = NMO * NMO, vhs_nc = nwalk * nsteps;
  if (transposed_vHS_)
    std::swap(vhs_nr, vhs_nc);
  int vhs3d_n1 = NMO, vhs3d_n2 = NMO, vhs3d_n3 = nwalk * nsteps;
  if (transposed_vHS_)
  {
    vhs3d_n1 = nwalk * nsteps;
    vhs3d_n2 = vhs3d_n3 = NMO;
  }
  int G_nr = Gsize, G_nc = nwalk;
  if (transposed_G_)
    std::swap(G_nr, G_nc);

  //  Shared buffer used for:
  //  G_for_vbias:     [ Gsize * nwalk ] (2 copies)
  //  vbias:           [ localnCV * nwalk ]
  //  X:               [ localnCV * nwalk * nstep ]
  //  vHS:             [ NMO*NMO * nwalk * nstep ] (3 copies)
  // memory_needs: nwalk * ( 2*nsteps + Gsize + localnCV*(nstep+1) + NMO*NMO*nstep )
  size_t memory_needs = nwalk * (Gsize + localnCV * (nsteps + 1) + NMO * NMO * nsteps);

  // 0. Allocate memory and set shared memory structures
  using std::copy_n;
  using std::fill_n;
  if (buffer.num_elements() < memory_needs)
  {
    app_log() << " Resizing buffer space in AFQMCDistributedPropagatorDistCV to "
              << memory_needs * sizeof(ComplexType) / 1024.0 / 1024.0 << " MBs. \n";
    {
      buffer = std::move(sharedCVector(iextensions<1u>{memory_needs}, aux_alloc_));
    }
    memory_report();
    fill_n(buffer.origin(), buffer.num_elements(), ComplexType(0.0));
  }

  memory_needs       = nwalk * (2 * Gsize + 2 * NMO * NMO * nsteps);
  bool new_shm_space = false;

  // 0. Allocate memory and set shared memory structures
  if (comm_buffer.num_elements() < memory_needs)
  {
    app_log() << " Resizing communication buffer space in AFQMCDistributedPropagatorDistCV to "
              << memory_needs * sizeof(ComplexType) / 1024.0 / 1024.0 << " MBs. \n";
    {
#ifdef ENABLE_CUDA
      comm_buffer = std::move(stdCVector(iextensions<1u>{memory_needs}));
#else
      comm_buffer = std::move(sharedCVector(iextensions<1u>{memory_needs}, aux_alloc_));
#endif
    }
    memory_report();
    fill_n(comm_buffer.origin(), comm_buffer.num_elements(), ComplexType(0.0));
    new_shm_space = true;
  }

  // convert array to a basic_array<element,1,pointer>. This generates a view of buffer
  // with type basic_array<ComplexType,1,pointer> which eliminates the need to cast origin()
  //auto mem_pool(boost::multi::static_array_cast<ComplexType, pointer>(buffer));
  CVector_ref mem_pool(make_device_ptr(buffer.origin()), buffer.extensions());
  stdCVector_ref comm_mem_pool(static_cast<ComplexType*>(comm_buffer.origin()), comm_buffer.extensions());

  size_t displ = 0;
  // Mixed Density Matrix for walkers at original configuration
  CMatrix_ref Gwork(mem_pool.origin() + displ, {G_nr, G_nc});
  displ += G_nr * G_nc;
  // vias potential for walkers at original configuration
  CMatrix_ref vbias(mem_pool.origin() + displ, {long(localnCV), nwalk});
  displ += localnCV * nwalk;
  // right hand side matrix in calculation of HS potential for all steps: ~ sigma + (vbias-vMF)
  // The same vbias is used in all steps
  CMatrix_ref X(mem_pool.origin() + displ, {long(localnCV), nwalk * nsteps});
  displ += localnCV * nwalk * nsteps;
  // HS potential for all steps.
  CMatrix_ref vHS(mem_pool.origin() + displ, {vhs_nr, vhs_nc});
  displ += vhs_nr * vhs_nc;
  // second view of vHS matrix for use in propagation step
  C3Tensor_ref vHS3D(vHS.origin(), {vhs3d_n1, vhs3d_n2, vhs3d_n3});

  // setup communication buffers
  displ = 0;
  // Mixed Density Matrix for walkers at original configuration
  stdCMatrix_ref Gsend(comm_mem_pool.origin() + displ, {G_nr, G_nc});
  displ += G_nr * G_nc;
  // Mixed Density Matrix used for communication
  stdCMatrix_ref Grecv(comm_mem_pool.origin() + displ, {G_nr, G_nc});
  displ += G_nr * G_nc;
  // HS potential for communication
  stdCMatrix_ref vsend(comm_mem_pool.origin() + displ, {vhs_nr, vhs_nc});
  displ += vhs_nr * vhs_nc;
  // HS potential for communication
  stdCMatrix_ref vrecv(comm_mem_pool.origin() + displ, {vhs_nr, vhs_nc});

  // partition G and v for communications: all cores communicate a piece of the matrix
  int vak0, vakN;
  int Gak0, GakN;
  std::tie(Gak0, GakN) = FairDivideBoundary(TG.getLocalTGRank(), int(Gsend.num_elements()), TG.getNCoresPerTG());
  std::tie(vak0, vakN) = FairDivideBoundary(TG.getLocalTGRank(), int(vHS.num_elements()), TG.getNCoresPerTG());
  if (new_shm_space)
  {
    // use mpi3 when ready
    if (req_Grecv != MPI_REQUEST_NULL)
      MPI_Request_free(&req_Grecv);
    if (req_Gsend != MPI_REQUEST_NULL)
      MPI_Request_free(&req_Gsend);
    if (req_vrecv != MPI_REQUEST_NULL)
      MPI_Request_free(&req_vrecv);
    if (req_vsend != MPI_REQUEST_NULL)
      MPI_Request_free(&req_vsend);
    MPI_Send_init(to_address(Gsend.origin()) + Gak0, (GakN - Gak0) * sizeof(ComplexType), MPI_CHAR, TG.prev_core(),
                  3456, &TG.TG(), &req_Gsend);
    MPI_Recv_init(to_address(Grecv.origin()) + Gak0, (GakN - Gak0) * sizeof(ComplexType), MPI_CHAR, TG.next_core(),
                  3456, &TG.TG(), &req_Grecv);
    MPI_Send_init(to_address(vsend.origin()) + vak0, (vakN - vak0) * sizeof(ComplexType), MPI_CHAR, TG.prev_core(),
                  5678, &TG.TG(), &req_vsend);
    MPI_Recv_init(to_address(vrecv.origin()) + vak0, (vakN - vak0) * sizeof(ComplexType), MPI_CHAR, TG.next_core(),
                  5678, &TG.TG(), &req_vrecv);
  }

  // local matrices for temporary accumulation
  if (MFfactor.size(0) != nnodes * nsteps || MFfactor.size(1) != nwalk)
    MFfactor = std::move(CMatrix({nnodes * nsteps, nwalk}));
  if (hybrid_weight.size(0) != nnodes * nsteps || hybrid_weight.size(1) != nwalk)
    hybrid_weight = std::move(CMatrix({nnodes * nsteps, nwalk}));
  if (new_overlaps.size(0) != nwalk)
    new_overlaps = std::move(CVector(iextensions<1u>{nwalk}));
  if (new_energies.size(0) != nwalk || new_energies.size(1) != 3)
    new_energies = std::move(CMatrix({nwalk, 3}));

  // if timestep changed, recalculate one body propagator
  if (std::abs(dt - old_dt) > 1e-6)
  {
    old_dt = dt;
    // generate1BodyPropagator currently expects a shared_allocator, fix later
    using P1shm = ma::sparse::csr_matrix<ComplexType, int, int, shared_allocator<ComplexType>, ma::sparse::is_root>;
    P1          = std::move(generate1BodyPropagator<P1shm>(TG, 1e-8, dt, H1));
  }

  fill_n(vsend.origin() + vak0, (vakN - vak0), zero);

  TG.local_barrier();
  AFQMCTimers[setup_timer].get().stop();

  MPI_Status st;

  // 1. Calculate Green function for all (local) walkers
  AFQMCTimers[G_for_vbias_timer].get().start();
  wfn.MixedDensityMatrix_for_vbias(wset, Gwork);
  ma::copy(Gwork.sliced(Gak0, GakN), Gsend.sliced(Gak0, GakN));
  TG.local_barrier();
  AFQMCTimers[G_for_vbias_timer].get().stop();

  for (int k = 0; k < nnodes; ++k)
  {
    // 2. wait for communication of previous step
    AFQMCTimers[vHS_comm_overhead_timer].get().start();
    if (k > 0)
    {
      MPI_Wait(&req_Grecv, &st);
      MPI_Wait(&req_Gsend, &st); // need to wait for Gsend in order to overwrite Gsend
      ma::copy(Grecv.sliced(Gak0, GakN), Gsend.sliced(Gak0, GakN));
      copy_n(Grecv.origin() + Gak0, GakN - Gak0, Gwork.origin() + Gak0);
      TG.local_barrier();
    }

    // 3. setup next communication
    if (k < nnodes - 1)
    {
      MPI_Start(&req_Gsend);
      MPI_Start(&req_Grecv);
    }
    AFQMCTimers[vHS_comm_overhead_timer].get().stop();

    // calculate vHS contribution from this node
    // 4a. Calculate vbias for initial configuration
    AFQMCTimers[vbias_timer].get().start();
    wfn.vbias(Gwork, vbias, sqrtdt);
    AFQMCTimers[vbias_timer].get().stop();

    // 4b. Assemble X(nCV,nsteps,nwalk)
    int q = (node_number + k) % nnodes;
    CMatrix_ref mf_(MFfactor[q * nsteps].origin(), {long(nsteps), long(nwalk)});
    CMatrix_ref hw_(hybrid_weight[q * nsteps].origin(), {long(nsteps), long(nwalk)});
    assemble_X(nsteps, nwalk, sqrtdt, X, vbias, mf_, hw_);

    // 4c. Calculate vHS(M*M,nsteps,nwalk)
    AFQMCTimers[vHS_timer].get().start();
    wfn.vHS(X, vHS, sqrtdt);
    TG.local_barrier();
    AFQMCTimers[vHS_timer].get().stop();

    AFQMCTimers[vHS_comm_overhead_timer].get().start();
    // 5. receive v
    if (k > 0)
    {
      MPI_Wait(&req_vrecv, &st);
      MPI_Wait(&req_vsend, &st);
      copy_n(vrecv.origin() + vak0, vakN - vak0, vsend.origin() + vak0);
    }

    // 6. add local contribution to vsend
    using ma::axpy;
#ifdef ENABLE_CUDA
    copy_n(vHS.origin() + vak0, vakN - vak0,
           vrecv.origin() + vak0); // using vrecv as temporary space, since it is free now
    axpy(vakN - vak0, one, vrecv.origin() + vak0, 1, vsend.origin() + vak0, 1);
#else
    axpy(vakN - vak0, one, vHS.origin() + vak0, 1, vsend.origin() + vak0, 1);
#endif

    // 7. start v communication
    MPI_Start(&req_vsend);
    MPI_Start(&req_vrecv);
    TG.local_barrier();
    AFQMCTimers[vHS_comm_overhead_timer].get().stop();
  }

  // after the wait, vrecv has the final vHS for the local walkers
  AFQMCTimers[vHS_comm_overhead_timer].get().start();
  MPI_Wait(&req_vrecv, &st);
  MPI_Wait(&req_vsend, &st);
  copy_n(vrecv.origin() + vak0, vakN - vak0, vHS.origin() + vak0);
  TG.local_barrier();

  // reduce MF and HWs
  if (TG.TG().size() > 1)
  {
    TG.TG().all_reduce_in_place_n(to_address(MFfactor.origin()), MFfactor.num_elements(), std::plus<>());
    TG.TG().all_reduce_in_place_n(to_address(hybrid_weight.origin()), hybrid_weight.num_elements(), std::plus<>());
  }
  TG.local_barrier();
  AFQMCTimers[vHS_comm_overhead_timer].get().stop();

  // From here on is similar to Shared
  int nx = 1;
  if (walker_type == COLLINEAR)
    nx = 2;

  // from now on, individual work on each walker/step
  const int ntasks_per_core     = int(nx * nwalk) / TG.getNCoresPerTG();
  const int ntasks_total_serial = ntasks_per_core * TG.getNCoresPerTG();
  const int nextra              = int(nx * nwalk) - ntasks_total_serial;

  // each processor does ntasks_percore_serial overlaps serially
  const int tk0 = TG.getLocalTGRank() * ntasks_per_core;
  const int tkN = (TG.getLocalTGRank() + 1) * ntasks_per_core;

  // make new communicator if nextra changed from last setting
  reset_nextra(nextra);

  int n0 = node_number * nsteps_;
  for (int ni = 0; ni < nsteps_; ni++)
  {
    // 5. Propagate walkers
    AFQMCTimers[propagate_timer].get().start();
    if (nbatched_propagation != 0)
    {
      if (wset.getNBackProp() > 0)
      {
        apply_propagators_construct_propagator_batched(wset, ni, vHS3D);
      }
      else
      {
        apply_propagators_batched(wset, ni, vHS3D);
      }
    }
    else
    {
      if (wset.getNBackProp() > 0)
      {
        apply_propagators_construct_propagator(wset, ni, tk0, tkN, ntasks_total_serial, vHS3D);
      }
      else
      {
        apply_propagators(wset, ni, tk0, tkN, ntasks_total_serial, vHS3D);
      }
    }
    AFQMCTimers[propagate_timer].get().stop();

    // 6. Calculate local energy/overlap
    AFQMCTimers[pseudo_energy_timer].get().start();
    if (hybrid)
    {
      wfn.Overlap(wset, new_overlaps);
    }
    else
    {
      wfn.Energy(wset, new_energies, new_overlaps);
    }
    TG.local_barrier();
    AFQMCTimers[pseudo_energy_timer].get().stop();

    // 7. update weights/energy/etc, apply constrains/bounds/etc
    AFQMCTimers[extra_timer].get().start();
    if (TG.TG_local().root())
    {
      if (free_projection)
      {
        free_projection_walker_update(wset, dt, new_overlaps, MFfactor[n0 + ni], Eshift, hybrid_weight[n0 + ni], work);
      }
      else if (hybrid)
      {
        hybrid_walker_update(wset, dt, apply_constrain, importance_sampling, Eshift, new_overlaps, MFfactor[n0 + ni],
                             hybrid_weight[n0 + ni], work);
      }
      else
      {
        local_energy_walker_update(wset, dt, apply_constrain, Eshift, new_overlaps, new_energies, MFfactor[n0 + ni],
                                   hybrid_weight[n0 + ni], work);
      }
    }
    TG.local_barrier();
    AFQMCTimers[extra_timer].get().stop();
  }
}

} // namespace afqmc

} // namespace qmcplusplus
