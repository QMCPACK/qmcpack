//////////////////////////////////////////////////////////////////////
// This file is distributed under the University of Illinois/NCSA Open Source
// License.  See LICENSE file in top directory for details.
//
// Copyright (c) 2016 Jeongnim Kim and QMCPACK developers.
//
// File developed by:
// Miguel A. Morales, moralessilva2@llnl.gov 
//    Lawrence Livermore National Laboratory 
//
// File created by:
// Miguel A. Morales, moralessilva2@llnl.gov 
//    Lawrence Livermore National Laboratory 
////////////////////////////////////////////////////////////////////////////////

#include <vector>
#include <map>
#include <string>
#include <iostream>
#include <tuple>

#include "AFQMC/config.h"
#include "Utilities/FairDivide.h"
#include "AFQMC/Propagators/WalkerSetUpdate.hpp"
#include "AFQMC/Walkers/WalkerConfig.hpp"

namespace qmcplusplus
{

namespace afqmc
{

/*
 * Propagates the walker population nsteps forward with a fixed vbias (from the initial 
 * configuration).   
 */
template<class WlkSet>
void AFQMCBasePropagator::step(int nsteps_, WlkSet& wset, RealType Eshift, RealType dt) 
{ 
  AFQMCTimers[setup_timer]->start();
  auto walker_type = wset.getWalkerType();
  int nsteps= nsteps_;
  int nwalk = wset.size();
  RealType sqrtdt = std::sqrt(dt);  
  long Gsize = wfn.size_of_G_for_vbias();
  int localnCV = wfn.local_number_of_cholesky_vectors();
  // if transposed_XXX_=true  --> XXX[nwalk][...], 
  // if transposed_XXX_=false --> XXX[...][nwalk]
  int vhs_nr = NMO*NMO, vhs_nc = nwalk*nsteps;
  if(transposed_vHS_) std::swap(vhs_nr,vhs_nc);
  int vhs3d_n1 = NMO, vhs3d_n2 = NMO, vhs3d_n3 = nwalk*nsteps;
  if(transposed_vHS_) {
    vhs3d_n1 = nwalk*nsteps;
    vhs3d_n2 = vhs3d_n3 = NMO; 
  }  
  int G_nr = Gsize, G_nc = nwalk;
  if(transposed_G_) std::swap(G_nr,G_nc);


  //  shared buffer used for:
  //  G_for_vbias:     [ Gsize * nwalk ]
  //  vbias:           [ localnCV * nwalk ]
  //  X:               [ localnCV * nwalk * nstep ]
  //  vHS:             [ NMO*NMO * nwalk * nstep ]      
  // memory_needs: nwalk * ( 2*nsteps + Gsize + localnCV*(nstep+1) + NMO*NMO*nstep )
  size_t memory_needs = nwalk * ( Gsize + size_t(localnCV)*(nsteps+1) + NMO*NMO*nsteps );

  // 0. Allocate memory and set shared memory structures
  if(buffer.num_elements() < memory_needs ) { 
    buffer = std::move(sharedCVector(iextensions<1u>{memory_needs},aux_alloc_));
    using std::fill_n;
    fill_n(buffer.origin(),buffer.num_elements(),ComplexType(0.0));
  }  

  // convert array to a basic_array<element,1,pointer>. This generates a view of buffer 
  // with type basic_array<ComplexType,1,pointer> which eliminates the need to cast origin()  
  //auto mem_pool(boost::multi::static_array_cast<ComplexType, pointer>(buffer));
  CVector_ref mem_pool(make_device_ptr(buffer.origin()),buffer.extensions());
  //auto mem_pool(array_cast<pointer>(buffer));

  size_t displ=0;
  // Mixed Density Matrix for walkers at original configuration
  // Actual dimensions depend on transposed_G_, see above
  CMatrix_ref G(mem_pool.origin()+displ, {G_nr,G_nc}); 
    displ+=G_nr*G_nc; 
  // vias potential for walkers at original configuration
  CMatrix_ref vbias(mem_pool.origin()+displ, {long(localnCV),long(nwalk)}); 
    displ+=size_t(localnCV)*nwalk;
  // right hand side matrix in calculation of HS potential for all steps: ~ sigma + (vbias-vMF)
  // The same vbias is used in all steps
  CMatrix_ref X(mem_pool.origin()+displ, {long(localnCV),long(nwalk*nsteps)}); 
    displ+=size_t(localnCV)*nwalk*nsteps;
  // HS potential for all steps.
  // Actual dimensions depend on transposed_vHS_, see above
  CMatrix_ref vHS(mem_pool.origin()+displ, {vhs_nr,vhs_nc}); 
  // second view of vHS matrix for use in propagation step
  // Actual dimensions depend on transposed_vHS_, see above
  C3Tensor_ref vHS3D(mem_pool.origin()+displ,{vhs3d_n1,vhs3d_n2,vhs3d_n3}); 

  int cv0,cvN;
  std::tie(cv0,cvN) = FairDivideBoundary(TG.getLocalTGRank(),localnCV,TG.getNCoresPerTG());

  // local matrices for temporary accumulation
// MAM: Note -  Since nsteps can change routinely, don't reextent these.
//              Instead have a buffer and create array_refs from them!!!
  if(MFfactor.size(0) != nsteps || MFfactor.size(1) != nwalk) 
    MFfactor = std::move(CMatrix({long(nsteps),long(nwalk)}));
  if(hybrid_weight.size(0) != nsteps || hybrid_weight.size(1) != nwalk) 
    hybrid_weight = std::move(CMatrix({long(nsteps),long(nwalk)}));
  if(new_overlaps.size(0) != nwalk) new_overlaps = std::move(CVector(iextensions<1u>{nwalk}));
  if(new_energies.size(0) != nwalk || new_energies.size(1) != 3) 
    new_energies = std::move(CMatrix({long(nwalk),3}));

  // if timestep changed, recalculate one body propagator
  if( std::abs(dt-old_dt) > 1e-6 ) 
    generateP1(dt,walker_type);
  AFQMCTimers[setup_timer]->stop();

  // 1. Calculate Green function for all walkers
  AFQMCTimers[G_for_vbias_timer]->start();
  wfn.MixedDensityMatrix_for_vbias(wset,G);
  AFQMCTimers[G_for_vbias_timer]->stop();
//std::cout<<" G: " <<ma::dot(G(G.extension(0),0),G(G.extension(0),0)) <<std::endl;

  // 2. Calculate vbias for initial configuration
  AFQMCTimers[vbias_timer]->start();
  if (free_projection) {
    fill_n(vbias.origin(), localnCV*nwalk, ComplexType(0.0,0.0));
  } else {
    wfn.vbias(G,vbias,sqrtdt);
  }
  AFQMCTimers[vbias_timer]->stop();
//std::cout<<" vbias: " <<ma::dot(vbias(vbias.extension(0),0),vbias(vbias.extension(0),0)) <<std::endl;

  // 3. Assemble X(nCV,nsteps,nwalk)
  AFQMCTimers[assemble_X_timer]->start();
  assemble_X(nsteps,nwalk,sqrtdt,X,vbias,MFfactor,hybrid_weight);
  if(TG.TG_local().size() > 1) {
    TG.TG_local().all_reduce_in_place_n(to_address(MFfactor.origin()),MFfactor.num_elements(),
                                        std::plus<>());  
    TG.TG_local().all_reduce_in_place_n(to_address(hybrid_weight.origin()),
                                        hybrid_weight.num_elements(),std::plus<>());  
  }  
  AFQMCTimers[assemble_X_timer]->stop();
// Store sqrtdt*X in wset if back propagating
  int bp_step=wset.getBPPos(), bp_max=wset.NumBackProp();
  if(bp_step >= 0 && bp_step<bp_max) {
    for(int ni=0; ni<nsteps; ni++) {
      if(bp_step<bp_max) {
        auto&& V(wset.getFields(bp_step));
        if(nsteps==1) {
          copy_n(X[cv0].origin(),nwalk*(cvN-cv0),V[cv0].origin());
          ma::scal(sqrtdt,V.sliced(cv0,cvN));
        } else {
          ma::add(ComplexType(0.0),   V.sliced(cv0,cvN),
                  ComplexType(sqrtdt),X( {cv0,cvN}, {ni*nwalk,(ni+1)*nwalk} ),
                                      V.sliced(cv0,cvN));
        }
        bp_step++;
      }   
    }
    TG.TG_local().barrier();
  }     
//std::cout<<" X: " <<ma::dot(X(X.extension(0),0),X(X.extension(0),0)) <<std::endl;

  // 4. Calculate vHS(M*M,nsteps,nwalk)/vHS(nsteps,nwalk,M*M)
  AFQMCTimers[vHS_timer]->start();
  wfn.vHS(X,vHS,sqrtdt);
  AFQMCTimers[vHS_timer]->stop();
//std::cout<<" Pg vHS: " <<TG.Global().rank() <<" " <<ma::sum(vHS) <<"\n" <<std::endl;
//std::cout<<" vHS: " <<ma::dot(vHS[0],vHS[0]) <<"\n\n" <<std::endl;

  int nx = 1;
  if(walker_type == COLLINEAR) nx=2;

  // from now on, individual work on each walker/step
  const int ntasks_per_core = int(nx*nwalk)/TG.getNCoresPerTG();
  const int ntasks_total_serial = ntasks_per_core*TG.getNCoresPerTG();
  const int nextra = int(nx*nwalk) - ntasks_total_serial;

  // each processor does ntasks_percore_serial overlaps serially
  const int tk0 = TG.getLocalTGRank()*ntasks_per_core;
  const int tkN = (TG.getLocalTGRank()+1)*ntasks_per_core;

  // make new communicator if nextra changed from last setting
  reset_nextra(nextra);

  for(int ni=0; ni<nsteps_; ni++) {

    // 5. Propagate walkers
    AFQMCTimers[propagate_timer]->start();
    if(nbatched_propagation != 0) { 
      apply_propagators_batched('N',wset,ni,vHS3D);
    } else {
      apply_propagators('N',wset,ni,tk0,tkN,ntasks_total_serial,vHS3D);
    }
    AFQMCTimers[propagate_timer]->stop();
//std::cout<<" propg wfn " <<std::endl; //: " 
//std::cout<<ma::sum(wset[0].SlaterMatrix(Alpha)) <<" " <<ma::sum(wset[0].SlaterMatrix(Beta)) <<std::endl;

    // 6. Calculate local energy/overlap
    AFQMCTimers[pseudo_energy_timer]->start();
    if(hybrid) {
      wfn.Overlap(wset,new_overlaps);
    } else {
      wfn.Energy(wset,new_energies,new_overlaps);
    }
    TG.local_barrier();
    AFQMCTimers[pseudo_energy_timer]->stop();
//std::cout<<" pseudo energy " <<std::endl;

    // 7. update weights/energy/etc, apply constrains/bounds/etc 
    AFQMCTimers[extra_timer]->start();
    if(TG.TG_local().root()) { 
      if(free_projection) { 
        free_projection_walker_update(wset,dt,new_overlaps,
                         MFfactor[ni],Eshift,hybrid_weight[ni],work);
      } else if(hybrid) {
        hybrid_walker_update(wset,dt,apply_constrain,importance_sampling,
                         Eshift,new_overlaps,MFfactor[ni],hybrid_weight[ni],work);
      } else {
        local_energy_walker_update(wset,dt,apply_constrain,Eshift,
                                   new_overlaps,new_energies,
                                   MFfactor[ni],hybrid_weight[ni],work);
      }
      if(wset.getBPPos() >=0 && wset.getBPPos()<wset.NumBackProp()) wset.advanceBPPos();   
// MAM: BP will not work if the nsteps*nsubsteps per block is not consistent with 
//      steps in BP
      if(wset.getBPPos() >=0) wset.advanceHistoryPos();  
    }
    TG.local_barrier();
    AFQMCTimers[extra_timer]->stop();
//std::cout<<" update: " <<hybrid_weight[0][0]  <<" " <<MFfactor[0][0] <<std::endl <<std::endl;

  }  
}


/*
 * This routine assumes that the 1 body propagator does not need updating
 */
template<class WlkSet, class CTens, class CMat>
void AFQMCBasePropagator::BackPropagate(int nbpsteps, int nStabalize, WlkSet& wset, CTens&& Refs, CMat&& detR)
{

  using std::copy_n;
  auto walker_type = wset.getWalkerType();
  int nwalk = wset.size();
  int globalnCV = wfn.global_number_of_cholesky_vectors();

  int vhs_nr = NMO*NMO, vhs_nc = nwalk;
  if(transposed_vHS_) std::swap(vhs_nr,vhs_nc);
  int vhs3d_n1 = NMO, vhs3d_n2 = NMO, vhs3d_n3 = nwalk;
  if(transposed_vHS_) {
    vhs3d_n1 = nwalk;
    vhs3d_n2 = vhs3d_n3 = NMO; 
  }  

  //  shared buffer used for:
  //  X:               [ globalnCV * nwalk ]
  //  vHS:             [ NMO*NMO * nwalk ]      
  // memory_needs: nwalk * ( globalnCV + NMO*NMO )
  size_t memory_needs = nwalk * ( globalnCV + NMO*NMO );

  // 0. Allocate memory and set shared memory structures
  if(buffer.num_elements() < memory_needs ) { 
    buffer = std::move(sharedCVector(iextensions<1u>{memory_needs},aux_alloc_));
    using std::fill_n;
    fill_n(buffer.origin(),buffer.num_elements(),ComplexType(0.0));
  }  

  // convert array to a basic_array<element,1,pointer>. This generates a view of buffer 
  // with type basic_array<ComplexType,1,pointer> which eliminates the need to cast origin()  
  //auto mem_pool(boost::multi::static_array_cast<ComplexType, pointer>(buffer));
  CVector_ref mem_pool(make_device_ptr(buffer.origin()),buffer.extensions());
  //auto mem_pool(array_cast<pointer>(buffer));

  size_t displ=0;
  CMatrix_ref X(mem_pool.origin()+displ, {long(globalnCV),long(nwalk)}); 
    displ+=X.num_elements();
  // HS potential for all steps.
  // Actual dimensions depend on transposed_vHS_, see above
  CMatrix_ref vHS(mem_pool.origin()+displ, {vhs_nr,vhs_nc}); 
  // second view of vHS matrix for use in propagation step
  // Actual dimensions depend on transposed_vHS_, see above
  C3Tensor_ref vHS3D(mem_pool.origin()+displ,{vhs3d_n1,vhs3d_n2,vhs3d_n3}); 

  auto&& Fields(wset.getFields());
  assert(Fields.size(0) >= nbpsteps);  
  assert(Fields.size(1) == globalnCV);  
  assert(Fields.size(2) == nwalk);  
  
  int nrow(NMO*((walker_type==NONCOLLINEAR)?2:1));
  int ncol(NAEA+((walker_type==CLOSED)?0:NAEB));
  assert(Refs.size(0) == nwalk);  
  int nrefs = Refs.size(1); 
  assert(Refs.size(2) == nrow*ncol);  

  int cv0,cvN;
  std::tie(cv0,cvN) = FairDivideBoundary(TG.getLocalTGRank(),globalnCV,TG.getNCoresPerTG());
  int r0,rN;
  std::tie(r0,rN) = FairDivideBoundary(TG.getLocalTGRank(),nrow*ncol,TG.getNCoresPerTG());

  int nx = 1;
  if(walker_type == COLLINEAR) nx=2;

  assert(detR.size(0) == nwalk);
  assert(detR.size(1) == nrefs*nx);
  std::fill_n(detR.origin(),detR.num_elements(),ComplexType(1.0,0.0));

  // from now on, individual work on each walker/step
  const int ntasks_per_core = int(nx*nwalk)/TG.getNCoresPerTG();
  const int ntasks_total_serial = ntasks_per_core*TG.getNCoresPerTG();
  const int nextra = int(nx*nwalk) - ntasks_total_serial;

  // each processor does ntasks_percore_serial overlaps serially
  const int tk0 = TG.getLocalTGRank()*ntasks_per_core;
  const int tkN = (TG.getLocalTGRank()+1)*ntasks_per_core;

  // make new communicator if nextra changed from last setting
  reset_nextra(nextra);

  // to avoid having to modify the existing routines, 
  // I'm storing the walkers SlaterMatrix on SlaterMatrixAux
  // and copying the back propagated references into SlaterMatrix
  // 0. copy SlaterMatrix to SlaterMatrixAux 
  for(int i=0; i<nwalk; i++) { 
    copy_n(wset[i].SlaterMatrix(Alpha).origin()+r0,rN-r0,
           wset[i].SlaterMatrixAux(Alpha).origin()+r0); 
    // optimize for the single reference case
    if(nrefs==1) 
      copy_n(Refs[i][0].origin()+r0,rN-r0,
             make_device_ptr(wset[i].SlaterMatrix(Alpha).origin())+r0);
  }  
  TG.TG_local().barrier();

  for(int ni=nbpsteps-1; ni>=0; --ni) {   

    // 1. Get X(nCV,nwalk) from wset
    copy_n(Fields[ni][cv0].origin(),nwalk*(cvN-cv0),X[cv0].origin());
//std::fill_n(X.origin(),X.num_elements(),ComplexType(0.0));
    TG.TG_local().barrier();    

    // 2. Calculate vHS(M*M,nwalk)/vHS(nwalk,M*M) 
    //  X has been scaled by sqrtdt to avoid needing it here 
    wfn.vHS(X,vHS);
//std::cout<<" BP vHS: " <<TG.Global().rank() <<" " <<ma::sum(vHS) <<"\n" <<std::endl;

    for(int nr=0; nr<nrefs; ++nr) {   
      
      // 3. copy reference to SlaterMatrix
      if(nrefs>1) 
        for(int i=0; i<nwalk; i++)
          copy_n(Refs[i][nr].origin()+r0,rN-r0,
                 make_device_ptr(wset[i].SlaterMatrix(Alpha).origin())+r0);
      TG.TG_local().barrier();

      // 4. Propagate walkers
      if(nbatched_propagation != 0) 
        apply_propagators_batched('H',wset,0,vHS3D);
      else 
        apply_propagators('H',wset,0,tk0,tkN,ntasks_total_serial,vHS3D);
      TG.local_barrier();

      // always end (n==0) with an orthogonalization  
      if(ni==0 || ni%nStabalize==0) {
        // orthogonalize
        if(nbatched_qr != 0) {
          if(walker_type!=COLLINEAR)
            Orthogonalize_batched(wset,detR(detR.extension(0),{nr,nr+1}));
          else
            Orthogonalize_batched(wset,detR(detR.extension(0),{2*nr,2*nr+2}));
        } else {
          if(walker_type!=COLLINEAR)
            Orthogonalize_shared(wset,detR(detR.extension(0),{nr,nr+1}));
          else
            Orthogonalize_shared(wset,detR(detR.extension(0),{2*nr,2*nr+2}));
        }
      }    
      TG.TG_local().barrier();

      // 5. copy reference to back 
      if(nrefs>1) 
        for(int i=0; i<nwalk; i++)
          copy_n(make_device_ptr(wset[i].SlaterMatrix(Alpha).origin())+r0,rN-r0,
                 Refs[i][nr].origin()+r0);
      TG.TG_local().barrier();

    }

  }  

  // 6. restore the Slater Matrix 
  for(int i=0; i<nwalk; i++) {
    if(nrefs==1) 
      copy_n(wset[i].SlaterMatrix(Alpha).origin()+r0,rN-r0,
             Refs[i][0].origin()+r0); 
    copy_n(wset[i].SlaterMatrixAux(Alpha).origin()+r0,rN-r0,
           wset[i].SlaterMatrix(Alpha).origin()+r0);
  }
  TG.TG_local().barrier();

}


template<class WSet>
void AFQMCBasePropagator::apply_propagators(char TA, WSet& wset, int ni, int tk0, int tkN, 
                                              int ntasks_total_serial,
                                              C3Tensor_ref& vHS3D)
{  
  int nwalk = wset.size();
  auto walker_type = wset.getWalkerType();

  int spin(0);
  if(spin_dependent_P1) {
    spin=1;
    if(walker_type!=COLLINEAR)
      APP_ABORT(" Error: Spin dependent P1 being used with CLOSED walker.\n");
  }  

  if(transposed_vHS_) {
    // vHS3D[nstep*nwalk][M][M]
    if(walker_type == COLLINEAR) {
      // in this case, tk corresponds to 2x the walker number  
      for(int tk=tk0; tk<tkN; ++tk) {
        int nt = ni*nwalk+tk/2;
        if(tk%2==0)
          SDetOp->Propagate(wset[tk/2].SlaterMatrix(Alpha),P1[0],vHS3D[nt],order,TA);
        else
          SDetOp->Propagate(wset[tk/2].SlaterMatrix(Beta),P1[spin],vHS3D[nt],order,TA);
      }
      if(last_nextra > 0) {
        int tk = (ntasks_total_serial+last_task_index);
        int nt = ni*nwalk+tk/2;
        if(tk%2==0)
          SDetOp->Propagate(wset[tk/2].SlaterMatrix(Alpha),P1[0],vHS3D[nt],local_group_comm,order,TA);
        else
          SDetOp->Propagate(wset[tk/2].SlaterMatrix(Beta),P1[spin],vHS3D[nt],local_group_comm,order,TA);
      }
    } else {
      // in this case, tk corresponds to walker number  
      for(int tk=tk0; tk<tkN; ++tk) {
        int nt = ni*nwalk+tk;
        SDetOp->Propagate(wset[tk].SlaterMatrix(Alpha),P1[0],vHS3D[nt],order,TA);
      }
      if(last_nextra > 0) {
        int iw = ntasks_total_serial+last_task_index;
        int nt = ni*nwalk+iw;
        SDetOp->Propagate(wset[iw].SlaterMatrix(Alpha),P1[0],vHS3D[nt],local_group_comm,order,TA);
      }
    }
  } else {  
    if(walker_type==NONCOLLINEAR) {
      if(local_vHS.size(0) != 2*NMO || local_vHS.size(1) != 2*NMO)
        local_vHS = std::move(CMatrix({2*NMO,2*NMO}));
    } else {
      if(local_vHS.size(0) != NMO || local_vHS.size(1) != NMO)
        local_vHS = std::move(CMatrix({NMO,NMO}));
    }
    // vHS3D[M][M][nstep*nwalk]: need temporary buffer in this case
    if(walker_type == COLLINEAR) {
      int oldw=-1;
      // in this case, tk corresponds to 2x the walker number  
      for(int tk=tk0; tk<tkN; ++tk) {
        int nt = ni*nwalk+tk/2;
        if(oldw != tk/2) { 
          local_vHS = vHS3D(local_vHS.extension(0),local_vHS.extension(1),nt);
          oldw=tk/2;
        }
        if(tk%2==0)
          SDetOp->Propagate(wset[tk/2].SlaterMatrix(Alpha),P1[0],local_vHS,order,TA);
        else
          SDetOp->Propagate(wset[tk/2].SlaterMatrix(Beta),P1[spin],local_vHS,order,TA);
      }
      if(last_nextra > 0) {
        int tk = (ntasks_total_serial+last_task_index);
        int nt = ni*nwalk+tk/2;
        local_vHS = vHS3D(local_vHS.extension(0),local_vHS.extension(1),nt);
        if(tk%2==0)
          SDetOp->Propagate(wset[tk/2].SlaterMatrix(Alpha),P1[0],local_vHS,local_group_comm,order,TA);
        else
          SDetOp->Propagate(wset[tk/2].SlaterMatrix(Beta),P1[spin],local_vHS,local_group_comm,order,TA);
      }
    } else {
      // in this case, tk corresponds to walker number  
      for(int tk=tk0; tk<tkN; ++tk) {
        int nt = ni*nwalk+tk;
        local_vHS = vHS3D(local_vHS.extension(0),local_vHS.extension(1),nt);
//std::cout<<" pp: " <<tk <<" " <<ma::sum(local_vHS) <<"\n" <<std::endl;
        SDetOp->Propagate(wset[tk].SlaterMatrix(Alpha),P1[0],local_vHS,order,TA);
      }
      if(last_nextra > 0) {
        int iw = ntasks_total_serial+last_task_index;
        int nt = ni*nwalk+iw;
        local_vHS = vHS3D(local_vHS.extension(0),local_vHS.extension(1),nt);
        SDetOp->Propagate(wset[iw].SlaterMatrix(Alpha),P1[0],local_vHS,local_group_comm,order,TA);
      }
    }
  }
  TG.local_barrier();
}

template<class WSet>
void AFQMCBasePropagator::apply_propagators_batched(char TA, WSet& wset, int ni, C3Tensor_ref& vHS3D)
{  
  int nwalk = wset.size();
  auto walker_type = wset.getWalkerType();
  int nbatch = std::min(nwalk,(nbatched_propagation<0?nwalk:nbatched_propagation)); 

  int spin(0);
  if(spin_dependent_P1) {
    spin=1;
    if(walker_type!=COLLINEAR)
      APP_ABORT(" Error: Spin dependent P1 being used with CLOSED walker.\n");
  }

  std::vector<CMatrix_ref> Ai; 
  Ai.reserve(nbatch);
  if(transposed_vHS_) {
    // vHS3D[nstep*nwalk][M][M]
    // in this case, tk corresponds to 2x the walker number  
    int nt = ni*nwalk;
    for(int iw=0; iw<nwalk; iw+=nbatch, nt+=nbatch) { 
      int nb = std::min(nbatch,nwalk-iw);
      Ai.clear();
      for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Alpha));
      SDetOp->BatchedPropagate(Ai,P1[0],vHS3D.sliced(nt,nt+nb),order,TA);
      if(walker_type == COLLINEAR) { 
        Ai.clear();
        for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Beta));
        SDetOp->BatchedPropagate(Ai,P1[spin],vHS3D.sliced(nt,nt+nb),order,TA);
      }  
    }
  } else {  
    int sz = (walker_type==NONCOLLINEAR?2*NMO:NMO);
    if(local_vHS.size(0) != nbatch || local_vHS.size(1) != sz*sz)
      local_vHS = std::move(CMatrix({nbatch,sz*sz}));
    // vHS3D[M][M][nstep*nwalk]: need temporary buffer in this case
    int N2 = vHS3D.size(0)*vHS3D.size(1);
    CMatrix_ref vHS2D(vHS3D.origin(),{N2,vHS3D.size(2)}); 
    C3Tensor_ref local3D(local_vHS.origin(),{nbatch,sz,sz}); 
    int nt = ni*nwalk;
    for(int iw=0; iw<nwalk; iw+=nbatch, nt+=nbatch) {
      int nb = std::min(nbatch,nwalk-iw);
      ma::transpose(vHS2D(vHS2D.extension(0),{nt,nt+nb}),local_vHS.sliced(0,nb));  
      Ai.clear();
      for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Alpha));
      SDetOp->BatchedPropagate(Ai,P1[0],local3D.sliced(0,nb),order,TA);
      if(walker_type == COLLINEAR) {
        Ai.clear();
        for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Beta));
        SDetOp->BatchedPropagate(Ai,P1[spin],local3D.sliced(0,nb),order,TA);
      }  
    }
  }
}

  /*
   * Orthogonalizes the Slater matrices of all walkers in the set.
   */
  template<class WlkSet, class CMat>
  void AFQMCBasePropagator::Orthogonalize_shared(WlkSet& wset, CMat&& detR) {
    if(new_overlaps.size() < 1)
      new_overlaps.reextent(iextensions<1u>{1});
    auto walker_type = wset.getWalkerType();
    double LogOverlapFactor(wset.getLogOverlapFactor());
    if(walker_type != COLLINEAR) {
      int iw=0;
      for(typename WlkSet::iterator it=wset.begin(); it!=wset.end(); ++it, ++iw) {
        if( iw%TG.getNCoresPerTG() == TG.getLocalTGRank() ) {
          detR[iw][0] *= SDetOp->Orthogonalize(it->SlaterMatrix(Alpha),LogOverlapFactor);
        }
      }
    } else {
      int cnt=0;
      int iw=0;
      for(typename WlkSet::iterator it=wset.begin(); it!=wset.end(); ++it, ++iw) {
        if( (cnt++)%TG.getNCoresPerTG() == TG.getLocalTGRank() ) {
          detR[iw][0] *= SDetOp->Orthogonalize(it->SlaterMatrix(Alpha),LogOverlapFactor);
        }
        if( (cnt++)%TG.getNCoresPerTG() == TG.getLocalTGRank() ) {
          detR[iw][1] *= SDetOp->Orthogonalize(it->SlaterMatrix(Beta),LogOverlapFactor);
        }
      }
    }
    TG.local_barrier();
  }

  /*
   * Orthogonalizes the Slater matrices of all walkers in the set.  
   * Options:
   *  - bool importanceSamplingt(default=true): use algorithm appropriate for importance sampling. 
   *         This means that the determinant of the R matrix in the QR decomposition is ignored.
   *         If false, add the determinant of R to the weight of the walker. 
   */
  template<class WlkSet, class CMat>
  void AFQMCBasePropagator::Orthogonalize_batched(WlkSet& wset, CMat&& detR) {
    auto walker_type = wset.getWalkerType();
    if(TG.TG_local().size() > 1)
      APP_ABORT(" Error: Batched routine called with TG.TG_local().size() > 1 \n");
    using std::fill_n;
    using std::copy_n;
    const int nw = wset.size();
    double LogOverlapFactor(wset.getLogOverlapFactor());
    int nbatch = std::min(nw,(nbatched_qr<0?nw:nbatched_qr));
    if(local_vHS.num_elements() < nbatch)
      local_vHS.reextent({nbatch,1});
    stdCVector detR_(iextensions<1u>{nbatch});
    std::vector<CMatrix_ref> Ai;
    Ai.reserve(nbatch);
    if(walker_type != COLLINEAR) {
      for(int iw=0; iw<nw; iw+=nbatch) {
        int nb = std::min(nbatch,nw-iw);
        Ai.clear();
        for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Alpha));
        SDetOp->BatchedOrthogonalize(Ai,LogOverlapFactor,local_vHS.origin());
        // GPU trickery,to make sure detR_ is in CPU, since detR is in CPU
        copy_n(local_vHS.origin(),nb,detR_.origin());
        for(int ni=0; ni<nb; ni++) detR[iw+ni][0] *= detR_[ni]; 
      }
    } else {

      for(int iw=0; iw<nw; iw+=nbatch) {
        int nb = std::min(nbatch,nw-iw);
        // Alpha 
        Ai.clear();
        for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Alpha));
        SDetOp->BatchedOrthogonalize(Ai,LogOverlapFactor,local_vHS.origin());
        copy_n(local_vHS.origin(),nb,detR_.origin());
        for(int ni=0; ni<nb; ni++) detR[iw+ni][0] *= detR_[ni]; 
        // Beta
        Ai.clear();
        for(int ni=0; ni<nb; ni++) Ai.emplace_back(wset[iw+ni].SlaterMatrix(Beta));
        SDetOp->BatchedOrthogonalize(Ai,LogOverlapFactor,local_vHS.origin());
        copy_n(local_vHS.origin(),nb,detR_.origin());
        for(int ni=0; ni<nb; ni++) detR[iw+ni][1] *= detR_[ni]; 
      }
    }
    TG.local_barrier();
  }

}

}

